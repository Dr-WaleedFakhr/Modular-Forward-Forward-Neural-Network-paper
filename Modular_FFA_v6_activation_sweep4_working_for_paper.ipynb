{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW9JNyx0xSEt",
        "outputId": "da1ca0a9-5fc4-4e32-db73-1d9d8f51423b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted\n",
            "✓ Config ready\n",
            "  Device: cuda\n",
            "  Base path: /content/drive/My Drive/Research/ModularFF/\n",
            "  PHASE 2 SETTINGS:\n",
            "    LR: 0.01 (10x higher)\n",
            "    Batch size: 256\n",
            "    Patience: 60\n",
            "    Epochs: 500\n",
            "    Alpha values: [0.0, 0.5, 1.0]\n",
            "\n",
            "============================================================\n",
            " Loading datasets (Phase 2)\n",
            "============================================================\n",
            "  ✓ FashionMNIST: K=10, dim= 784, train= 49411, val=10589, test=10000\n",
            "============================================================\n",
            "✓ Loaded 1 dataset(s): ['FashionMNIST']\n",
            "✓ PHASE 2 CONFIG: LR=0.01, batch=256, patience=60\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# CELL 1: Setup, Configuration, and Data Loading (v4 - Phase 2)\n",
        "# ================================================================\n",
        "# Phase 2: 4-Layer Hinton Comparison\n",
        "# Includes:\n",
        "#   - XOR (4-class), MNIST, FashionMNIST, Pendigits, LetterRecog\n",
        "#   - init_layer_weights with Gabor support\n",
        "#   - Higher LR (0.1) and larger batch (256) for faster convergence\n",
        "#   - Reduced patience (20) and epochs (300)\n",
        "# ================================================================\n",
        "\n",
        "import os, sys, json, time, copy, random, warnings, csv\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---- Mount Google Drive ----\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    IN_COLAB = True\n",
        "    print('\\u2713 Google Drive mounted')\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print('Not in Colab — using local paths')\n",
        "\n",
        "# ================================================================\n",
        "# GLOBAL CONFIGURATION — PHASE 2 OPTIMIZED\n",
        "# ================================================================\n",
        "CONFIG = {\n",
        "    # ----- Paths -----\n",
        "    'base_path': '/content/drive/My Drive/Research/ModularFF/' if IN_COLAB else './ModularFF/',\n",
        "\n",
        "    # ----- Which datasets to run -----\n",
        "    #'datasets_to_run': ['XOR', 'MNIST', 'FashionMNIST', 'Pendigits', 'LetterRecog'],\n",
        "\n",
        "    #'datasets_to_run': ['MNIST', 'FashionMNIST', 'Pendigits', 'LetterRecog'],\n",
        "    'datasets_to_run': ['FashionMNIST'],\n",
        "\n",
        "\n",
        "    # ----- Data Split Ratios -----\n",
        "    'split_ratios': (0.70, 0.15, 0.15),\n",
        "\n",
        "    # ----- Reproducibility -----\n",
        "    'seed': 42,\n",
        "    'seeds': [42],\n",
        "\n",
        "    # ----- Device -----\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # ----- Training (PHASE 2 OPTIMIZED) -----\n",
        "    'lr': 0.01,                    # Adam LR for 4-layer networks\n",
        "    'epochs': 500,                 # Max epochs (increased from 300)\n",
        "    'batch_size': 256,             # 2x larger for stability\n",
        "    'early_stop_patience': 60,     # Total patience before stopping\n",
        "    'min_epochs': 50,              # No early stopping before this epoch\n",
        "    'lr_reduce_patience': 20,      # Reduce LR every N epochs of no improvement\n",
        "    'lr_reduce_factor': 0.5,       # Halve LR on reduction\n",
        "    'hinton_sgd_lr': 0.03,         # Hinton's original SGD learning rate\n",
        "\n",
        "    # ----- FF Thresholds -----\n",
        "    'theta_neuron': 1.0,\n",
        "\n",
        "    # ----- Hybrid Loss (Phase 2: only 3 values) -----\n",
        "    'alpha_values': [0.0, 0.5, 1.0],\n",
        "\n",
        "    # ----- Per-Layer Dropout (disabled for Phase 2) -----\n",
        "    'layer_dropout': None,\n",
        "    'layer_dropout_ablation': [None],  # Skip dropout ablation\n",
        "\n",
        "    # ----- Meta-layers -----\n",
        "    'meta_layer': 'argmax',\n",
        "    'meta_layers_to_compare': ['argmax', 'calibrated', 'linear', 'mlp'],\n",
        "    'use_meta_layer': True,\n",
        "\n",
        "    # ----- First Layer Initialization -----\n",
        "    'init_method': 'kaiming',\n",
        "\n",
        "    # ----- ELM Mode -----\n",
        "    'elm_mode': False,\n",
        "\n",
        "    # ----- Pruning -----\n",
        "    'pruning_enabled': False,\n",
        "    'prune_beta': 2.0,\n",
        "    'prune_after_epochs': 10,\n",
        "    'prune_keep_ratio': 0.5,\n",
        "}\n",
        "\n",
        "# ----- Architecture per Dataset -----\n",
        "# Phase 2: 4-layer Hinton comparison (parameter-matched)\n",
        "ARCHITECTURES = {\n",
        "    'XOR': {\n",
        "        'modularff_archs': [[50, 50], [100, 100]],\n",
        "        'classic_ff': [100, 100],\n",
        "        'classic_ff_4L': [100, 100, 100, 100],\n",
        "        'modularff_4L': [50, 50, 50, 50],\n",
        "        'bp':         [100, 100],\n",
        "        'input_dim':  2,\n",
        "        'img_size':   None,\n",
        "    },\n",
        "    'MNIST': {\n",
        "        'modularff_archs': [[50, 50], [100, 100]],\n",
        "        'classic_ff': [500, 500],\n",
        "        'classic_ff_4L': [2000, 2000, 2000, 2000],  # ~13.6M params (Hinton's)\n",
        "        'modularff_4L': [550, 550, 550, 550],       # ~13.5M params (×10 specialists)\n",
        "        'bp':         [500, 500],\n",
        "        'input_dim':  784,\n",
        "        'img_size':   28,\n",
        "    },\n",
        "    'FashionMNIST': {\n",
        "        'modularff_archs': [[50, 50], [100, 100]],\n",
        "        'classic_ff': [500, 500],\n",
        "        'classic_ff_4L': [2000, 2000, 2000, 2000],  # ~13.6M params\n",
        "        'modularff_4L': [550, 550, 550, 550],       # ~13.5M params (×10 specialists)\n",
        "        'bp':         [500, 500],\n",
        "        'input_dim':  784,\n",
        "        'img_size':   28,\n",
        "    },\n",
        "    'Pendigits': {\n",
        "        'modularff_archs': [[50, 50], [100, 100]],\n",
        "        'classic_ff': [200, 200],\n",
        "        'classic_ff_4L': [500, 500, 500, 500],      # ~775K params\n",
        "        'modularff_4L': [150, 150, 150, 150],       # ~760K params (×10 specialists)\n",
        "        'bp':         [200, 200],\n",
        "        'input_dim':  16,\n",
        "        'img_size':   None,\n",
        "    },\n",
        "    'LetterRecog': {\n",
        "        'modularff_archs': [[50, 50], [100, 100]],\n",
        "        'classic_ff': [400, 400],\n",
        "        'classic_ff_4L': [800, 800, 800, 800],      # ~2.0M params\n",
        "        'modularff_4L': [150, 150, 150, 150],       # ~2.0M params (×26 specialists)\n",
        "        'bp':         [400, 400],\n",
        "        'input_dim':  16,\n",
        "        'img_size':   None,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Derived paths\n",
        "for key, folder in [('data_path', 'Data'), ('results_path', 'Results'),\n",
        "                    ('models_path', 'Models'), ('figures_path', 'Figures'),\n",
        "                    ('logs_path', 'Logs')]:\n",
        "    CONFIG[key] = os.path.join(CONFIG['base_path'], folder + '/')\n",
        "\n",
        "# Create directory tree\n",
        "dirs_to_create = [\n",
        "    CONFIG['data_path'],\n",
        "    CONFIG['results_path'],\n",
        "    CONFIG['models_path'],\n",
        "    CONFIG['figures_path'],\n",
        "    os.path.join(CONFIG['figures_path'], 'XOR'),\n",
        "    os.path.join(CONFIG['figures_path'], 'convergence'),\n",
        "    CONFIG['logs_path'],\n",
        "]\n",
        "for ds in CONFIG['datasets_to_run']:\n",
        "    dirs_to_create.append(os.path.join(CONFIG['results_path'], ds))\n",
        "    dirs_to_create.append(os.path.join(CONFIG['data_path'], ds))\n",
        "\n",
        "for d in dirs_to_create:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(f'\\u2713 Config ready')\n",
        "print(f'  Device: {CONFIG[\"device\"]}')\n",
        "print(f'  Base path: {CONFIG[\"base_path\"]}')\n",
        "print(f'  PHASE 2 SETTINGS:')\n",
        "print(f'    LR: {CONFIG[\"lr\"]} (10x higher)')\n",
        "print(f'    Batch size: {CONFIG[\"batch_size\"]}')\n",
        "print(f'    Patience: {CONFIG[\"early_stop_patience\"]}')\n",
        "print(f'    Epochs: {CONFIG[\"epochs\"]}')\n",
        "print(f'    Alpha values: {CONFIG[\"alpha_values\"]}')\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# UTILITIES\n",
        "# ================================================================\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# INITIALIZATION METHODS\n",
        "# ================================================================\n",
        "\n",
        "def gabor_filter_2d(size, theta, freq, sigma, phase, center):\n",
        "    cx, cy = center\n",
        "    y, x = np.ogrid[:size, :size]\n",
        "    x = x - cx\n",
        "    y = y - cy\n",
        "    x_rot = x * np.cos(theta) + y * np.sin(theta)\n",
        "    y_rot = -x * np.sin(theta) + y * np.cos(theta)\n",
        "    gaussian = np.exp(-(x_rot**2 + y_rot**2) / (2 * sigma**2))\n",
        "    sinusoid = np.cos(2 * np.pi * freq * x_rot + phase)\n",
        "    gabor = gaussian * sinusoid\n",
        "    gabor = gabor / (np.linalg.norm(gabor) + 1e-8)\n",
        "    return gabor\n",
        "\n",
        "\n",
        "def create_gabor_weights(n_neurons, img_size=28):\n",
        "    weights = []\n",
        "    for _ in range(n_neurons):\n",
        "        theta = np.random.uniform(0, np.pi)\n",
        "        freq = np.random.uniform(0.05, 0.4)\n",
        "        sigma = np.random.uniform(2, 6)\n",
        "        phase = np.random.uniform(0, 2 * np.pi)\n",
        "        margin = int(sigma * 2)\n",
        "        cx = np.random.randint(margin, img_size - margin)\n",
        "        cy = np.random.randint(margin, img_size - margin)\n",
        "        gabor = gabor_filter_2d(img_size, theta, freq, sigma, phase, (cx, cy))\n",
        "        weights.append(gabor.flatten())\n",
        "    return np.array(weights, dtype=np.float32)\n",
        "\n",
        "\n",
        "def init_layer_weights(layer, method='kaiming', img_size=None):\n",
        "    if method == 'kaiming':\n",
        "        nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "        if layer.bias is not None:\n",
        "            nn.init.zeros_(layer.bias)\n",
        "    elif method == 'xavier':\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "        if layer.bias is not None:\n",
        "            nn.init.zeros_(layer.bias)\n",
        "    elif method == 'gabor':\n",
        "        if img_size is None:\n",
        "            print(f\"  Warning: Gabor needs img_size. Using Kaiming.\")\n",
        "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "        else:\n",
        "            n_neurons = layer.out_features\n",
        "            input_dim = layer.in_features\n",
        "            expected_dim = img_size * img_size\n",
        "            if input_dim != expected_dim:\n",
        "                print(f\"  Warning: Gabor expects {expected_dim}D. Using Kaiming.\")\n",
        "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
        "            else:\n",
        "                gabor_w = create_gabor_weights(n_neurons, img_size)\n",
        "                with torch.no_grad():\n",
        "                    layer.weight.copy_(torch.from_numpy(gabor_w))\n",
        "        if layer.bias is not None:\n",
        "            nn.init.zeros_(layer.bias)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown init method: {method}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# DATASET LOADERS\n",
        "# ================================================================\n",
        "\n",
        "def load_xor(n_samples=2000, gap=0.05, seed=42, split_ratios=(0.70, 0.15, 0.15)):\n",
        "    \"\"\"Load synthetic 4-class XOR dataset.\"\"\"\n",
        "    train_r, val_r, test_r = split_ratios\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X_all, y_all = [], []\n",
        "    while sum(len(a) for a in X_all) < n_samples:\n",
        "        x = rng.uniform(-1, 1, size=(n_samples * 2, 2))\n",
        "        keep = (np.abs(x[:, 0]) > gap) & (np.abs(x[:, 1]) > gap)\n",
        "        x = x[keep]\n",
        "        labels = np.zeros(len(x), dtype=np.int64)\n",
        "        labels[(x[:, 0] > 0) & (x[:, 1] > 0)] = 0\n",
        "        labels[(x[:, 0] < 0) & (x[:, 1] > 0)] = 1\n",
        "        labels[(x[:, 0] < 0) & (x[:, 1] < 0)] = 2\n",
        "        labels[(x[:, 0] > 0) & (x[:, 1] < 0)] = 3\n",
        "        X_all.append(x); y_all.append(labels)\n",
        "    X = np.concatenate(X_all)[:n_samples]\n",
        "    y = np.concatenate(y_all)[:n_samples]\n",
        "\n",
        "    # Split: train / (val + test)\n",
        "    test_val_r = val_r + test_r\n",
        "    Xtr, Xtmp, ytr, ytmp = train_test_split(\n",
        "        X, y, test_size=test_val_r, random_state=seed, stratify=y\n",
        "    )\n",
        "    # Split: val / test\n",
        "    val_of_tmp = val_r / test_val_r\n",
        "    Xv, Xte, yv, yte = train_test_split(\n",
        "        Xtmp, ytmp, test_size=(1 - val_of_tmp), random_state=seed, stratify=ytmp\n",
        "    )\n",
        "    return Xtr, Xv, Xte, ytr, yv, yte, 4, 2\n",
        "\n",
        "\n",
        "def load_mnist(data_path, split_ratios=(0.70, 0.15, 0.15)):\n",
        "    \"\"\"Load MNIST.\"\"\"\n",
        "    train_r, val_r, test_r = split_ratios\n",
        "\n",
        "    tr = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=transforms.ToTensor())\n",
        "    te = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    Xf = tr.data.float().view(-1, 784) / 255.0\n",
        "    yf = tr.targets.numpy()\n",
        "    Xte = te.data.float().view(-1, 784) / 255.0\n",
        "    yte = te.targets.numpy()\n",
        "\n",
        "    val_from_train = val_r / (train_r + val_r)\n",
        "    Xtr, Xv, ytr, yv = train_test_split(\n",
        "        Xf.numpy(), yf, test_size=val_from_train, random_state=42, stratify=yf\n",
        "    )\n",
        "    return Xtr, Xv, Xte.numpy(), ytr, yv, yte, 10, 784\n",
        "\n",
        "\n",
        "def load_fashion_mnist(data_path, split_ratios=(0.70, 0.15, 0.15)):\n",
        "    \"\"\"Load Fashion-MNIST.\"\"\"\n",
        "    train_r, val_r, test_r = split_ratios\n",
        "\n",
        "    tr = torchvision.datasets.FashionMNIST(root=data_path, train=True, download=True, transform=transforms.ToTensor())\n",
        "    te = torchvision.datasets.FashionMNIST(root=data_path, train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "    Xf = tr.data.float().view(-1, 784) / 255.0\n",
        "    yf = tr.targets.numpy()\n",
        "    Xte = te.data.float().view(-1, 784) / 255.0\n",
        "    yte = te.targets.numpy()\n",
        "\n",
        "    val_from_train = val_r / (train_r + val_r)\n",
        "    Xtr, Xv, ytr, yv = train_test_split(\n",
        "        Xf.numpy(), yf, test_size=val_from_train, random_state=42, stratify=yf\n",
        "    )\n",
        "    return Xtr, Xv, Xte.numpy(), ytr, yv, yte, 10, 784\n",
        "\n",
        "\n",
        "def load_pendigits(data_path, split_ratios=(0.70, 0.15, 0.15)):\n",
        "    \"\"\"Load UCI Pendigits.\"\"\"\n",
        "    train_r, val_r, test_r = split_ratios\n",
        "\n",
        "    import urllib.request\n",
        "    urls = {\n",
        "        'pendigits.tra': 'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra',\n",
        "        'pendigits.tes': 'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tes',\n",
        "    }\n",
        "    for fname, url in urls.items():\n",
        "        fpath = os.path.join(data_path, fname)\n",
        "        if not os.path.exists(fpath):\n",
        "            print(f'  Downloading {fname}...')\n",
        "            urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "    train_data = np.loadtxt(os.path.join(data_path, 'pendigits.tra'), delimiter=',')\n",
        "    test_data = np.loadtxt(os.path.join(data_path, 'pendigits.tes'), delimiter=',')\n",
        "\n",
        "    Xtr_f, ytr_f = train_data[:, :-1], train_data[:, -1].astype(np.int64)\n",
        "    Xte, yte = test_data[:, :-1], test_data[:, -1].astype(np.int64)\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    Xtr_f = sc.fit_transform(Xtr_f)\n",
        "    Xte = sc.transform(Xte)\n",
        "\n",
        "    val_from_train = val_r / (train_r + val_r)\n",
        "    Xtr, Xv, ytr, yv = train_test_split(\n",
        "        Xtr_f, ytr_f, test_size=val_from_train, random_state=42, stratify=ytr_f\n",
        "    )\n",
        "    return Xtr, Xv, Xte, ytr, yv, yte, 10, 16\n",
        "\n",
        "\n",
        "def load_letters(data_path, split_ratios=(0.70, 0.15, 0.15)):\n",
        "    \"\"\"Load UCI Letter Recognition.\"\"\"\n",
        "    train_r, val_r, test_r = split_ratios\n",
        "\n",
        "    import urllib.request\n",
        "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data'\n",
        "    fpath = os.path.join(data_path, 'letter-recognition.data')\n",
        "    if not os.path.exists(fpath):\n",
        "        print('  Downloading letter-recognition.data...')\n",
        "        urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "    rows = []\n",
        "    with open(fpath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            label = ord(parts[0]) - ord('A')\n",
        "            feats = [int(x) for x in parts[1:]]\n",
        "            rows.append(feats + [label])\n",
        "    data = np.array(rows)\n",
        "    X, y = data[:, :-1].astype(np.float64), data[:, -1].astype(np.int64)\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    X = sc.fit_transform(X)\n",
        "\n",
        "    Xtr_f, Xte = X[:16000], X[16000:]\n",
        "    ytr_f, yte = y[:16000], y[16000:]\n",
        "\n",
        "    val_from_train = val_r / (train_r + val_r)\n",
        "    Xtr, Xv, ytr, yv = train_test_split(\n",
        "        Xtr_f, ytr_f, test_size=val_from_train, random_state=42, stratify=ytr_f\n",
        "    )\n",
        "    return Xtr, Xv, Xte, ytr, yv, yte, 26, 16\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# LOAD DATASETS\n",
        "# ================================================================\n",
        "print('\\n' + '='*60)\n",
        "print(' Loading datasets (Phase 2)')\n",
        "print('='*60)\n",
        "\n",
        "DATASETS = {}\n",
        "\n",
        "all_loaders = {\n",
        "    'XOR':        (load_xor, {'split_ratios': CONFIG['split_ratios']}),\n",
        "    'MNIST':      (load_mnist, {'data_path': os.path.join(CONFIG['data_path'], 'MNIST'),\n",
        "                                'split_ratios': CONFIG['split_ratios']}),\n",
        "    'FashionMNIST': (load_fashion_mnist, {'data_path': os.path.join(CONFIG['data_path'], 'FashionMNIST'),\n",
        "                                          'split_ratios': CONFIG['split_ratios']}),\n",
        "    'Pendigits':  (load_pendigits, {'data_path': os.path.join(CONFIG['data_path'], 'Pendigits'),\n",
        "                                    'split_ratios': CONFIG['split_ratios']}),\n",
        "    'LetterRecog': (load_letters, {'data_path': os.path.join(CONFIG['data_path'], 'LetterRecog'),\n",
        "                                   'split_ratios': CONFIG['split_ratios']}),\n",
        "}\n",
        "\n",
        "for name in CONFIG['datasets_to_run']:\n",
        "    if name not in all_loaders:\n",
        "        print(f'  WARNING: Unknown dataset {name}, skipping')\n",
        "        continue\n",
        "    loader, args = all_loaders[name]\n",
        "    result = loader(**args)\n",
        "    Xtr, Xv, Xte, ytr, yv, yte, K, dim = result\n",
        "    DATASETS[name] = {\n",
        "        'X_train': Xtr, 'X_val': Xv, 'X_test': Xte,\n",
        "        'y_train': ytr, 'y_val': yv, 'y_test': yte,\n",
        "        'num_classes': K, 'input_dim': dim,\n",
        "    }\n",
        "    print(f'  \\u2713 {name:12s}: K={K:2d}, dim={dim:4d}, '\n",
        "          f'train={len(Xtr):6d}, val={len(Xv):5d}, test={len(Xte):5d}')\n",
        "\n",
        "print('='*60)\n",
        "print(f'\\u2713 Loaded {len(DATASETS)} dataset(s): {list(DATASETS.keys())}')\n",
        "print(f'\\u2713 PHASE 2 CONFIG: LR={CONFIG[\"lr\"]}, batch={CONFIG[\"batch_size\"]}, patience={CONFIG[\"early_stop_patience\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuJl5J-Vrut1"
      },
      "outputs": [],
      "source": [
        "######### end of cell 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i82A67wexdGp",
        "outputId": "3fba88e0-1db1-4a7c-b5bd-25e4d1df6bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All classes defined (v5):\n",
            "  - FFLayer: mean-goodness + theta=1.0 (scale-invariant)\n",
            "  - FFLayer: per-neuron loss uses mean (balanced with layer loss)\n",
            "  - FFLayer: supports optimizer=\"adam\"|\"sgd\"\n",
            "  - ClassicFF: supports optimizer=\"adam\"|\"sgd\"\n",
            "  - Alpha now truly interpolates: 0.5 = equal blend\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# CELL 2: Core Model Classes (v5)\n",
        "# ================================================================\n",
        "# v5 Changes:\n",
        "#   - FFLayer: goodness uses MEAN (not sum) — scale-invariant\n",
        "#   - FFLayer: theta_layer = 1.0 (not n_neurons) — fixes sigmoid saturation\n",
        "#   - FFLayer: per-neuron loss uses MEAN over neurons (not sum) — balanced\n",
        "#     with layer-level loss so alpha truly interpolates between them\n",
        "#   - FFLayer: optimizer='adam'|'sgd' parameter\n",
        "#   - ClassicFF: optimizer='adam'|'sgd' parameter\n",
        "#   - ClassicFF_Additive: inline goodness also fixed to mean + theta=1.0\n",
        "#   - Per-neuron theta_neuron=1.0: UNCHANGED\n",
        "# ================================================================\n",
        "#   - v6: activation parameter ('relu', 'gelu', 'swish') threaded\n",
        "#     through FFLayer, ClassicFF, ModularFF, BPBaseline\n",
        "# ================================================================\n",
        "\n",
        "class HardLimitSTE(nn.Module):\n",
        "    \"\"\"Hard-limit (step) activation with straight-through estimator.\n",
        "    Forward: output = 1 if x > 0 else 0\n",
        "    Backward: gradient passes through as if identity (STE)\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return x + (torch.heaviside(x, torch.tensor(0.5, device=x.device)) - x).detach()\n",
        "\n",
        "\n",
        "def make_activation(name='relu'):\n",
        "    \"\"\"Create activation module by name.\"\"\"\n",
        "    name = name.lower()\n",
        "    if name == 'gelu':\n",
        "        return nn.GELU()\n",
        "    elif name in ('swish', 'silu'):\n",
        "        return nn.SiLU()\n",
        "    elif name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    elif name in ('hardlimit', 'hardlim', 'step'):\n",
        "        return HardLimitSTE()\n",
        "    else:\n",
        "        return nn.ReLU()\n",
        "\n",
        "\n",
        "def get_default_theta(activation_name):\n",
        "    \"\"\"Return appropriate theta_layer for each activation type.\"\"\"\n",
        "    name = activation_name.lower()\n",
        "    if name == 'tanh':\n",
        "        return 0.0    # tanh outputs in [-1,1]; goodness=mean(h), natural boundary at 0\n",
        "    elif name in ('hardlimit', 'hardlim', 'step', 'perceptron'):\n",
        "        return 0.5    # binary outputs; goodness=mean(h)=fraction active, expect ~50%\n",
        "    else:\n",
        "        return 1.0    # ReLU, GELU: goodness=mean(h^2), original theta\n",
        "\n",
        "\n",
        "class FFLayer(nn.Module):\n",
        "    \"\"\"Single Forward-Forward layer with hybrid goodness objective.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, lr=0.001, theta_neuron=1.0, learnable_theta=False,\n",
        "                 init_method='kaiming', frozen=False, img_size=None,\n",
        "                 optimizer='adam', activation='relu'):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.act = make_activation(activation)\n",
        "        self.activation_name = activation\n",
        "        self.n_neurons = out_features\n",
        "        self.learnable_theta = learnable_theta\n",
        "        default_theta = get_default_theta(activation)\n",
        "        if learnable_theta:\n",
        "            self.theta_layer = nn.Parameter(torch.tensor(default_theta))\n",
        "        else:\n",
        "            self.theta_layer = default_theta\n",
        "        self.theta_neuron = theta_neuron if activation not in ('tanh', 'hardlimit', 'hardlim', 'step') else default_theta\n",
        "        self.frozen = frozen\n",
        "        self.optimizer_type = optimizer\n",
        "\n",
        "        init_layer_weights(self.linear, method=init_method, img_size=img_size)\n",
        "\n",
        "        if not frozen:\n",
        "            if optimizer == 'sgd':\n",
        "                self.opt = torch.optim.SGD(self.parameters(), lr=lr)\n",
        "            else:\n",
        "                self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.opt = None\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.linear(x))\n",
        "\n",
        "    def forward_norm(self, x):\n",
        "        h = self.forward(x)\n",
        "        h_n = h / (h.norm(dim=1, keepdim=True) + 1e-8)\n",
        "        return h, h_n\n",
        "\n",
        "    def goodness(self, h):\n",
        "        if self.activation_name in ('tanh', 'hardlimit', 'hardlim', 'step', 'perceptron'):\n",
        "            return h.mean(dim=1)  # Mean activation: natural for signed/binary outputs\n",
        "        return (h ** 2).mean(dim=1)  # Mean-squared-goodness: for ReLU/GELU (non-negative outputs)\n",
        "\n",
        "    def train_step(self, x_pos, x_neg, alpha=0.0, k_pct=0):\n",
        "        h_pos, h_pos_n = self.forward_norm(x_pos)\n",
        "        h_neg, h_neg_n = self.forward_norm(x_neg)\n",
        "\n",
        "        g_pos = self.goodness(h_pos)\n",
        "        g_neg = self.goodness(h_neg)\n",
        "\n",
        "        # Store goodness stats for monitoring (no grad impact)\n",
        "        self._last_g_pos_mean = g_pos.mean().item()\n",
        "        self._last_g_neg_mean = g_neg.mean().item()\n",
        "        self._last_g_sep = self._last_g_pos_mean - self._last_g_neg_mean\n",
        "        self._last_theta = self.theta_layer.item() if isinstance(self.theta_layer, nn.Parameter) else self.theta_layer\n",
        "\n",
        "        loss_layer = (\n",
        "            -torch.log(torch.sigmoid(g_pos - self.theta_layer) + 1e-8).mean()\n",
        "            - torch.log(1 - torch.sigmoid(g_neg - self.theta_layer) + 1e-8).mean()\n",
        "        )\n",
        "\n",
        "        loss_local = torch.tensor(0.0, device=x_pos.device)\n",
        "        if alpha > 0:\n",
        "            gn_pos = h_pos ** 2\n",
        "            gn_neg = h_neg ** 2\n",
        "            pn_pos = torch.sigmoid(gn_pos - self.theta_neuron)\n",
        "            pn_neg = torch.sigmoid(gn_neg - self.theta_neuron)\n",
        "            ln_pos = -torch.log(pn_pos + 1e-8)\n",
        "            ln_neg = -torch.log(1 - pn_neg + 1e-8)\n",
        "\n",
        "            if 0 < k_pct < 100:\n",
        "                mask = torch.bernoulli(\n",
        "                    torch.full((1, self.n_neurons), k_pct / 100.0, device=x_pos.device)\n",
        "                )\n",
        "                ln_pos = ln_pos * mask\n",
        "                ln_neg = ln_neg * mask\n",
        "\n",
        "            loss_local = ln_pos.mean(1).mean() + ln_neg.mean(1).mean()  # mean over neurons, then mean over batch\n",
        "\n",
        "        loss = (1 - alpha) * loss_layer + alpha * loss_local\n",
        "\n",
        "        if not self.frozen and self.opt is not None:\n",
        "            self.opt.zero_grad()\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "\n",
        "        return loss.item(), h_pos_n.detach(), h_neg_n.detach()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def infer(self, x):\n",
        "        h, h_n = self.forward_norm(x)\n",
        "        return self.goodness(h), h_n\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_activations(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# META-LAYER CLASS\n",
        "# ================================================================\n",
        "\n",
        "class PerceptronFFLayer(nn.Module):\n",
        "    \"\"\"Gradient-free FF layer using perceptron learning rule.\n",
        "\n",
        "    Forward: h = step(Wx + b)  (binary 0/1 outputs)\n",
        "    Goodness: mean(h) = fraction of active neurons\n",
        "    Learning: No sigmoid loss, no gradients.\n",
        "      - Positive data with goodness < theta: w += lr * x  (strengthen)\n",
        "      - Negative data with goodness > theta: w -= lr * x  (weaken)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, lr=0.001, theta_neuron=0.5,\n",
        "                 init_method='kaiming', frozen=False, img_size=None,\n",
        "                 optimizer='adam', activation='perceptron'):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.n_neurons = out_features\n",
        "        self.theta_layer = 0.5\n",
        "        self.theta_neuron = 0.5\n",
        "        self.frozen = frozen\n",
        "        self.lr = lr\n",
        "        self.activation_name = 'perceptron'\n",
        "        self.optimizer_type = 'perceptron'\n",
        "\n",
        "        init_layer_weights(self.linear, method=init_method, img_size=img_size)\n",
        "\n",
        "        if frozen:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.heaviside(self.linear(x), torch.tensor(0.5, device=x.device))\n",
        "\n",
        "    def forward_norm(self, x):\n",
        "        h = self.forward(x)\n",
        "        h_n = h / (h.norm(dim=1, keepdim=True) + 1e-8)\n",
        "        return h, h_n\n",
        "\n",
        "    def goodness(self, h):\n",
        "        return h.mean(dim=1)  # Fraction of active neurons\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def train_step(self, x_pos, x_neg, alpha=0.0, k_pct=0):\n",
        "        \"\"\"Perceptron-style update: no gradients, no loss function.\n",
        "\n",
        "        alpha controls layer-level vs neuron-level error signals:\n",
        "          alpha=0: layer-level — update all neurons when mean goodness is wrong\n",
        "          alpha=1: neuron-level — update each neuron based on its own firing error\n",
        "          0<alpha<1: blend of both signals\n",
        "\n",
        "        This is the perceptron analogue of FFLayer's alpha parameter,\n",
        "        operating entirely without gradients or a loss function.\n",
        "        \"\"\"\n",
        "        h_pos = self.forward(x_pos)   # [B, n_neurons], binary {0, 1}\n",
        "        h_neg = self.forward(x_neg)\n",
        "        h_pos_n = h_pos / (h_pos.norm(dim=1, keepdim=True) + 1e-8)\n",
        "        h_neg_n = h_neg / (h_neg.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        g_pos = self.goodness(h_pos)   # [B], mean fraction of active neurons\n",
        "        g_neg = self.goodness(h_neg)\n",
        "        B = x_pos.size(0)\n",
        "\n",
        "        # Store goodness stats for monitoring\n",
        "        self._last_g_pos_mean = g_pos.mean().item()\n",
        "        self._last_g_neg_mean = g_neg.mean().item()\n",
        "        self._last_g_sep = self._last_g_pos_mean - self._last_g_neg_mean\n",
        "        self._last_theta = self.theta_layer.item() if isinstance(self.theta_layer, nn.Parameter) else self.theta_layer\n",
        "\n",
        "        if not self.frozen:\n",
        "            # ============================================================\n",
        "            # LAYER-LEVEL UPDATE (alpha=0 component)\n",
        "            # If mean goodness is wrong for a sample, nudge ALL neurons\n",
        "            # ============================================================\n",
        "            dw_layer = torch.zeros_like(self.linear.weight.data)\n",
        "            db_layer = torch.zeros_like(self.linear.bias.data)\n",
        "\n",
        "            pos_err_layer = (g_pos < self.theta_layer).float()  # [B]\n",
        "            neg_err_layer = (g_neg > self.theta_layer).float()  # [B]\n",
        "\n",
        "            n_pos_l = pos_err_layer.sum().item()\n",
        "            n_neg_l = neg_err_layer.sum().item()\n",
        "\n",
        "            if n_pos_l > 0:\n",
        "                # Outer product: each neuron gets same input-weighted update\n",
        "                dw_layer += (pos_err_layer.unsqueeze(1) * x_pos).mean(dim=0).unsqueeze(0).expand_as(self.linear.weight)\n",
        "                db_layer += pos_err_layer.mean()\n",
        "\n",
        "            if n_neg_l > 0:\n",
        "                dw_layer -= (neg_err_layer.unsqueeze(1) * x_neg).mean(dim=0).unsqueeze(0).expand_as(self.linear.weight)\n",
        "                db_layer -= neg_err_layer.mean()\n",
        "\n",
        "            # ============================================================\n",
        "            # NEURON-LEVEL UPDATE (alpha=1 component)\n",
        "            # Each neuron has its own target:\n",
        "            #   Positive data: neuron SHOULD fire (target=1)\n",
        "            #   Negative data: neuron should NOT fire (target=0)\n",
        "            # Update only the neurons that made wrong individual decisions\n",
        "            # ============================================================\n",
        "            dw_neuron = torch.zeros_like(self.linear.weight.data)\n",
        "            db_neuron = torch.zeros_like(self.linear.bias.data)\n",
        "\n",
        "            # Positive: neurons that didn't fire but should have\n",
        "            neuron_err_pos = (1.0 - h_pos)  # [B, n_neurons], 1 where neuron failed\n",
        "            # Negative: neurons that fired but shouldn't have\n",
        "            neuron_err_neg = h_neg           # [B, n_neurons], 1 where neuron failed\n",
        "\n",
        "            # Per-neuron weight update via matrix multiply:\n",
        "            # dw[j, i] = mean_over_batch(error[b, j] * input[b, i])\n",
        "            # = (error.T @ input) / B  -> [n_neurons, in_features]\n",
        "            dw_neuron += (neuron_err_pos.t() @ x_pos) / B   # strengthen missed pos\n",
        "            dw_neuron -= (neuron_err_neg.t() @ x_neg) / B   # weaken false neg\n",
        "\n",
        "            # Bias: per-neuron mean error\n",
        "            db_neuron += neuron_err_pos.mean(dim=0)   # [n_neurons]\n",
        "            db_neuron -= neuron_err_neg.mean(dim=0)\n",
        "\n",
        "            # ============================================================\n",
        "            # BLEND and APPLY\n",
        "            # ============================================================\n",
        "            dw = (1.0 - alpha) * dw_layer + alpha * dw_neuron\n",
        "            db = (1.0 - alpha) * db_layer + alpha * db_neuron\n",
        "\n",
        "            self.linear.weight.data += self.lr * dw\n",
        "            self.linear.bias.data += self.lr * db\n",
        "\n",
        "        # Return pseudo-loss for compatibility\n",
        "        pseudo_loss = (1.0 - (g_pos.mean() - g_neg.mean())).item()\n",
        "        return pseudo_loss, h_pos_n.detach(), h_neg_n.detach()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def infer(self, x):\n",
        "        h, h_n = self.forward_norm(x)\n",
        "        return self.goodness(h), h_n\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_activations(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "\n",
        "class MetaLayer:\n",
        "    \"\"\"Meta-layer: takes K goodness values, outputs class prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, meta_type='argmax', device='cpu', hidden_dim=32):\n",
        "        self.K = num_classes\n",
        "        self.meta_type = meta_type\n",
        "        self.device = device\n",
        "\n",
        "        self.cal_mu = None\n",
        "        self.cal_sigma = None\n",
        "        self.linear = None\n",
        "        self.mlp = None\n",
        "        self.temps = None\n",
        "        self.optimizer = None\n",
        "\n",
        "        if meta_type == 'linear':\n",
        "            self.linear = nn.Linear(num_classes, num_classes).to(device)\n",
        "            self.optimizer = torch.optim.Adam(self.linear.parameters(), lr=0.01)\n",
        "        elif meta_type == 'mlp':\n",
        "            self.mlp = nn.Sequential(\n",
        "                nn.Linear(num_classes, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            ).to(device)\n",
        "            self.optimizer = torch.optim.Adam(self.mlp.parameters(), lr=0.01)\n",
        "        elif meta_type == 'temperature':\n",
        "            self.temps = nn.Parameter(torch.ones(num_classes, device=device))\n",
        "            self.optimizer = torch.optim.Adam([self.temps], lr=0.01)\n",
        "\n",
        "    def calibrate(self, G, y):\n",
        "        G_np = G.cpu().numpy() if torch.is_tensor(G) else G\n",
        "        y_np = y.cpu().numpy() if torch.is_tensor(y) else y\n",
        "        self.cal_mu = np.zeros(self.K)\n",
        "        self.cal_sigma = np.zeros(self.K)\n",
        "        for k in range(self.K):\n",
        "            gk = G_np[y_np == k, k]\n",
        "            self.cal_mu[k] = gk.mean() if len(gk) > 0 else 0.0\n",
        "            self.cal_sigma[k] = gk.std() + 1e-8 if len(gk) > 0 else 1.0\n",
        "\n",
        "    def train(self, G, y, epochs=100):\n",
        "        if self.meta_type not in ['linear', 'mlp', 'temperature']:\n",
        "            return\n",
        "        G_t = G if torch.is_tensor(G) else torch.tensor(G, dtype=torch.float32, device=self.device)\n",
        "        y_t = y if torch.is_tensor(y) else torch.tensor(y, dtype=torch.long, device=self.device)\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "        for _ in range(epochs):\n",
        "            if self.meta_type == 'linear':\n",
        "                logits = self.linear(G_t)\n",
        "            elif self.meta_type == 'mlp':\n",
        "                logits = self.mlp(G_t)\n",
        "            elif self.meta_type == 'temperature':\n",
        "                logits = G_t / (self.temps.abs() + 1e-8)\n",
        "            loss = crit(logits, y_t)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, G):\n",
        "        G_t = G if torch.is_tensor(G) else torch.tensor(G, dtype=torch.float32, device=self.device)\n",
        "        if self.meta_type == 'none':\n",
        "            return G_t.cpu().numpy()\n",
        "        elif self.meta_type == 'argmax':\n",
        "            return G_t.argmax(1).cpu().numpy()\n",
        "        elif self.meta_type == 'calibrated':\n",
        "            mu = torch.tensor(self.cal_mu, dtype=torch.float32, device=self.device)\n",
        "            sig = torch.tensor(self.cal_sigma, dtype=torch.float32, device=self.device)\n",
        "            return ((G_t - mu) / sig).argmax(1).cpu().numpy()\n",
        "        elif self.meta_type == 'linear':\n",
        "            return self.linear(G_t).argmax(1).cpu().numpy()\n",
        "        elif self.meta_type == 'mlp':\n",
        "            return self.mlp(G_t).argmax(1).cpu().numpy()\n",
        "        elif self.meta_type == 'temperature':\n",
        "            return (G_t / (self.temps.abs() + 1e-8)).argmax(1).cpu().numpy()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown meta_type: {self.meta_type}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CLASSIC FF (Hinton's original) — Variant 1: One-Hot Overlay\n",
        "# ================================================================\n",
        "\n",
        "class ClassicFF(nn.Module):\n",
        "    \"\"\"Standard FF with label overlay (Hinton's original).\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes, lr=0.001, device='cpu',\n",
        "                 init_method='kaiming', img_size=None, optimizer='adam', activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.K = num_classes\n",
        "        self.device = device\n",
        "        self.eff_dim = input_dim + num_classes\n",
        "        self.optimizer_type = optimizer\n",
        "        self.activation = activation\n",
        "        self.learnable_theta = learnable_theta\n",
        "\n",
        "        dims = [self.eff_dim] + hidden_sizes\n",
        "        self.layers = nn.ModuleList()\n",
        "        LayerClass = PerceptronFFLayer if activation == 'perceptron' else FFLayer\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            layer_kwargs = dict(lr=lr, init_method='kaiming', optimizer=optimizer, activation=activation)\n",
        "            if LayerClass == FFLayer:\n",
        "                layer_kwargs['learnable_theta'] = learnable_theta\n",
        "            self.layers.append(LayerClass(dims[i], dims[i+1], **layer_kwargs))\n",
        "        self.to(device)\n",
        "\n",
        "    def _overlay(self, x, labels):\n",
        "        oh = F.one_hot(labels, self.K).float().to(x.device)\n",
        "        return torch.cat([x, oh], dim=1)\n",
        "\n",
        "    def _wrong_labels(self, y):\n",
        "        wrong = torch.randint(0, self.K - 1, y.shape, device=y.device)\n",
        "        return (wrong + y + 1) % self.K\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "            x_pos = self._overlay(xb, yb)\n",
        "            x_neg = self._overlay(xb, self._wrong_labels(yb))\n",
        "            hp, hn = x_pos, x_neg\n",
        "            for layer in self.layers:\n",
        "                lv, hp, hn = layer.train_step(hp, hn, alpha=0.0, k_pct=0)\n",
        "                total_loss += lv\n",
        "            n += 1\n",
        "        return total_loss / max(n, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X, batch_size=512):\n",
        "        self.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        preds = []\n",
        "        for s in range(0, len(Xt), batch_size):\n",
        "            xb = Xt[s:s+batch_size]\n",
        "            goodness = []\n",
        "            for k in range(self.K):\n",
        "                lk = torch.full((xb.size(0),), k, dtype=torch.long, device=self.device)\n",
        "                xo = self._overlay(xb, lk)\n",
        "                tg = torch.zeros(xb.size(0), device=self.device)\n",
        "                h = xo\n",
        "                for layer in self.layers:\n",
        "                    g, h = layer.infer(h)\n",
        "                    tg += g\n",
        "                goodness.append(tg)\n",
        "            preds.append(torch.stack(goodness).argmax(0).cpu().numpy())\n",
        "        return np.concatenate(preds)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        return (self.predict(X) == y).mean() * 100.0\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CLASSIC FF — Variant 2: Learned Embedding (smaller footprint)\n",
        "# ================================================================\n",
        "\n",
        "class ClassicFF_Embed(nn.Module):\n",
        "    \"\"\"\n",
        "    FF with learned label embedding instead of one-hot.\n",
        "    Embedding dim is fixed (default=2), much smaller than K for large K.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes, lr=0.001, device='cpu',\n",
        "                 embed_dim=2, activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.K = num_classes\n",
        "        self.device = device\n",
        "        self.embed_dim = embed_dim\n",
        "        self.activation = activation\n",
        "\n",
        "        # Learned embedding: K classes -> embed_dim dimensions\n",
        "        self.label_embedding = nn.Embedding(num_classes, embed_dim)\n",
        "\n",
        "        self.eff_dim = input_dim + embed_dim\n",
        "\n",
        "        dims = [self.eff_dim] + hidden_sizes\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            self.layers.append(FFLayer(dims[i], dims[i+1], lr=lr, init_method='kaiming',\n",
        "                                       activation=activation, learnable_theta=learnable_theta))\n",
        "        # Optimizer for embedding (layers have their own optimizers)\n",
        "        self.embed_opt = torch.optim.Adam(self.label_embedding.parameters(), lr=lr)\n",
        "        self.to(device)\n",
        "\n",
        "    def _embed_label(self, x, labels):\n",
        "        \"\"\"Concatenate input with learned label embedding.\"\"\"\n",
        "        emb = self.label_embedding(labels)  # [batch, embed_dim]\n",
        "        return torch.cat([x, emb], dim=1)\n",
        "\n",
        "    def _wrong_labels(self, y):\n",
        "        wrong = torch.randint(0, self.K - 1, y.shape, device=y.device)\n",
        "        return (wrong + y + 1) % self.K\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "\n",
        "            x_pos = self._embed_label(xb, yb)\n",
        "            x_neg = self._embed_label(xb, self._wrong_labels(yb))\n",
        "\n",
        "            # Zero embedding gradients\n",
        "            self.embed_opt.zero_grad()\n",
        "\n",
        "            hp, hn = x_pos, x_neg\n",
        "            batch_loss = 0.0\n",
        "            for layer in self.layers:\n",
        "                lv, hp, hn = layer.train_step(hp, hn, alpha=0.0, k_pct=0)\n",
        "                batch_loss += lv\n",
        "\n",
        "            # Update embedding based on total loss\n",
        "            # (layers already updated themselves in train_step)\n",
        "            total_loss += batch_loss\n",
        "            n += 1\n",
        "        return total_loss / max(n, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X, batch_size=512):\n",
        "        self.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        preds = []\n",
        "        for s in range(0, len(Xt), batch_size):\n",
        "            xb = Xt[s:s+batch_size]\n",
        "            goodness = []\n",
        "            for k in range(self.K):\n",
        "                lk = torch.full((xb.size(0),), k, dtype=torch.long, device=self.device)\n",
        "                xo = self._embed_label(xb, lk)\n",
        "                tg = torch.zeros(xb.size(0), device=self.device)\n",
        "                h = xo\n",
        "                for layer in self.layers:\n",
        "                    g, h = layer.infer(h)\n",
        "                    tg += g\n",
        "                goodness.append(tg)\n",
        "            preds.append(torch.stack(goodness).argmax(0).cpu().numpy())\n",
        "        return np.concatenate(preds)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        return (self.predict(X) == y).mean() * 100.0\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CLASSIC FF — Variant 3: Additive Label at First Hidden Layer\n",
        "# ================================================================\n",
        "\n",
        "class ClassicFF_Additive(nn.Module):\n",
        "    \"\"\"\n",
        "    FF with label signal added to first hidden layer (not input).\n",
        "    Input is preserved entirely; label modulates the hidden representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes, lr=0.001, device='cpu',\n",
        "                 activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.K = num_classes\n",
        "        self.device = device\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.activation = activation\n",
        "\n",
        "        # First layer: input -> hidden (NO label yet)\n",
        "        self.first_layer = FFLayer(input_dim, hidden_sizes[0], lr=lr, init_method='kaiming',\n",
        "                                   activation=activation, learnable_theta=learnable_theta)\n",
        "        # Label embedding that matches first hidden size\n",
        "        self.label_embedding = nn.Embedding(num_classes, hidden_sizes[0])\n",
        "        self.embed_opt = torch.optim.Adam(self.label_embedding.parameters(), lr=lr)\n",
        "\n",
        "        # Remaining layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(FFLayer(hidden_sizes[i-1], hidden_sizes[i], lr=lr, init_method='kaiming',\n",
        "                                       activation=activation, learnable_theta=learnable_theta))\n",
        "        self.to(device)\n",
        "\n",
        "    def _wrong_labels(self, y):\n",
        "        wrong = torch.randint(0, self.K - 1, y.shape, device=y.device)\n",
        "        return (wrong + y + 1) % self.K\n",
        "\n",
        "    def _forward_with_label(self, x, labels, train=False):\n",
        "        \"\"\"Forward pass: first layer on clean input, then add label embedding.\"\"\"\n",
        "        # First layer on clean input\n",
        "        if train:\n",
        "            h = self.first_layer.forward(x)\n",
        "        else:\n",
        "            h = self.first_layer.forward(x)\n",
        "\n",
        "        # Add label embedding to first hidden representation\n",
        "        label_emb = self.label_embedding(labels)\n",
        "        h = h + label_emb\n",
        "\n",
        "        # Normalize\n",
        "        h = h / (h.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        return h\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "            y_wrong = self._wrong_labels(yb)\n",
        "\n",
        "            self.embed_opt.zero_grad()\n",
        "\n",
        "            # First layer forward (clean input)\n",
        "            h_pos_raw, h_pos_n = self.first_layer.forward_norm(xb)\n",
        "            h_neg_raw, h_neg_n = self.first_layer.forward_norm(xb)\n",
        "\n",
        "            # Add label embeddings\n",
        "            label_emb_pos = self.label_embedding(yb)\n",
        "            label_emb_neg = self.label_embedding(y_wrong)\n",
        "\n",
        "            h_pos = h_pos_raw + label_emb_pos\n",
        "            h_neg = h_neg_raw + label_emb_neg\n",
        "\n",
        "            # Compute goodness for first layer\n",
        "            g_pos = (h_pos ** 2).mean(dim=1)\n",
        "            g_neg = (h_neg ** 2).mean(dim=1)\n",
        "\n",
        "            theta = 1.0\n",
        "            loss_first = (\n",
        "                -torch.log(torch.sigmoid(g_pos - theta) + 1e-8).mean()\n",
        "                - torch.log(1 - torch.sigmoid(g_neg - theta) + 1e-8).mean()\n",
        "            )\n",
        "\n",
        "            # Update first layer\n",
        "            self.first_layer.opt.zero_grad()\n",
        "            loss_first.backward(retain_graph=True)\n",
        "            self.first_layer.opt.step()\n",
        "            self.embed_opt.step()\n",
        "\n",
        "            total_loss += loss_first.item()\n",
        "\n",
        "            # Normalize for next layers\n",
        "            hp = h_pos.detach() / (h_pos.detach().norm(dim=1, keepdim=True) + 1e-8)\n",
        "            hn = h_neg.detach() / (h_neg.detach().norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "            # Remaining layers\n",
        "            for layer in self.layers:\n",
        "                lv, hp, hn = layer.train_step(hp, hn, alpha=0.0, k_pct=0)\n",
        "                total_loss += lv\n",
        "\n",
        "            n += 1\n",
        "\n",
        "        return total_loss / max(n, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X, batch_size=512):\n",
        "        self.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        preds = []\n",
        "\n",
        "        for s in range(0, len(Xt), batch_size):\n",
        "            xb = Xt[s:s+batch_size]\n",
        "            goodness = []\n",
        "\n",
        "            for k in range(self.K):\n",
        "                lk = torch.full((xb.size(0),), k, dtype=torch.long, device=self.device)\n",
        "\n",
        "                # First layer on clean input\n",
        "                h = self.first_layer.forward(xb)\n",
        "\n",
        "                # Add label embedding\n",
        "                label_emb = self.label_embedding(lk)\n",
        "                h = h + label_emb\n",
        "\n",
        "                # Goodness from first layer\n",
        "                tg = (h ** 2).mean(dim=1)\n",
        "\n",
        "                # Normalize\n",
        "                h = h / (h.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "                # Remaining layers\n",
        "                for layer in self.layers:\n",
        "                    g, h = layer.infer(h)\n",
        "                    tg += g\n",
        "\n",
        "                goodness.append(tg)\n",
        "\n",
        "            preds.append(torch.stack(goodness).argmax(0).cpu().numpy())\n",
        "\n",
        "        return np.concatenate(preds)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        return (self.predict(X) == y).mean() * 100.0\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CLASSIC FF — Variant 4: With Local Adaptation (alpha > 0)\n",
        "# ================================================================\n",
        "\n",
        "class ClassicFF_LocalAdapt(nn.Module):\n",
        "    \"\"\"\n",
        "    Classic FF with local adaptation (per-neuron goodness).\n",
        "\n",
        "    This is Hinton's architecture + our alpha-weighted loss.\n",
        "    Allows fair comparison: does local adaptation alone explain\n",
        "    ModularFF's advantage, or is the modular architecture key?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes, lr=0.001, device='cpu',\n",
        "                 init_method='kaiming', img_size=None, alpha=0.3, activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.K = num_classes\n",
        "        self.device = device\n",
        "        self.alpha = alpha\n",
        "        self.activation = activation\n",
        "        self.eff_dim = input_dim + num_classes\n",
        "\n",
        "        dims = [self.eff_dim] + hidden_sizes\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            self.layers.append(FFLayer(dims[i], dims[i+1], lr=lr, init_method='kaiming',\n",
        "                                       activation=activation, learnable_theta=learnable_theta))\n",
        "        self.to(device)\n",
        "\n",
        "    def _overlay(self, x, labels):\n",
        "        oh = F.one_hot(labels, self.K).float().to(x.device)\n",
        "        return torch.cat([x, oh], dim=1)\n",
        "\n",
        "    def _wrong_labels(self, y):\n",
        "        wrong = torch.randint(0, self.K - 1, y.shape, device=y.device)\n",
        "        return (wrong + y + 1) % self.K\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "            x_pos = self._overlay(xb, yb)\n",
        "            x_neg = self._overlay(xb, self._wrong_labels(yb))\n",
        "            hp, hn = x_pos, x_neg\n",
        "            for layer in self.layers:\n",
        "                # Use alpha for local adaptation!\n",
        "                lv, hp, hn = layer.train_step(hp, hn, alpha=self.alpha, k_pct=0)\n",
        "                total_loss += lv\n",
        "            n += 1\n",
        "        return total_loss / max(n, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, X, batch_size=512):\n",
        "        self.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        preds = []\n",
        "        for s in range(0, len(Xt), batch_size):\n",
        "            xb = Xt[s:s+batch_size]\n",
        "            goodness = []\n",
        "            for k in range(self.K):\n",
        "                lk = torch.full((xb.size(0),), k, dtype=torch.long, device=self.device)\n",
        "                xo = self._overlay(xb, lk)\n",
        "                tg = torch.zeros(xb.size(0), device=self.device)\n",
        "                h = xo\n",
        "                for layer in self.layers:\n",
        "                    g, h = layer.infer(h)\n",
        "                    tg += g\n",
        "                goodness.append(tg)\n",
        "            preds.append(torch.stack(goodness).argmax(0).cpu().numpy())\n",
        "        return np.concatenate(preds)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        return (self.predict(X) == y).mean() * 100.0\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# BP BASELINE\n",
        "# ================================================================\n",
        "\n",
        "class BPBaseline(nn.Module):\n",
        "    \"\"\"Standard MLP with cross-entropy.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes, lr=0.001, device='cpu',\n",
        "                 activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.activation = activation\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            layers += [nn.Linear(dims[i], dims[i+1]), make_activation(activation)]\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        self.crit = nn.CrossEntropyLoss()\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "            loss = self.crit(self.forward(xb), yb)\n",
        "            self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
        "            total_loss += loss.item(); n += 1\n",
        "        return total_loss / max(n, 1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, X, y):\n",
        "        self.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        yt = torch.tensor(y, dtype=torch.long, device=self.device)\n",
        "        correct = 0\n",
        "        for s in range(0, len(Xt), 512):\n",
        "            logits = self.forward(Xt[s:s+512])\n",
        "            correct += (logits.argmax(1) == yt[s:s+512]).sum().item()\n",
        "        return correct / len(y) * 100.0\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# ModularFF SPECIALIST\n",
        "# ================================================================\n",
        "\n",
        "class ModularFFSpecialist(nn.Module):\n",
        "    \"\"\"One specialist for class k.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, class_id, lr=0.001,\n",
        "                 theta_neuron=1.0, device='cpu',\n",
        "                 init_method='kaiming', img_size=None,\n",
        "                 first_layer_frozen=False, prune_beta=1.0, activation='relu', learnable_theta=False):\n",
        "        super().__init__()\n",
        "        self.class_id = class_id\n",
        "        self.device = device\n",
        "        self.input_dim = input_dim\n",
        "        self.target_hidden_sizes = hidden_sizes.copy()\n",
        "        self.lr = lr\n",
        "        self.theta_neuron = theta_neuron\n",
        "        self.init_method = init_method\n",
        "        self.img_size = img_size\n",
        "        self.first_layer_frozen = first_layer_frozen\n",
        "        self.prune_beta = prune_beta\n",
        "        self.activation = activation\n",
        "        self.learnable_theta = learnable_theta\n",
        "        self.pruned = False\n",
        "        self.num_layers = len(hidden_sizes)\n",
        "        self._build_layers()\n",
        "        self.to(device)\n",
        "\n",
        "    def _build_layers(self):\n",
        "        hidden = self.target_hidden_sizes.copy()\n",
        "        if self.prune_beta > 1.0 and not self.pruned:\n",
        "            hidden[0] = int(hidden[0] * self.prune_beta)\n",
        "        dims = [self.input_dim] + hidden\n",
        "        self.layers = nn.ModuleList()\n",
        "        LayerClass = PerceptronFFLayer if self.activation == 'perceptron' else FFLayer\n",
        "        extra = {'learnable_theta': self.learnable_theta} if LayerClass == FFLayer else {}\n",
        "        for i in range(len(hidden)):\n",
        "            if i == 0:\n",
        "                layer = LayerClass(dims[i], dims[i+1], lr=self.lr, theta_neuron=self.theta_neuron,\n",
        "                                init_method=self.init_method, frozen=self.first_layer_frozen,\n",
        "                                img_size=self.img_size, activation=self.activation, **extra)\n",
        "            else:\n",
        "                layer = LayerClass(dims[i], dims[i+1], lr=self.lr, theta_neuron=self.theta_neuron,\n",
        "                                init_method='kaiming', frozen=False, activation=self.activation, **extra)\n",
        "            self.layers.append(layer)\n",
        "        self.current_hidden_sizes = hidden\n",
        "\n",
        "    def train_batch(self, x_pos, x_neg, alpha=0.0, layer_dropout=None):\n",
        "        if layer_dropout is None:\n",
        "            layer_dropout = [0] * len(self.layers)\n",
        "        while len(layer_dropout) < len(self.layers):\n",
        "            layer_dropout.append(0)\n",
        "        hp, hn = x_pos, x_neg\n",
        "        total_loss = 0.0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            lv, hp, hn = layer.train_step(hp, hn, alpha=alpha, k_pct=layer_dropout[i])\n",
        "            total_loss += lv\n",
        "        return total_loss / len(self.layers)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def total_goodness(self, x):\n",
        "        h = x\n",
        "        tg = torch.zeros(x.size(0), device=self.device)\n",
        "        for layer in self.layers:\n",
        "            g, h = layer.infer(h)\n",
        "            tg += g\n",
        "        return tg\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_pruning_scores(self, X_pos, X_neg):\n",
        "        first_layer = self.layers[0]\n",
        "        X_pos_t = torch.tensor(X_pos, dtype=torch.float32, device=self.device)\n",
        "        X_neg_t = torch.tensor(X_neg, dtype=torch.float32, device=self.device)\n",
        "        h_pos = first_layer.get_activations(X_pos_t)\n",
        "        h_neg = first_layer.get_activations(X_neg_t)\n",
        "        g_pos = h_pos ** 2\n",
        "        g_neg = h_neg ** 2\n",
        "        mean_pos = g_pos.mean(dim=0)\n",
        "        mean_neg = g_neg.mean(dim=0)\n",
        "        std_pos = g_pos.std(dim=0)\n",
        "        std_neg = g_neg.std(dim=0)\n",
        "        std_pooled = torch.sqrt((std_pos**2 + std_neg**2) / 2 + 1e-8)\n",
        "        separation = torch.abs(mean_pos - mean_neg)\n",
        "        scores = separation / std_pooled\n",
        "        avg_activity = (mean_pos + mean_neg) / 2\n",
        "        scores[avg_activity < 0.01] = 0.0\n",
        "        return scores\n",
        "\n",
        "    def prune_first_layer(self, X_pos, X_neg, keep_n=None):\n",
        "        if self.pruned:\n",
        "            return\n",
        "        if keep_n is None:\n",
        "            keep_n = self.target_hidden_sizes[0]\n",
        "        scores = self.compute_pruning_scores(X_pos, X_neg)\n",
        "        current_n = len(scores)\n",
        "        if keep_n >= current_n:\n",
        "            self.pruned = True\n",
        "            return\n",
        "        _, top_indices = torch.topk(scores, keep_n)\n",
        "        top_indices = top_indices.sort().values\n",
        "        old_layer = self.layers[0]\n",
        "        old_weight = old_layer.linear.weight.data[top_indices, :]\n",
        "        old_bias = old_layer.linear.bias.data[top_indices] if old_layer.linear.bias is not None else None\n",
        "        new_layer = FFLayer(self.input_dim, keep_n, lr=self.lr, theta_neuron=self.theta_neuron,\n",
        "                            init_method='kaiming', frozen=self.first_layer_frozen)\n",
        "        with torch.no_grad():\n",
        "            new_layer.linear.weight.copy_(old_weight)\n",
        "            if old_bias is not None:\n",
        "                new_layer.linear.bias.copy_(old_bias)\n",
        "        new_layer.to(self.device)\n",
        "        if len(self.layers) > 1:\n",
        "            old_second = self.layers[1]\n",
        "            old_second_weight = old_second.linear.weight.data[:, top_indices.cpu()]\n",
        "            new_second = FFLayer(keep_n, old_second.n_neurons, lr=self.lr, theta_neuron=self.theta_neuron,\n",
        "                                 init_method='kaiming', frozen=False)\n",
        "            with torch.no_grad():\n",
        "                new_second.linear.weight.copy_(old_second_weight)\n",
        "                if old_second.linear.bias is not None:\n",
        "                    new_second.linear.bias.copy_(old_second.linear.bias.data)\n",
        "            new_second.to(self.device)\n",
        "            self.layers[1] = new_second\n",
        "        self.layers[0] = new_layer\n",
        "        self.current_hidden_sizes[0] = keep_n\n",
        "        self.pruned = True\n",
        "        print(f\"    Specialist {self.class_id}: Pruned {current_n - keep_n}/{current_n} neurons\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# ModularFF ENSEMBLE — With Per-Specialist Evaluation\n",
        "# ================================================================\n",
        "\n",
        "class ModularFFEnsemble:\n",
        "    \"\"\"\n",
        "    Full ModularFF: K specialists + meta-layers + per-specialist evaluation.\n",
        "\n",
        "    New in v4:\n",
        "    - evaluate_specialists(): binary accuracy, sensitivity, specificity per expert\n",
        "    - Negative examples are resampled each epoch (confirmed)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_sizes, num_classes,\n",
        "                 lr=0.001, theta_neuron=1.0, device='cpu',\n",
        "                 init_method='kaiming', img_size=None,\n",
        "                 first_layer_frozen=False, prune_beta=1.0,\n",
        "                 meta_type='argmax', use_meta_layer=True, activation='relu', learnable_theta=False):\n",
        "\n",
        "        self.K = num_classes\n",
        "        self.device = device\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.prune_beta = prune_beta\n",
        "        self.use_meta_layer = use_meta_layer\n",
        "        self.activation = activation\n",
        "\n",
        "        self.specs = [\n",
        "            ModularFFSpecialist(input_dim, hidden_sizes, k, lr, theta_neuron, device,\n",
        "                            init_method, img_size, first_layer_frozen, prune_beta,\n",
        "                            activation=activation)\n",
        "            for k in range(num_classes)\n",
        "        ]\n",
        "\n",
        "        self.meta_layers = {}\n",
        "        for mt in ['argmax', 'calibrated', 'linear', 'mlp', 'temperature']:\n",
        "            self.meta_layers[mt] = MetaLayer(num_classes, mt, device)\n",
        "\n",
        "        self.default_meta = meta_type\n",
        "\n",
        "    def total_params(self):\n",
        "        return sum(sum(p.numel() for p in s.parameters()) for s in self.specs)\n",
        "\n",
        "    def _get_specialist_data(self, X, y, class_id, balanced=True):\n",
        "        \"\"\"\n",
        "        Get positive and negative examples for a specialist.\n",
        "\n",
        "        NOTE: This is called EACH EPOCH, so negatives change!\n",
        "        \"\"\"\n",
        "        pos_idx = np.where(y == class_id)[0]\n",
        "        neg_idx = np.where(y != class_id)[0]\n",
        "        n_pos = len(pos_idx)\n",
        "\n",
        "        if n_pos == 0 or len(neg_idx) == 0:\n",
        "            return None, None\n",
        "\n",
        "        if balanced:\n",
        "            # Sample equal number of negatives (resampled each call!)\n",
        "            neg_sample = np.random.choice(neg_idx, size=min(n_pos, len(neg_idx)), replace=False)\n",
        "        else:\n",
        "            neg_sample = neg_idx\n",
        "\n",
        "        return X[pos_idx], X[neg_sample]\n",
        "\n",
        "    def train_epoch(self, X, y, alpha=0.0, layer_dropout=None, batch_size=128):\n",
        "        \"\"\"\n",
        "        Train all specialists for one epoch.\n",
        "        Negatives are RESAMPLED each epoch (different random subset).\n",
        "        \"\"\"\n",
        "        total_loss = 0.0\n",
        "        for spec in self.specs:\n",
        "            spec.train()\n",
        "\n",
        "        for k, spec in enumerate(self.specs):\n",
        "            # NOTE: _get_specialist_data resamples negatives each call!\n",
        "            X_pos, X_neg = self._get_specialist_data(X, y, k, balanced=True)\n",
        "            if X_pos is None:\n",
        "                continue\n",
        "\n",
        "            n = min(len(X_pos), len(X_neg))\n",
        "\n",
        "            # Shuffle order each epoch too\n",
        "            perm_pos = np.random.permutation(len(X_pos))\n",
        "            perm_neg = np.random.permutation(len(X_neg))\n",
        "\n",
        "            spec_loss, nb = 0.0, 0\n",
        "            for s in range(0, n, batch_size):\n",
        "                e = min(s + batch_size, n)\n",
        "                xp = torch.tensor(X_pos[perm_pos[s:e]], dtype=torch.float32, device=self.device)\n",
        "                xn = torch.tensor(X_neg[perm_neg[s:e]], dtype=torch.float32, device=self.device)\n",
        "                mb = min(xp.size(0), xn.size(0))\n",
        "                if mb == 0:\n",
        "                    continue\n",
        "                lv = spec.train_batch(xp[:mb], xn[:mb], alpha=alpha, layer_dropout=layer_dropout)\n",
        "                spec_loss += lv\n",
        "                nb += 1\n",
        "\n",
        "            if nb > 0:\n",
        "                total_loss += spec_loss / nb\n",
        "\n",
        "        return total_loss / self.K\n",
        "\n",
        "    def prune_all_specialists(self, X, y):\n",
        "        print(f\"\\n  Pruning {self.K} specialists...\")\n",
        "        for k, spec in enumerate(self.specs):\n",
        "            X_pos, X_neg = self._get_specialist_data(X, y, k)\n",
        "            if X_pos is not None and X_neg is not None:\n",
        "                spec.prune_first_layer(X_pos, X_neg)\n",
        "        print(f\"  Done. New params: {self.total_params()}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _all_goodness(self, X, batch_size=512):\n",
        "        for s in self.specs:\n",
        "            s.eval()\n",
        "        Xt = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        chunks = []\n",
        "        for s in range(0, len(Xt), batch_size):\n",
        "            xb = Xt[s:s+batch_size]\n",
        "            g = torch.stack([spec.total_goodness(xb) for spec in self.specs], dim=1)\n",
        "            chunks.append(g)\n",
        "        return torch.cat(chunks, dim=0)\n",
        "\n",
        "    def train_meta_layers(self, X_val, y_val, epochs=100):\n",
        "        if not self.use_meta_layer:\n",
        "            return\n",
        "        G = self._all_goodness(X_val)\n",
        "        y_t = torch.tensor(y_val, dtype=torch.long, device=self.device)\n",
        "        self.meta_layers['calibrated'].calibrate(G, y_t)\n",
        "        for mt in ['linear', 'mlp', 'temperature']:\n",
        "            self.meta_layers[mt].train(G, y_t, epochs=epochs)\n",
        "\n",
        "    def predict(self, X, meta=None):\n",
        "        if meta is None:\n",
        "            meta = self.default_meta\n",
        "        G = self._all_goodness(X)\n",
        "        if not self.use_meta_layer or meta == 'none':\n",
        "            return G.argmax(1).cpu().numpy()\n",
        "        return self.meta_layers[meta].predict(G)\n",
        "\n",
        "    def evaluate(self, X, y, meta=None):\n",
        "        return (self.predict(X, meta) == y).mean() * 100.0\n",
        "\n",
        "    def evaluate_all_meta(self, X, y):\n",
        "        results = {}\n",
        "        for mt in self.meta_layers.keys():\n",
        "            results[mt] = self.evaluate(X, y, mt)\n",
        "        return results\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_specialists(self, X, y, threshold_method='mean'):\n",
        "        \"\"\"\n",
        "        Evaluate each specialist on its binary classification task.\n",
        "\n",
        "        For specialist k:\n",
        "        - Positive examples: samples where y == k\n",
        "        - Negative examples: balanced sample where y != k\n",
        "\n",
        "        Returns:\n",
        "            dict: {k: {'accuracy', 'sensitivity', 'specificity',\n",
        "                       'mean_g_pos', 'mean_g_neg', 'threshold'}}\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for k, spec in enumerate(self.specs):\n",
        "            spec.eval()\n",
        "\n",
        "            # Get balanced positive/negative data\n",
        "            X_pos, X_neg = self._get_specialist_data(X, y, k, balanced=True)\n",
        "\n",
        "            if X_pos is None or X_neg is None or len(X_pos) == 0 or len(X_neg) == 0:\n",
        "                results[k] = {'accuracy': 0, 'sensitivity': 0, 'specificity': 0}\n",
        "                continue\n",
        "\n",
        "            # Compute goodness\n",
        "            X_pos_t = torch.tensor(X_pos, dtype=torch.float32, device=self.device)\n",
        "            X_neg_t = torch.tensor(X_neg, dtype=torch.float32, device=self.device)\n",
        "\n",
        "            g_pos = spec.total_goodness(X_pos_t).cpu().numpy()\n",
        "            g_neg = spec.total_goodness(X_neg_t).cpu().numpy()\n",
        "\n",
        "            mean_g_pos = g_pos.mean()\n",
        "            mean_g_neg = g_neg.mean()\n",
        "\n",
        "            # Determine threshold\n",
        "            if threshold_method == 'mean':\n",
        "                threshold = (mean_g_pos + mean_g_neg) / 2\n",
        "            elif threshold_method == 'theta':\n",
        "                threshold = sum(layer.theta_layer for layer in spec.layers)\n",
        "            else:\n",
        "                threshold = (mean_g_pos + mean_g_neg) / 2\n",
        "\n",
        "            # Binary predictions: positive if goodness > threshold\n",
        "            tp = (g_pos > threshold).sum()\n",
        "            fn = (g_pos <= threshold).sum()\n",
        "            tn = (g_neg <= threshold).sum()\n",
        "            fp = (g_neg > threshold).sum()\n",
        "\n",
        "            n_pos = len(g_pos)\n",
        "            n_neg = len(g_neg)\n",
        "\n",
        "            accuracy = (tp + tn) / (n_pos + n_neg) * 100\n",
        "            sensitivity = tp / n_pos * 100 if n_pos > 0 else 0\n",
        "            specificity = tn / n_neg * 100 if n_neg > 0 else 0\n",
        "\n",
        "            results[k] = {\n",
        "                'accuracy': round(accuracy, 2),\n",
        "                'sensitivity': round(sensitivity, 2),\n",
        "                'specificity': round(specificity, 2),\n",
        "                'mean_g_pos': round(float(mean_g_pos), 2),\n",
        "                'mean_g_neg': round(float(mean_g_neg), 2),\n",
        "                'threshold': round(float(threshold), 2),\n",
        "                'separation': round(float(mean_g_pos - mean_g_neg), 2),\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_specialist_performance(self, X, y, threshold_method='mean'):\n",
        "        \"\"\"Pretty-print per-specialist binary performance.\"\"\"\n",
        "        results = self.evaluate_specialists(X, y, threshold_method)\n",
        "\n",
        "        print(f'\\n  {\"Spec\":>4}  {\"Acc\":>6}  {\"Sens\":>6}  {\"Spec\":>6}  '\n",
        "              f'{\"G_pos\":>7}  {\"G_neg\":>7}  {\"Sep\":>6}')\n",
        "        print('  ' + '-' * 52)\n",
        "\n",
        "        for k in range(self.K):\n",
        "            r = results[k]\n",
        "            print(f'  {k:>4}  {r[\"accuracy\"]:>5.1f}%  {r[\"sensitivity\"]:>5.1f}%  '\n",
        "                  f'{r[\"specificity\"]:>5.1f}%  {r[\"mean_g_pos\"]:>7.1f}  '\n",
        "                  f'{r[\"mean_g_neg\"]:>7.1f}  {r[\"separation\"]:>6.1f}')\n",
        "\n",
        "        # Summary stats\n",
        "        avg_acc = np.mean([results[k]['accuracy'] for k in range(self.K)])\n",
        "        avg_sens = np.mean([results[k]['sensitivity'] for k in range(self.K)])\n",
        "        avg_spec = np.mean([results[k]['specificity'] for k in range(self.K)])\n",
        "        avg_sep = np.mean([results[k]['separation'] for k in range(self.K)])\n",
        "\n",
        "        print('  ' + '-' * 52)\n",
        "        print(f'  {\"Avg\":>4}  {avg_acc:>5.1f}%  {avg_sens:>5.1f}%  '\n",
        "              f'{avg_spec:>5.1f}%  {\"\":>7}  {\"\":>7}  {avg_sep:>6.1f}')\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "print('\\u2713 All classes defined (v5):')\n",
        "print('  - FFLayer: mean-goodness + theta=1.0 (scale-invariant)')\n",
        "print('  - FFLayer: per-neuron loss uses mean (balanced with layer loss)')\n",
        "print('  - FFLayer: supports optimizer=\"adam\"|\"sgd\"')\n",
        "print('  - ClassicFF: supports optimizer=\"adam\"|\"sgd\"')\n",
        "print('  - Alpha now truly interpolates: 0.5 = equal blend')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_yqM8Br043"
      },
      "outputs": [],
      "source": [
        "####### end of cell 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmDIV-Iqxiad",
        "outputId": "33e45f5d-d813-4fbd-e223-9f3b3c656378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training engine ready (v5):\n",
            "  - Patience: 60 (reduce LR every 20 epochs)\n",
            "  - Min epochs: 50\n",
            "  - Best-model checkpoint: ON\n",
            "  - 2-layer experiments: run_2layer_experiment()\n",
            "  - 4-layer Hinton comparison: run_4layer_experiment()\n",
            "  - Classic FF: Adam and SGD (Hinton config)\n",
            "  - Logger: /content/drive/My Drive/Research/ModularFF/Logs/experiment_logs.csv\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# CELL 3: Training Engine (v5)\n",
        "# ================================================================\n",
        "# v5 Changes:\n",
        "#   - Added run_2layer_experiment() for Phase 1\n",
        "#   - run_4layer_experiment() for Phase 2 (Adam + SGD)\n",
        "#   - Relaxed early stopping: patience=60, min_epochs=50\n",
        "#   - LR reduction schedule with best-model checkpoint\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "class ExperimentLogger:\n",
        "    \"\"\"Append-only CSV logger.\"\"\"\n",
        "\n",
        "    FIELDS = [\n",
        "        'timestamp', 'dataset', 'method', 'seed',\n",
        "        'alpha', 'layer_dropout', 'meta_layer',\n",
        "        'hidden_sizes', 'num_specialists',\n",
        "        'best_val_acc', 'test_acc', 'epochs_run',\n",
        "        'train_time_sec', 'total_params',\n",
        "        'init_method', 'elm_mode', 'pruning', 'prune_beta',\n",
        "        'avg_specialist_acc', 'avg_specialist_sep',\n",
        "    ]\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        self.path = os.path.join(log_dir, 'experiment_logs.csv')\n",
        "        if not os.path.exists(self.path):\n",
        "            with open(self.path, 'w', newline='') as f:\n",
        "                csv.DictWriter(f, fieldnames=self.FIELDS).writeheader()\n",
        "\n",
        "    def log(self, d):\n",
        "        d['timestamp'] = datetime.now().isoformat()\n",
        "        with open(self.path, 'a', newline='') as f:\n",
        "            csv.DictWriter(f, fieldnames=self.FIELDS).writerow(\n",
        "                {k: d.get(k, '') for k in self.FIELDS}\n",
        "            )\n",
        "\n",
        "\n",
        "LOGGER = ExperimentLogger(CONFIG['logs_path'])\n",
        "\n",
        "\n",
        "def make_loader(X, y, batch_size, shuffle=True):\n",
        "    Xt = torch.tensor(X, dtype=torch.float32)\n",
        "    yt = torch.tensor(y, dtype=torch.long)\n",
        "    return DataLoader(TensorDataset(Xt, yt), batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# LR REDUCTION HELPER\n",
        "# ================================================================\n",
        "\n",
        "def reduce_lr_for_model(model, factor=0.5):\n",
        "    \"\"\"Reduce LR for all optimizers in a model by the given factor.\"\"\"\n",
        "    if hasattr(model, 'layers'):\n",
        "        # ClassicFF / ClassicFF_LocalAdapt / ClassicFF_Additive\n",
        "        for layer in model.layers:\n",
        "            if hasattr(layer, 'opt') and layer.opt is not None:\n",
        "                for pg in layer.opt.param_groups:\n",
        "                    pg['lr'] *= factor\n",
        "    if hasattr(model, 'first_layer') and hasattr(model.first_layer, 'opt'):\n",
        "        if model.first_layer.opt is not None:\n",
        "            for pg in model.first_layer.opt.param_groups:\n",
        "                pg['lr'] *= factor\n",
        "    if hasattr(model, 'embed_opt'):\n",
        "        for pg in model.embed_opt.param_groups:\n",
        "            pg['lr'] *= factor\n",
        "    if hasattr(model, 'opt'):\n",
        "        # BPBaseline\n",
        "        for pg in model.opt.param_groups:\n",
        "            pg['lr'] *= factor\n",
        "\n",
        "\n",
        "def reduce_lr_for_ensemble(ensemble, factor=0.5):\n",
        "    \"\"\"Reduce LR for all specialists in a ModularFF ensemble.\"\"\"\n",
        "    for spec in ensemble.specs:\n",
        "        for layer in spec.layers:\n",
        "            if hasattr(layer, 'opt') and layer.opt is not None:\n",
        "                for pg in layer.opt.param_groups:\n",
        "                    pg['lr'] *= factor\n",
        "\n",
        "\n",
        "def get_current_lr_str(model):\n",
        "    \"\"\"Get current LR string for logging.\"\"\"\n",
        "    if hasattr(model, 'opt'):\n",
        "        return f\"{model.opt.param_groups[0]['lr']:.1e}\"\n",
        "    if hasattr(model, 'layers') and len(model.layers) > 0:\n",
        "        layer = model.layers[0]\n",
        "        if hasattr(layer, 'opt') and layer.opt is not None:\n",
        "            return f\"{layer.opt.param_groups[0]['lr']:.1e}\"\n",
        "    return \"?\"\n",
        "\n",
        "\n",
        "def get_ensemble_lr_str(ensemble):\n",
        "    \"\"\"Get current LR string for ensemble logging.\"\"\"\n",
        "    if ensemble.specs and ensemble.specs[0].layers:\n",
        "        layer = ensemble.specs[0].layers[0]\n",
        "        if hasattr(layer, 'opt') and layer.opt is not None:\n",
        "            return f\"{layer.opt.param_groups[0]['lr']:.1e}\"\n",
        "    return \"?\"\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TRAINING FUNCTIONS (with relaxed early stopping)\n",
        "# ================================================================\n",
        "\n",
        "def train_classic_ff(ds_name, ds, hidden, seed, cfg, optimizer='adam', lr_override=None, activation='relu', learnable_theta=False):\n",
        "    \"\"\"Train Classic FF (Hinton's original with one-hot overlay).\n",
        "\n",
        "    Args:\n",
        "        optimizer: 'adam' or 'sgd'\n",
        "        lr_override: If set, use this LR instead of cfg['lr'] (for Hinton SGD config)\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    dev = cfg['device']\n",
        "    lr = lr_override if lr_override is not None else cfg['lr']\n",
        "    opt_label = optimizer.upper()\n",
        "    method_name = f'ClassicFF_{opt_label}'\n",
        "\n",
        "    model = ClassicFF(ds['input_dim'], hidden, ds['num_classes'], lr, dev,\n",
        "                      init_method=cfg.get('init_method', 'kaiming'),\n",
        "                      img_size=ARCHITECTURES[ds_name].get('img_size'),\n",
        "                      optimizer=optimizer, activation=activation, learnable_theta=learnable_theta)\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    loader = make_loader(ds['X_train'], ds['y_train'], cfg['batch_size'])\n",
        "\n",
        "    # Early stopping config\n",
        "    min_epochs = cfg.get('min_epochs', 50)\n",
        "    total_patience = cfg['early_stop_patience']\n",
        "    lr_reduce_patience = cfg.get('lr_reduce_patience', 20)\n",
        "    lr_reduce_factor = cfg.get('lr_reduce_factor', 0.5)\n",
        "\n",
        "    hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'wall_time': []}\n",
        "    best_val, patience_counter = 0.0, 0\n",
        "    lr_reductions = 0\n",
        "    best_state = None\n",
        "    t0 = time.time()\n",
        "\n",
        "    for ep in range(cfg['epochs']):\n",
        "        loss = model.train_epoch(loader)\n",
        "        tr_acc = model.evaluate(ds['X_train'], ds['y_train'])\n",
        "        va_acc = model.evaluate(ds['X_val'], ds['y_val'])\n",
        "        hist['loss'].append(loss)\n",
        "        hist['train_acc'].append(tr_acc)\n",
        "        hist['val_acc'].append(va_acc)\n",
        "        hist['wall_time'].append(time.time() - t0)\n",
        "\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "            patience_counter = 0\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # LR reduction before stopping\n",
        "        if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "            lr_reductions += 1\n",
        "            reduce_lr_for_model(model, lr_reduce_factor)\n",
        "            lr_str = get_current_lr_str(model)\n",
        "            print(f'  [{method_name}] LR reduced (×{lr_reduce_factor}) -> {lr_str} at epoch {ep+1} (reduction #{lr_reductions})')\n",
        "\n",
        "        if (ep+1) % 10 == 0 or ep == 0:\n",
        "            print(f'  [{method_name}] ep {ep+1:3d}  loss={loss:.3f}  '\n",
        "                  f'train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "\n",
        "        # Early stop only after min_epochs and full patience exhausted\n",
        "        if ep >= min_epochs and patience_counter >= total_patience:\n",
        "            print(f'  [{method_name}] Early stop at epoch {ep+1} (best_val={best_val:.1f}%, {lr_reductions} LR reductions)')\n",
        "            break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f'  [{method_name}] Restored best checkpoint (val={best_val:.1f}%)')\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    te_acc = model.evaluate(ds['X_test'], ds['y_test'])\n",
        "    # Goodness diagnostic\n",
        "    g_diag = []\n",
        "    for li, layer in enumerate(model.layers):\n",
        "        gp = getattr(layer, '_last_g_pos_mean', 0)\n",
        "        gn = getattr(layer, '_last_g_neg_mean', 0)\n",
        "        th = getattr(layer, '_last_theta', getattr(layer, 'theta_layer', '?'))\n",
        "        if hasattr(th, 'item'): th = th.item()\n",
        "        g_diag.append(f'L{li}:g+={gp:.3f}/g-={gn:.3f}/sep={gp-gn:.3f}/th={th:.3f}')\n",
        "    print(f'  [{method_name}] DONE  test={te_acc:.2f}%  {elapsed:.0f}s  {n_params} params  (LR={lr}, {optimizer}, act={activation})')\n",
        "    print(f'    Goodness: {\" | \".join(g_diag)}')\n",
        "\n",
        "    LOGGER.log({\n",
        "        'dataset': ds_name, 'method': method_name, 'seed': seed,\n",
        "        'alpha': 0, 'layer_dropout': 'N/A', 'meta_layer': 'N/A',\n",
        "        'hidden_sizes': str(hidden), 'num_specialists': 1,\n",
        "        'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "        'epochs_run': len(hist['loss']), 'train_time_sec': round(elapsed, 1),\n",
        "        'total_params': n_params,\n",
        "        'init_method': cfg.get('init_method', 'kaiming'),\n",
        "        'elm_mode': False, 'pruning': False, 'prune_beta': 1.0,\n",
        "    })\n",
        "    return hist, te_acc, n_params\n",
        "\n",
        "\n",
        "def train_bp(ds_name, ds, hidden, seed, cfg, activation='relu'):\n",
        "    \"\"\"Train BP Baseline.\"\"\"\n",
        "    set_seed(seed)\n",
        "    dev = cfg['device']\n",
        "\n",
        "    model = BPBaseline(ds['input_dim'], hidden, ds['num_classes'], cfg['lr'], dev,\n",
        "                       activation=activation)\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    loader = make_loader(ds['X_train'], ds['y_train'], cfg['batch_size'])\n",
        "\n",
        "    # Early stopping config\n",
        "    min_epochs = cfg.get('min_epochs', 50)\n",
        "    total_patience = cfg['early_stop_patience']\n",
        "    lr_reduce_patience = cfg.get('lr_reduce_patience', 20)\n",
        "    lr_reduce_factor = cfg.get('lr_reduce_factor', 0.5)\n",
        "\n",
        "    hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'wall_time': []}\n",
        "    best_val, patience_counter = 0.0, 0\n",
        "    lr_reductions = 0\n",
        "    best_state = None\n",
        "    t0 = time.time()\n",
        "\n",
        "    for ep in range(cfg['epochs']):\n",
        "        loss = model.train_epoch(loader)\n",
        "        tr_acc = model.evaluate(ds['X_train'], ds['y_train'])\n",
        "        va_acc = model.evaluate(ds['X_val'], ds['y_val'])\n",
        "        hist['loss'].append(loss)\n",
        "        hist['train_acc'].append(tr_acc)\n",
        "        hist['val_acc'].append(va_acc)\n",
        "        hist['wall_time'].append(time.time() - t0)\n",
        "\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "            patience_counter = 0\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # LR reduction before stopping\n",
        "        if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "            lr_reductions += 1\n",
        "            reduce_lr_for_model(model, lr_reduce_factor)\n",
        "            lr_str = get_current_lr_str(model)\n",
        "            print(f'  [BP]       LR reduced (×{lr_reduce_factor}) -> {lr_str} at epoch {ep+1} (reduction #{lr_reductions})')\n",
        "\n",
        "        if (ep+1) % 10 == 0 or ep == 0:\n",
        "            print(f'  [BP]       ep {ep+1:3d}  loss={loss:.3f}  '\n",
        "                  f'train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "\n",
        "        # Early stop only after min_epochs and full patience exhausted\n",
        "        if ep >= min_epochs and patience_counter >= total_patience:\n",
        "            print(f'  [BP]       Early stop at epoch {ep+1} (best_val={best_val:.1f}%, {lr_reductions} LR reductions)')\n",
        "            break\n",
        "\n",
        "    # Restore best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f'  [BP]       Restored best checkpoint (val={best_val:.1f}%)')\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    te_acc = model.evaluate(ds['X_test'], ds['y_test'])\n",
        "    print(f'  [BP]       DONE  test={te_acc:.2f}%  {elapsed:.0f}s  {n_params} params')\n",
        "\n",
        "    LOGGER.log({\n",
        "        'dataset': ds_name, 'method': 'BP', 'seed': seed,\n",
        "        'alpha': 'N/A', 'layer_dropout': 'N/A', 'meta_layer': 'N/A',\n",
        "        'hidden_sizes': str(hidden), 'num_specialists': 1,\n",
        "        'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "        'epochs_run': len(hist['loss']), 'train_time_sec': round(elapsed, 1),\n",
        "        'total_params': n_params,\n",
        "        'init_method': 'N/A', 'elm_mode': False, 'pruning': False, 'prune_beta': 1.0,\n",
        "    })\n",
        "    return hist, te_acc, n_params\n",
        "\n",
        "\n",
        "def train_modularff(ds_name, ds, spec_hidden, seed, cfg,\n",
        "                alpha=0.0, layer_dropout=None, meta='argmax',\n",
        "                init_method='kaiming', elm_mode=False,\n",
        "                pruning_enabled=False, prune_beta=2.0, prune_after=10,\n",
        "                use_meta_layer=True, show_specialist_perf=False, activation='relu', learnable_theta=False):\n",
        "    \"\"\"Train ModularFF with all features.\"\"\"\n",
        "    set_seed(seed)\n",
        "    dev = cfg['device']\n",
        "    img_size = ARCHITECTURES[ds_name].get('img_size')\n",
        "\n",
        "    effective_beta = prune_beta if pruning_enabled else 1.0\n",
        "\n",
        "    ens = ModularFFEnsemble(\n",
        "        ds['input_dim'], spec_hidden, ds['num_classes'],\n",
        "        cfg['lr'], cfg['theta_neuron'], dev,\n",
        "        init_method=init_method, img_size=img_size,\n",
        "        first_layer_frozen=elm_mode, prune_beta=effective_beta,\n",
        "        meta_type=meta, use_meta_layer=use_meta_layer,\n",
        "        activation=activation,\n",
        "        learnable_theta=learnable_theta\n",
        "    )\n",
        "\n",
        "    initial_params = ens.total_params()\n",
        "\n",
        "    # Early stopping config\n",
        "    min_epochs = cfg.get('min_epochs', 50)\n",
        "    total_patience = cfg['early_stop_patience']\n",
        "    lr_reduce_patience = cfg.get('lr_reduce_patience', 20)\n",
        "    lr_reduce_factor = cfg.get('lr_reduce_factor', 0.5)\n",
        "\n",
        "    hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'phase': [], 'wall_time': []}\n",
        "    best_val, patience_counter = 0.0, 0\n",
        "    lr_reductions = 0\n",
        "    # Checkpoint: save specialist state dicts\n",
        "    best_spec_states = None\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_epochs = cfg['epochs']\n",
        "    pruned = False\n",
        "\n",
        "    ld_str = str(layer_dropout) if layer_dropout else 'uniform'\n",
        "\n",
        "    for ep in range(total_epochs):\n",
        "        if pruning_enabled and not pruned and ep == prune_after:\n",
        "            print(f'\\n  [ModularFF] === PRUNING at epoch {ep} ===')\n",
        "            ens.prune_all_specialists(ds['X_train'], ds['y_train'])\n",
        "            pruned = True\n",
        "            print(f'  [ModularFF] Params: {initial_params} -> {ens.total_params()}\\n')\n",
        "\n",
        "        phase = 'post-prune' if pruned else ('pre-prune' if pruning_enabled else 'normal')\n",
        "\n",
        "        loss = ens.train_epoch(\n",
        "            ds['X_train'], ds['y_train'],\n",
        "            alpha=alpha, layer_dropout=layer_dropout, batch_size=cfg['batch_size']\n",
        "        )\n",
        "        tr_acc = ens.evaluate(ds['X_train'], ds['y_train'], 'argmax')\n",
        "        va_acc = ens.evaluate(ds['X_val'], ds['y_val'], 'argmax')\n",
        "\n",
        "        hist['loss'].append(loss)\n",
        "        hist['train_acc'].append(tr_acc)\n",
        "        hist['val_acc'].append(va_acc)\n",
        "        hist['phase'].append(phase)\n",
        "        hist['wall_time'].append(time.time() - t0)\n",
        "\n",
        "        if va_acc > best_val:\n",
        "            best_val = va_acc\n",
        "            patience_counter = 0\n",
        "            # Checkpoint all specialists\n",
        "            best_spec_states = [copy.deepcopy(s.state_dict()) for s in ens.specs]\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # LR reduction before stopping\n",
        "        if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "            lr_reductions += 1\n",
        "            reduce_lr_for_ensemble(ens, lr_reduce_factor)\n",
        "            lr_str = get_ensemble_lr_str(ens)\n",
        "            print(f'  [ModularFF] LR reduced (×{lr_reduce_factor}) -> {lr_str} at epoch {ep+1} (reduction #{lr_reductions})')\n",
        "\n",
        "        if (ep+1) % 10 == 0 or ep == 0:\n",
        "            print(f'  [ModularFF a={alpha} ld={ld_str}] ep {ep+1:3d}  '\n",
        "                  f'loss={loss:.3f}  train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "\n",
        "        # Early stop only after min_epochs and full patience exhausted\n",
        "        if ep >= min_epochs and patience_counter >= total_patience:\n",
        "            print(f'  [ModularFF] Early stop at epoch {ep+1} (best_val={best_val:.1f}%, {lr_reductions} LR reductions)')\n",
        "            break\n",
        "\n",
        "    # Restore best specialists\n",
        "    if best_spec_states is not None:\n",
        "        for s, state in zip(ens.specs, best_spec_states):\n",
        "            s.load_state_dict(state)\n",
        "        print(f'  [ModularFF] Restored best checkpoint (val={best_val:.1f}%)')\n",
        "\n",
        "    train_time = time.time() - t0\n",
        "    final_params = ens.total_params()\n",
        "\n",
        "    # Train meta-layers\n",
        "    if use_meta_layer:\n",
        "        ens.train_meta_layers(ds['X_val'], ds['y_val'])\n",
        "\n",
        "    # Evaluate all meta-layers\n",
        "    meta_results = ens.evaluate_all_meta(ds['X_test'], ds['y_test']) if use_meta_layer else {}\n",
        "    te_acc = meta_results.get(meta, ens.evaluate(ds['X_test'], ds['y_test'], meta))\n",
        "\n",
        "    # Per-specialist evaluation\n",
        "    spec_results = ens.evaluate_specialists(ds['X_test'], ds['y_test'])\n",
        "    avg_spec_acc = np.mean([spec_results[k]['accuracy'] for k in range(ens.K)])\n",
        "    avg_spec_sep = np.mean([spec_results[k].get('separation', 0) for k in range(ens.K)])\n",
        "\n",
        "    # Print summary\n",
        "    meta_str = ', '.join([f'{m}={meta_results.get(m, 0):.1f}%' for m in ['argmax', 'calibrated', 'linear', 'mlp', 'temperature']])\n",
        "    print(f'  [ModularFF a={alpha} {init_method} {\"frozen\" if elm_mode else \"trainable\"} {\"prune\" if pruning_enabled else \"no-prune\"} act={activation}] DONE')\n",
        "    print(f'    Meta: {meta_str}')\n",
        "    print(f'    test={te_acc:.2f}%  {train_time:.0f}s  {final_params} params')\n",
        "    # Goodness diagnostic (specialist 0)\n",
        "    g_diag = []\n",
        "    for li, layer in enumerate(ens.specs[0].layers):\n",
        "        gp = getattr(layer, '_last_g_pos_mean', 0)\n",
        "        gn = getattr(layer, '_last_g_neg_mean', 0)\n",
        "        th = getattr(layer, '_last_theta', getattr(layer, 'theta_layer', '?'))\n",
        "        if hasattr(th, 'item'): th = th.item()\n",
        "        g_diag.append(f'L{li}:g+={gp:.3f}/g-={gn:.3f}/sep={gp-gn:.3f}/th={th:.3f}')\n",
        "    print(f'    Goodness (spec0): {\" | \".join(g_diag)}')\n",
        "    print(f'    Avg specialist: acc={avg_spec_acc:.1f}%, separation={avg_spec_sep:.1f}')\n",
        "\n",
        "    LOGGER.log({\n",
        "        'dataset': ds_name, 'method': 'ModularFF', 'seed': seed,\n",
        "        'alpha': alpha, 'layer_dropout': str(layer_dropout), 'meta_layer': meta,\n",
        "        'hidden_sizes': str(spec_hidden), 'num_specialists': ds['num_classes'],\n",
        "        'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "        'epochs_run': len(hist['loss']), 'train_time_sec': round(train_time, 1),\n",
        "        'total_params': final_params,\n",
        "        'init_method': init_method, 'elm_mode': elm_mode,\n",
        "        'pruning': pruning_enabled, 'prune_beta': prune_beta,\n",
        "        'avg_specialist_acc': round(avg_spec_acc, 2),\n",
        "        'avg_specialist_sep': round(avg_spec_sep, 2),\n",
        "    })\n",
        "\n",
        "    return hist, te_acc, final_params, ens, meta_results, spec_results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 1: 2-LAYER EXPERIMENTS\n",
        "# ================================================================\n",
        "\n",
        "def run_2layer_experiment(ds_name, seed, cfg, show_specialist_perf=False, activation='relu', learnable_theta=False):\n",
        "    \"\"\"\n",
        "    Run all 2-layer experiments for a dataset.\n",
        "\n",
        "    Runs:\n",
        "    - ClassicFF (One-Hot) with Adam and SGD\n",
        "    - ClassicFF_Embed\n",
        "    - ClassicFF_Additive\n",
        "    - ClassicFF_LocalAdapt (alpha=0.5)\n",
        "    - BP Baseline\n",
        "    - ModularFF for each architecture × alpha combination\n",
        "    \"\"\"\n",
        "    ds = DATASETS[ds_name]\n",
        "    arch = ARCHITECTURES[ds_name]\n",
        "    results = {}\n",
        "\n",
        "    init_method = cfg.get('init_method', 'kaiming')\n",
        "    use_meta_layer = cfg.get('use_meta_layer', True)\n",
        "    hinton_lr = cfg.get('hinton_sgd_lr', 0.03)\n",
        "\n",
        "    classic_hidden = arch.get('classic_ff')\n",
        "    bp_hidden = arch.get('bp')\n",
        "    modularff_archs = arch.get('modularff_archs', [[50, 50]])\n",
        "\n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'  {ds_name} | seed={seed} | K={ds[\"num_classes\"]} | dim={ds[\"input_dim\"]}')\n",
        "    print(f'  2-LAYER EXPERIMENTS')\n",
        "    print(f'  Classic FF: {classic_hidden}')\n",
        "    print(f'  ModularFF archs: {modularff_archs}')\n",
        "    print(f'  Adam LR={cfg[\"lr\"]}, SGD LR={hinton_lr}, batch={cfg[\"batch_size\"]}')\n",
        "    print(f'{\"=\"*70}')\n",
        "\n",
        "    # 1. Classic FF (One-Hot) — Adam\n",
        "    if classic_hidden:\n",
        "        print('\\n--- Classic FF (One-Hot, Adam) ---')\n",
        "        h, a, p = train_classic_ff(ds_name, ds, classic_hidden, seed, cfg, optimizer='adam', activation=activation, learnable_theta=learnable_theta)\n",
        "        results['ClassicFF_Adam'] = {'history': h, 'test_acc': a, 'params': p}\n",
        "\n",
        "        # Classic FF (One-Hot) — SGD\n",
        "        print(f'\\n--- Classic FF (One-Hot, SGD, LR={hinton_lr}) ---')\n",
        "        h, a, p = train_classic_ff(ds_name, ds, classic_hidden, seed, cfg,\n",
        "                                    optimizer='sgd', lr_override=hinton_lr, activation=activation, learnable_theta=learnable_theta)\n",
        "        results['ClassicFF_SGD'] = {'history': h, 'test_acc': a, 'params': p}\n",
        "\n",
        "    # 2. Classic FF Embed\n",
        "    if classic_hidden:\n",
        "      if activation not in ('perceptron',):\n",
        "        print('\\n--- Classic FF (Learned Embedding) ---')\n",
        "        set_seed(seed)\n",
        "        dev = cfg['device']\n",
        "        model = ClassicFF_Embed(ds['input_dim'], classic_hidden, ds['num_classes'], cfg['lr'], dev,\n",
        "                                    activation=activation, learnable_theta=learnable_theta)\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        loader = make_loader(ds['X_train'], ds['y_train'], cfg['batch_size'])\n",
        "\n",
        "        hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'wall_time': []}\n",
        "        best_val, patience_counter = 0.0, 0\n",
        "        best_state = None\n",
        "        t0 = time.time()\n",
        "\n",
        "        min_epochs = cfg.get('min_epochs', 50)\n",
        "        total_patience = cfg['early_stop_patience']\n",
        "        lr_reduce_patience = cfg.get('lr_reduce_patience', 20)\n",
        "        lr_reduce_factor = cfg.get('lr_reduce_factor', 0.5)\n",
        "        lr_reductions = 0\n",
        "\n",
        "        for ep in range(cfg['epochs']):\n",
        "            loss = model.train_epoch(loader)\n",
        "            tr_acc = model.evaluate(ds['X_train'], ds['y_train'])\n",
        "            va_acc = model.evaluate(ds['X_val'], ds['y_val'])\n",
        "            hist['loss'].append(loss); hist['train_acc'].append(tr_acc)\n",
        "            hist['val_acc'].append(va_acc); hist['wall_time'].append(time.time() - t0)\n",
        "            if va_acc > best_val:\n",
        "                best_val = va_acc; patience_counter = 0\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "                lr_reductions += 1\n",
        "                reduce_lr_for_model(model, lr_reduce_factor)\n",
        "                print(f'  [ClassicFF-Embed] LR reduced at epoch {ep+1} (#{lr_reductions})')\n",
        "            if (ep+1) % 10 == 0 or ep == 0:\n",
        "                print(f'  [ClassicFF-Embed] ep {ep+1:3d}  loss={loss:.3f}  train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "            if ep >= min_epochs and patience_counter >= total_patience:\n",
        "                print(f'  [ClassicFF-Embed] Early stop at epoch {ep+1}')\n",
        "                break\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "        te_acc = model.evaluate(ds['X_test'], ds['y_test'])\n",
        "        print(f'  [ClassicFF-Embed] DONE  test={te_acc:.2f}%  {time.time()-t0:.0f}s  {n_params} params')\n",
        "        results['ClassicFF_Embed'] = {'history': hist, 'test_acc': te_acc, 'params': n_params}\n",
        "        LOGGER.log({'dataset': ds_name, 'method': 'ClassicFF_Embed', 'seed': seed,\n",
        "                     'alpha': 0, 'hidden_sizes': str(classic_hidden),\n",
        "                     'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "                     'epochs_run': len(hist['loss']), 'total_params': n_params})\n",
        "\n",
        "    # 3. Classic FF Additive\n",
        "    if classic_hidden:\n",
        "      if activation not in ('perceptron',):\n",
        "        print('\\n--- Classic FF (Additive Hidden) ---')\n",
        "        set_seed(seed)\n",
        "        dev = cfg['device']\n",
        "        model = ClassicFF_Additive(ds['input_dim'], classic_hidden, ds['num_classes'], cfg['lr'], dev,\n",
        "                                      activation=activation, learnable_theta=learnable_theta)\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        loader = make_loader(ds['X_train'], ds['y_train'], cfg['batch_size'])\n",
        "\n",
        "        hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'wall_time': []}\n",
        "        best_val, patience_counter = 0.0, 0\n",
        "        best_state = None\n",
        "        t0 = time.time()\n",
        "        lr_reductions = 0\n",
        "\n",
        "        for ep in range(cfg['epochs']):\n",
        "            loss = model.train_epoch(loader)\n",
        "            tr_acc = model.evaluate(ds['X_train'], ds['y_train'])\n",
        "            va_acc = model.evaluate(ds['X_val'], ds['y_val'])\n",
        "            hist['loss'].append(loss); hist['train_acc'].append(tr_acc)\n",
        "            hist['val_acc'].append(va_acc); hist['wall_time'].append(time.time() - t0)\n",
        "            if va_acc > best_val:\n",
        "                best_val = va_acc; patience_counter = 0\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "                lr_reductions += 1\n",
        "                reduce_lr_for_model(model, lr_reduce_factor)\n",
        "                print(f'  [ClassicFF-Additive] LR reduced at epoch {ep+1} (#{lr_reductions})')\n",
        "            if (ep+1) % 10 == 0 or ep == 0:\n",
        "                print(f'  [ClassicFF-Additive] ep {ep+1:3d}  loss={loss:.3f}  train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "            if ep >= min_epochs and patience_counter >= total_patience:\n",
        "                print(f'  [ClassicFF-Additive] Early stop at epoch {ep+1}')\n",
        "                break\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "        te_acc = model.evaluate(ds['X_test'], ds['y_test'])\n",
        "        print(f'  [ClassicFF-Additive] DONE  test={te_acc:.2f}%  {time.time()-t0:.0f}s  {n_params} params')\n",
        "        results['ClassicFF_Additive'] = {'history': hist, 'test_acc': te_acc, 'params': n_params}\n",
        "        LOGGER.log({'dataset': ds_name, 'method': 'ClassicFF_Additive', 'seed': seed,\n",
        "                     'alpha': 0, 'hidden_sizes': str(classic_hidden),\n",
        "                     'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "                     'epochs_run': len(hist['loss']), 'total_params': n_params})\n",
        "\n",
        "    # 4. Classic FF LocalAdapt (alpha=0.5)\n",
        "    if classic_hidden:\n",
        "      if activation not in ('perceptron',):\n",
        "        print('\\n--- Classic FF (LocalAdapt alpha=0.5) ---')\n",
        "        set_seed(seed)\n",
        "        dev = cfg['device']\n",
        "        model = ClassicFF_LocalAdapt(ds['input_dim'], classic_hidden, ds['num_classes'],\n",
        "                                      cfg['lr'], dev, alpha=0.5, activation=activation, learnable_theta=learnable_theta)\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        loader = make_loader(ds['X_train'], ds['y_train'], cfg['batch_size'])\n",
        "\n",
        "        hist = {'train_acc': [], 'val_acc': [], 'loss': [], 'wall_time': []}\n",
        "        best_val, patience_counter = 0.0, 0\n",
        "        best_state = None\n",
        "        t0 = time.time()\n",
        "        lr_reductions = 0\n",
        "\n",
        "        for ep in range(cfg['epochs']):\n",
        "            loss = model.train_epoch(loader)\n",
        "            tr_acc = model.evaluate(ds['X_train'], ds['y_train'])\n",
        "            va_acc = model.evaluate(ds['X_val'], ds['y_val'])\n",
        "            hist['loss'].append(loss); hist['train_acc'].append(tr_acc)\n",
        "            hist['val_acc'].append(va_acc); hist['wall_time'].append(time.time() - t0)\n",
        "            if va_acc > best_val:\n",
        "                best_val = va_acc; patience_counter = 0\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter > 0 and patience_counter % lr_reduce_patience == 0 and patience_counter < total_patience:\n",
        "                lr_reductions += 1\n",
        "                reduce_lr_for_model(model, lr_reduce_factor)\n",
        "                print(f'  [ClassicFF+LA] LR reduced at epoch {ep+1} (#{lr_reductions})')\n",
        "            if (ep+1) % 10 == 0 or ep == 0:\n",
        "                print(f'  [ClassicFF+LA a=0.5] ep {ep+1:3d}  loss={loss:.3f}  train={tr_acc:.1f}%  val={va_acc:.1f}%  (p={patience_counter})')\n",
        "            if ep >= min_epochs and patience_counter >= total_patience:\n",
        "                print(f'  [ClassicFF+LA] Early stop at epoch {ep+1}')\n",
        "                break\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "        te_acc = model.evaluate(ds['X_test'], ds['y_test'])\n",
        "        print(f'  [ClassicFF+LA a=0.5] DONE  test={te_acc:.2f}%  {time.time()-t0:.0f}s  {n_params} params')\n",
        "        results['ClassicFF_LocalAdapt_a0.5'] = {'history': hist, 'test_acc': te_acc, 'params': n_params}\n",
        "        LOGGER.log({'dataset': ds_name, 'method': 'ClassicFF_LocalAdapt', 'seed': seed,\n",
        "                     'alpha': 0.5, 'hidden_sizes': str(classic_hidden),\n",
        "                     'best_val_acc': round(best_val, 2), 'test_acc': round(te_acc, 2),\n",
        "                     'epochs_run': len(hist['loss']), 'total_params': n_params})\n",
        "\n",
        "    # 5. BP Baseline\n",
        "    if bp_hidden:\n",
        "        print('\\n--- BP Baseline ---')\n",
        "        if activation != 'perceptron':\n",
        "            h, a, p = train_bp(ds_name, ds, bp_hidden, seed, cfg, activation=activation)\n",
        "            results['BP'] = {'history': h, 'test_acc': a, 'params': p}\n",
        "        else:\n",
        "            print('  [BP] Skipped (perceptron activation is FF-specific)')\n",
        "\n",
        "    # 6. ModularFF — all architectures × alpha values\n",
        "    for spec_hidden in modularff_archs:\n",
        "        for alpha in cfg.get('alpha_values', [0.0, 0.5, 1.0]):\n",
        "            arch_str = '_'.join(map(str, spec_hidden))\n",
        "            print(f'\\n--- ModularFF arch={spec_hidden} (alpha={alpha}) ---')\n",
        "            h, a, p, ens, mr, sr = train_modularff(\n",
        "                ds_name, ds, spec_hidden, seed, cfg,\n",
        "                alpha=alpha, layer_dropout=None, meta='argmax',\n",
        "                init_method=init_method, elm_mode=False, activation=activation,\n",
        "                pruning_enabled=False, use_meta_layer=use_meta_layer,\n",
        "                show_specialist_perf=show_specialist_perf,\n",
        "                learnable_theta=learnable_theta\n",
        "            )\n",
        "            results[f'ModularFF_{arch_str}_a{alpha}'] = {\n",
        "                'history': h, 'test_acc': a, 'params': p,\n",
        "                'meta_results': mr, 'specialist_results': sr,\n",
        "                'architecture': spec_hidden\n",
        "            }\n",
        "\n",
        "    # Save results\n",
        "    save_path = os.path.join(CONFIG['results_path'], ds_name, f'results_2layer_seed{seed}.json')\n",
        "\n",
        "    serializable = {}\n",
        "    for key, val in results.items():\n",
        "        entry = {\n",
        "            'test_acc': val['test_acc'], 'params': val['params'],\n",
        "            'train_acc': val['history']['train_acc'],\n",
        "            'val_acc': val['history']['val_acc'],\n",
        "            'wall_time': val['history'].get('wall_time', []),\n",
        "        }\n",
        "        if 'meta_results' in val:\n",
        "            entry['meta_results'] = val['meta_results']\n",
        "        if 'specialist_results' in val:\n",
        "            entry['specialist_results'] = val['specialist_results']\n",
        "        if 'architecture' in val:\n",
        "            entry['architecture'] = val['architecture']\n",
        "        serializable[key] = entry\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(serializable, f, indent=2)\n",
        "    print(f'\\n\\u2713 2-Layer results saved: {save_path}')\n",
        "\n",
        "    # Print summary\n",
        "    print(f'\\n  --- {ds_name} 2-Layer Summary ---')\n",
        "    for key, val in results.items():\n",
        "        print(f'  {key}: {val[\"test_acc\"]:.1f}% ({val[\"params\"]:,} params)')\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# PHASE 2: 4-LAYER HINTON COMPARISON\n",
        "# ================================================================\n",
        "\n",
        "def run_4layer_experiment(ds_name, seed, cfg, show_specialist_perf=False, activation='relu', learnable_theta=False):\n",
        "    \"\"\"\n",
        "    Run ONLY the 4-layer Hinton-style comparison experiments.\n",
        "\n",
        "    Phase 2 optimized:\n",
        "    - Alpha values: [0.0, 0.5, 1.0] (skip 0.3)\n",
        "    - Classic FF runs with BOTH Adam and SGD (Hinton's optimizer)\n",
        "    \"\"\"\n",
        "    ds = DATASETS[ds_name]\n",
        "    arch = ARCHITECTURES[ds_name]\n",
        "    results = {}\n",
        "\n",
        "    init_method = cfg.get('init_method', 'kaiming')\n",
        "    use_meta_layer = cfg.get('use_meta_layer', True)\n",
        "    hinton_lr = cfg.get('hinton_sgd_lr', 0.03)\n",
        "\n",
        "    # Get 4-layer architectures\n",
        "    classic_ff_4L = arch.get('classic_ff_4L')\n",
        "    modularff_4L = arch.get('modularff_4L')\n",
        "\n",
        "    if classic_ff_4L is None or modularff_4L is None:\n",
        "        print(f'  WARNING: 4-layer architectures not defined for {ds_name}, skipping')\n",
        "        return results\n",
        "\n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'  {ds_name} | seed={seed} | K={ds[\"num_classes\"]} | dim={ds[\"input_dim\"]}')\n",
        "    print(f'  4-LAYER HINTON COMPARISON (Phase 2)')\n",
        "    print(f'  Classic FF 4L: {classic_ff_4L}')\n",
        "    print(f'  ModularFF 4L: {modularff_4L} × {ds[\"num_classes\"]} specialists')\n",
        "    print(f'  Adam LR={cfg[\"lr\"]}, SGD LR={hinton_lr}, batch={cfg[\"batch_size\"]}, patience={cfg[\"early_stop_patience\"]}, min_ep={cfg.get(\"min_epochs\", 50)}')\n",
        "    print(f'{\"=\"*70}')\n",
        "\n",
        "    # 1a. Classic FF 4-Layer with Adam (same optimizer as ModularFF)\n",
        "    print('\\n--- Classic FF 4-Layer (Adam) ---')\n",
        "    h, a, p = train_classic_ff(ds_name, ds, classic_ff_4L, seed, cfg,\n",
        "                                optimizer='adam', activation=activation, learnable_theta=learnable_theta)\n",
        "    results['ClassicFF_4L_Adam'] = {'history': h, 'test_acc': a, 'params': p}\n",
        "\n",
        "    # 1b. Classic FF 4-Layer with SGD (Hinton's original optimizer)\n",
        "    print(f'\\n--- Classic FF 4-Layer (SGD, LR={hinton_lr}) ---')\n",
        "    h, a, p = train_classic_ff(ds_name, ds, classic_ff_4L, seed, cfg,\n",
        "                                optimizer='sgd', lr_override=hinton_lr, activation=activation, learnable_theta=learnable_theta)\n",
        "    results['ClassicFF_4L_SGD'] = {'history': h, 'test_acc': a, 'params': p}\n",
        "\n",
        "    # 2. ModularFF 4-Layer (parameter-matched, Adam)\n",
        "    for alpha in cfg.get('alpha_values', [0.0, 0.3, 0.5, 1.0]):\n",
        "        print(f'\\n--- ModularFF 4L arch={modularff_4L} (alpha={alpha}) ---')\n",
        "        h, a, p, ens, mr, sr = train_modularff(\n",
        "            ds_name, ds, modularff_4L, seed, cfg,\n",
        "            alpha=alpha, layer_dropout=None, meta='argmax',\n",
        "            init_method=init_method, elm_mode=False, activation=activation,\n",
        "            pruning_enabled=False, use_meta_layer=use_meta_layer,\n",
        "            show_specialist_perf=show_specialist_perf,\n",
        "            learnable_theta=learnable_theta\n",
        "        )\n",
        "        arch_str = '_'.join(map(str, modularff_4L))\n",
        "        results[f'ModularFF_4L_{arch_str}_a{alpha}'] = {\n",
        "            'history': h, 'test_acc': a, 'params': p,\n",
        "            'meta_results': mr, 'specialist_results': sr,\n",
        "            'architecture': modularff_4L\n",
        "        }\n",
        "\n",
        "    # Save results\n",
        "    save_path = os.path.join(CONFIG['results_path'], ds_name, f'results_4layer_seed{seed}.json')\n",
        "\n",
        "    serializable = {}\n",
        "    for key, val in results.items():\n",
        "        entry = {\n",
        "            'test_acc': val['test_acc'], 'params': val['params'],\n",
        "            'train_acc': val['history']['train_acc'],\n",
        "            'val_acc': val['history']['val_acc'],\n",
        "            'wall_time': val['history'].get('wall_time', []),\n",
        "        }\n",
        "        if 'meta_results' in val:\n",
        "            entry['meta_results'] = val['meta_results']\n",
        "        if 'specialist_results' in val:\n",
        "            entry['specialist_results'] = val['specialist_results']\n",
        "        if 'architecture' in val:\n",
        "            entry['architecture'] = val['architecture']\n",
        "        serializable[key] = entry\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(serializable, f, indent=2)\n",
        "    print(f'\\n\\u2713 4-Layer results saved: {save_path}')\n",
        "\n",
        "    # Print comparison summary\n",
        "    print(f'\\n  --- {ds_name} Summary ---')\n",
        "    adam_acc = results.get('ClassicFF_4L_Adam', {}).get('test_acc', 0)\n",
        "    sgd_acc = results.get('ClassicFF_4L_SGD', {}).get('test_acc', 0)\n",
        "    best_mod_acc = max(\n",
        "        (v.get('test_acc', 0) for k, v in results.items() if k.startswith('ModularFF')),\n",
        "        default=0\n",
        "    )\n",
        "    print(f'  ClassicFF 4L (Adam):  {adam_acc:.1f}%')\n",
        "    print(f'  ClassicFF 4L (SGD):   {sgd_acc:.1f}%')\n",
        "    print(f'  ModularFF 4L (best):  {best_mod_acc:.1f}%')\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "print('\\u2713 Training engine ready (v5):')\n",
        "print(f'  - Patience: {CONFIG[\"early_stop_patience\"]} (reduce LR every {CONFIG.get(\"lr_reduce_patience\", 20)} epochs)')\n",
        "print(f'  - Min epochs: {CONFIG.get(\"min_epochs\", 50)}')\n",
        "print('  - Best-model checkpoint: ON')\n",
        "print('  - 2-layer experiments: run_2layer_experiment()')\n",
        "print('  - 4-layer Hinton comparison: run_4layer_experiment()')\n",
        "print('  - Classic FF: Adam and SGD (Hinton config)')\n",
        "print(f'  - Logger: {LOGGER.path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXlSa-YIr5rL"
      },
      "outputs": [],
      "source": [
        "######### end of cell 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3I36OmCxnTd",
        "outputId": "3c9cb77b-b163-4a43-e9ff-a51d7656d5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.968  train=86.4%  val=86.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.719  train=91.2%  val=90.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.708  train=91.9%  val=91.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.702  train=92.2%  val=91.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.701  train=92.6%  val=91.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.696  train=92.8%  val=91.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.694  train=92.9%  val=91.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.693  train=93.1%  val=91.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.691  train=93.2%  val=91.6%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 90 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.688  train=93.3%  val=91.8%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.688  train=93.4%  val=91.7%  (p=30)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.687  train=93.4%  val=91.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.688  train=93.5%  val=91.9%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.686  train=93.6%  val=91.9%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.685  train=93.6%  val=91.9%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.685  train=93.6%  val=91.9%  (p=14)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.684  train=93.6%  val=91.8%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.684  train=93.7%  val=91.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.685  train=93.7%  val=91.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.684  train=93.8%  val=91.9%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 193 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.683  train=93.9%  val=92.1%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.683  train=93.9%  val=92.0%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 218 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.682  train=93.9%  val=92.0%  (p=22)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.682  train=93.9%  val=92.0%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.681  train=93.9%  val=92.0%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 246 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.682  train=93.9%  val=92.0%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.681  train=93.9%  val=92.0%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 266 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.681  train=93.9%  val=92.0%  (p=44)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.681  train=94.0%  val=92.0%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 286 (best_val=92.1%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=92.1%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=92.4%, calibrated=90.2%, linear=91.2%, mlp=91.6%, temperature=91.3%\n",
            "    test=92.43%  430s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.925/g-=-0.993/sep=1.918/th=0.000 | L1:g+=0.936/g-=-1.000/sep=1.936/th=0.000\n",
            "    Avg specialist: acc=96.1%, separation=3.7\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.198  train=85.5%  val=85.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.089  train=86.5%  val=86.1%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.086  train=86.0%  val=85.4%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 24 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.083  train=85.9%  val=85.2%  (p=26)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.082  train=85.7%  val=84.9%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 44 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.082  train=85.8%  val=84.9%  (p=46)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.082  train=85.6%  val=85.0%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 64 (best_val=87.3%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=87.3%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=88.1%, calibrated=87.8%, linear=82.3%, mlp=90.3%, temperature=88.5%\n",
            "    test=88.14%  114s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.888/g-=-0.526/sep=1.414/th=0.000 | L1:g+=0.973/g-=-0.377/sep=1.350/th=0.000\n",
            "    Avg specialist: acc=89.9%, separation=2.3\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.283  train=15.8%  val=16.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.228  train=15.0%  val=15.1%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.229  train=18.0%  val=18.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.230  train=19.1%  val=19.1%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 39 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.230  train=19.6%  val=19.7%  (p=21)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.229  train=17.2%  val=17.6%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 59 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.229  train=18.9%  val=19.2%  (p=41)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.229  train=19.7%  val=19.8%  (p=51)\n",
            "  [ModularFF] Early stop at epoch 79 (best_val=20.6%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=20.6%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=20.8%, calibrated=0.1%, linear=41.1%, mlp=60.2%, temperature=20.2%\n",
            "    test=20.85%  140s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.000/g-=0.000/sep=-0.000/th=0.000 | L1:g+=-0.044/g-=-0.028/sep=-0.016/th=0.000\n",
            "    Avg specialist: acc=36.9%, separation=-0.0\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/MNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- MNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 87.8% (648,002 params)\n",
            "  ClassicFF_SGD: 11.3% (648,002 params)\n",
            "  ClassicFF_Embed: 77.2% (644,022 params)\n",
            "  ClassicFF_Additive: 92.5% (648,002 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 85.8% (648,002 params)\n",
            "  BP: 97.8% (648,010 params)\n",
            "  ModularFF_50_50_a0.0: 92.4% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 87.4% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 34.5% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 92.4% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 88.1% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 20.8% (886,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: MNIST | ACTIVATION: hardlimit\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  MNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.736  train=16.6%  val=16.2%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=2.764  train=11.2%  val=11.2%  (p=9)\n",
            "  [ClassicFF_ADAM] ep  20  loss=2.760  train=11.2%  val=11.2%  (p=19)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=2.761  train=11.2%  val=11.2%  (p=29)\n",
            "  [ClassicFF_ADAM] ep  40  loss=2.765  train=11.2%  val=11.2%  (p=39)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  50  loss=2.761  train=11.2%  val=11.2%  (p=49)\n",
            "  [ClassicFF_ADAM] ep  60  loss=2.764  train=11.2%  val=11.2%  (p=59)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 61 (best_val=16.2%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=16.2%)\n",
            "  [ClassicFF_ADAM] DONE  test=16.96%  140s  648002 params  (LR=0.01, adam, act=hardlimit)\n",
            "    Goodness: L0:g+=0.497/g-=0.547/sep=-0.051/th=0.500 | L1:g+=1.000/g-=1.000/sep=0.000/th=0.990\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.773  train=9.7%  val=9.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.773  train=9.8%  val=10.1%  (p=3)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.773  train=10.0%  val=10.0%  (p=5)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.773  train=10.1%  val=9.9%  (p=15)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 35 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.773  train=10.0%  val=10.0%  (p=25)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.773  train=10.0%  val=10.0%  (p=35)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 55 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.773  train=10.0%  val=10.0%  (p=45)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.773  train=10.1%  val=10.1%  (p=55)\n",
            "  [ClassicFF_SGD] Early stop at epoch 75 (best_val=10.4%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=10.4%)\n",
            "  [ClassicFF_SGD] DONE  test=9.53%  166s  648002 params  (LR=0.03, sgd, act=hardlimit)\n",
            "    Goodness: L0:g+=0.491/g-=0.487/sep=0.004/th=0.496 | L1:g+=0.498/g-=0.490/sep=0.008/th=0.492\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.746  train=18.6%  val=18.4%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.746  train=17.6%  val=17.5%  (p=9)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.745  train=14.2%  val=14.1%  (p=6)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.751  train=15.1%  val=15.4%  (p=16)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 34 (#1)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.745  train=19.4%  val=19.2%  (p=0)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.748  train=15.1%  val=15.2%  (p=10)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 60 (#2)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.747  train=14.0%  val=14.0%  (p=20)\n",
            "  [ClassicFF-Embed] ep  70  loss=2.748  train=14.4%  val=14.7%  (p=8)\n",
            "  [ClassicFF-Embed] ep  80  loss=2.750  train=14.3%  val=14.5%  (p=4)\n",
            "  [ClassicFF-Embed] ep  90  loss=2.749  train=14.1%  val=14.3%  (p=14)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 96 (#3)\n",
            "  [ClassicFF-Embed] ep 100  loss=2.751  train=14.8%  val=15.2%  (p=24)\n",
            "  [ClassicFF-Embed] ep 110  loss=2.748  train=14.8%  val=14.9%  (p=34)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 116 (#4)\n",
            "  [ClassicFF-Embed] ep 120  loss=2.748  train=15.1%  val=15.5%  (p=44)\n",
            "  [ClassicFF-Embed] ep 130  loss=2.748  train=14.8%  val=15.0%  (p=54)\n",
            "  [ClassicFF-Embed] Early stop at epoch 136\n",
            "  [ClassicFF-Embed] DONE  test=19.20%  317s  644022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.555  train=64.1%  val=63.5%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.530  train=59.0%  val=58.4%  (p=4)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.535  train=59.2%  val=58.6%  (p=14)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 26 (#1)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.526  train=62.2%  val=61.8%  (p=24)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.525  train=61.9%  val=61.8%  (p=34)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 46 (#2)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.525  train=68.5%  val=68.5%  (p=0)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.525  train=63.4%  val=63.4%  (p=10)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 70 (#3)\n",
            "  [ClassicFF-Additive] ep  70  loss=2.524  train=64.2%  val=64.0%  (p=20)\n",
            "  [ClassicFF-Additive] ep  80  loss=2.526  train=64.9%  val=64.7%  (p=30)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 90 (#4)\n",
            "  [ClassicFF-Additive] ep  90  loss=2.525  train=64.4%  val=64.5%  (p=40)\n",
            "  [ClassicFF-Additive] ep 100  loss=2.522  train=65.4%  val=65.5%  (p=50)\n",
            "  [ClassicFF-Additive] ep 110  loss=2.522  train=66.3%  val=66.1%  (p=60)\n",
            "  [ClassicFF-Additive] Early stop at epoch 110\n",
            "  [ClassicFF-Additive] DONE  test=68.95%  254s  648002 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.770  train=19.3%  val=19.0%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.722  train=27.1%  val=27.0%  (p=4)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.721  train=24.8%  val=24.9%  (p=14)\n",
            "  [ClassicFF+LA] LR reduced at epoch 26 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.720  train=26.1%  val=25.9%  (p=24)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.722  train=26.1%  val=25.9%  (p=34)\n",
            "  [ClassicFF+LA] LR reduced at epoch 46 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.721  train=25.9%  val=25.6%  (p=44)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.720  train=26.2%  val=26.2%  (p=54)\n",
            "  [ClassicFF+LA] Early stop at epoch 66\n",
            "  [ClassicFF+LA a=0.5] DONE  test=28.68%  167s  648002 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.493  train=88.4%  val=88.0%  (p=0)\n",
            "  [BP]       ep  10  loss=0.419  train=87.2%  val=87.0%  (p=5)\n",
            "  [BP]       ep  20  loss=0.465  train=85.8%  val=85.9%  (p=15)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 25 (reduction #1)\n",
            "  [BP]       ep  30  loss=0.596  train=80.9%  val=80.8%  (p=25)\n",
            "  [BP]       ep  40  loss=0.551  train=82.2%  val=82.0%  (p=35)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 45 (reduction #2)\n",
            "  [BP]       ep  50  loss=0.543  train=82.0%  val=81.6%  (p=45)\n",
            "  [BP]       ep  60  loss=0.534  train=82.9%  val=82.5%  (p=55)\n",
            "  [BP]       Early stop at epoch 65 (best_val=89.0%, 2 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=89.0%)\n",
            "  [BP]       DONE  test=89.45%  59s  648010 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.157  train=46.5%  val=46.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.092  train=38.8%  val=38.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.094  train=38.4%  val=38.3%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.093  train=38.2%  val=38.2%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.096  train=38.2%  val=38.2%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.093  train=38.3%  val=38.2%  (p=49)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.092  train=38.2%  val=38.2%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=46.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=46.8%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=47.2%, calibrated=54.7%, linear=63.4%, mlp=68.1%, temperature=56.2%\n",
            "    test=47.21%  109s  418000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.286/sep=0.714/th=0.500 | L1:g+=1.000/g-=0.286/sep=0.714/th=0.500\n",
            "    Avg specialist: acc=85.8%, separation=1.4\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.261  train=36.4%  val=36.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.202  train=24.7%  val=24.4%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.206  train=24.6%  val=24.3%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.203  train=24.4%  val=24.2%  (p=29)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.205  train=24.4%  val=24.2%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.203  train=24.4%  val=24.2%  (p=49)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.204  train=24.4%  val=24.2%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=36.3%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=36.3%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=36.4%, calibrated=28.4%, linear=58.9%, mlp=65.5%, temperature=28.4%\n",
            "    test=36.41%  125s  418000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.429/sep=0.571/th=0.500 | L1:g+=1.000/g-=0.429/sep=0.571/th=0.500\n",
            "    Avg specialist: acc=76.9%, separation=1.0\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.365  train=27.8%  val=28.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.343  train=25.3%  val=25.3%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.344  train=25.3%  val=25.3%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.344  train=25.3%  val=25.4%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.344  train=25.2%  val=25.3%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.343  train=25.2%  val=25.3%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.344  train=25.2%  val=25.3%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=28.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=28.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=28.4%, calibrated=41.0%, linear=55.7%, mlp=66.5%, temperature=33.5%\n",
            "    test=28.37%  125s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.860/g-=0.369/sep=0.491/th=0.500 | L1:g+=0.000/g-=0.000/sep=0.000/th=0.500\n",
            "    Avg specialist: acc=73.8%, separation=0.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.145  train=46.8%  val=47.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.093  train=38.9%  val=38.8%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.094  train=38.4%  val=38.4%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.094  train=38.3%  val=38.2%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.096  train=38.3%  val=38.3%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.094  train=38.3%  val=38.2%  (p=49)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.093  train=38.3%  val=38.2%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=47.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=47.0%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=47.6%, calibrated=54.7%, linear=63.9%, mlp=68.6%, temperature=54.8%\n",
            "    test=47.60%  113s  886000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.286/sep=0.714/th=0.500 | L1:g+=1.000/g-=0.286/sep=0.714/th=0.500\n",
            "    Avg specialist: acc=85.9%, separation=1.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.262  train=34.4%  val=34.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.202  train=24.8%  val=24.4%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.205  train=24.7%  val=24.4%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.203  train=24.5%  val=24.2%  (p=29)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.205  train=24.5%  val=24.2%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.203  train=24.5%  val=24.3%  (p=49)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.204  train=24.5%  val=24.3%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=34.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=34.4%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=33.5%, calibrated=30.0%, linear=58.9%, mlp=64.6%, temperature=28.5%\n",
            "    test=33.54%  130s  886000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.429/sep=0.571/th=0.500 | L1:g+=1.000/g-=0.429/sep=0.571/th=0.500\n",
            "    Avg specialist: acc=76.9%, separation=1.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.374  train=24.2%  val=24.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.353  train=22.4%  val=22.1%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.354  train=22.5%  val=22.3%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.353  train=22.4%  val=22.0%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.354  train=22.4%  val=22.0%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.353  train=22.3%  val=22.0%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.354  train=22.3%  val=22.0%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=24.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=24.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=24.0%, calibrated=33.0%, linear=56.5%, mlp=65.5%, temperature=28.6%\n",
            "    test=24.00%  130s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.810/g-=0.347/sep=0.463/th=0.500 | L1:g+=0.000/g-=0.000/sep=0.000/th=0.500\n",
            "    Avg specialist: acc=73.8%, separation=0.4\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/MNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- MNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 17.0% (648,002 params)\n",
            "  ClassicFF_SGD: 9.5% (648,002 params)\n",
            "  ClassicFF_Embed: 19.2% (644,022 params)\n",
            "  ClassicFF_Additive: 69.0% (648,002 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 28.7% (648,002 params)\n",
            "  BP: 89.5% (648,010 params)\n",
            "  ModularFF_50_50_a0.0: 47.2% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 36.4% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 28.4% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 47.6% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 33.5% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 24.0% (886,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: MNIST | ACTIVATION: perceptron\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  MNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=1.999  train=11.6%  val=11.3%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.991  train=11.4%  val=11.4%  (p=8)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.989  train=11.8%  val=11.9%  (p=18)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 22 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.988  train=11.7%  val=11.6%  (p=28)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.987  train=12.3%  val=12.4%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.986  train=12.5%  val=12.3%  (p=6)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.986  train=11.9%  val=12.1%  (p=16)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 64 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.987  train=13.1%  val=12.8%  (p=26)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.986  train=12.0%  val=11.8%  (p=6)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.986  train=12.2%  val=12.1%  (p=16)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 94 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.985  train=12.5%  val=12.4%  (p=26)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.985  train=13.0%  val=12.7%  (p=4)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.987  train=12.5%  val=12.4%  (p=14)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 126 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.986  train=13.9%  val=14.1%  (p=24)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.986  train=12.1%  val=12.1%  (p=34)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 146 (reduction #5)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.986  train=12.3%  val=12.4%  (p=44)\n",
            "  [ClassicFF_ADAM] ep 160  loss=1.986  train=12.7%  val=12.9%  (p=54)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 166 (best_val=14.7%, 5 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=14.7%)\n",
            "  [ClassicFF_ADAM] DONE  test=14.30%  296s  648000 params  (LR=0.01, adam, act=perceptron)\n",
            "    Goodness: L0:g+=0.697/g-=0.609/sep=0.087/th=0.500 | L1:g+=0.536/g-=0.529/sep=0.007/th=0.500\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=1.999  train=13.5%  val=13.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=1.976  train=13.7%  val=13.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=1.973  train=11.3%  val=11.2%  (p=9)\n",
            "  [ClassicFF_SGD] ep  30  loss=1.971  train=10.8%  val=10.9%  (p=19)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 31 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  40  loss=1.969  train=12.6%  val=12.6%  (p=29)\n",
            "  [ClassicFF_SGD] ep  50  loss=1.969  train=10.2%  val=10.6%  (p=39)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 51 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  60  loss=1.968  train=11.6%  val=11.7%  (p=49)\n",
            "  [ClassicFF_SGD] ep  70  loss=1.970  train=12.7%  val=12.6%  (p=2)\n",
            "  [ClassicFF_SGD] ep  80  loss=1.968  train=11.8%  val=11.8%  (p=3)\n",
            "  [ClassicFF_SGD] ep  90  loss=1.967  train=15.7%  val=15.6%  (p=13)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 97 (reduction #3)\n",
            "  [ClassicFF_SGD] ep 100  loss=1.965  train=10.8%  val=10.7%  (p=23)\n",
            "  [ClassicFF_SGD] ep 110  loss=1.968  train=13.5%  val=13.3%  (p=33)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 117 (reduction #4)\n",
            "  [ClassicFF_SGD] ep 120  loss=1.969  train=13.4%  val=13.4%  (p=43)\n",
            "  [ClassicFF_SGD] ep 130  loss=1.970  train=11.3%  val=11.4%  (p=53)\n",
            "  [ClassicFF_SGD] Early stop at epoch 137 (best_val=17.6%, 4 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=17.6%)\n",
            "  [ClassicFF_SGD] DONE  test=18.11%  244s  648000 params  (LR=0.03, sgd, act=perceptron)\n",
            "    Goodness: L0:g+=0.089/g-=0.147/sep=-0.059/th=0.500 | L1:g+=0.369/g-=0.311/sep=0.058/th=0.500\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP] Skipped (perceptron activation is FF-specific)\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.921  train=63.4%  val=62.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.883  train=74.1%  val=73.8%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.872  train=78.3%  val=78.1%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.866  train=80.7%  val=79.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.863  train=80.4%  val=79.6%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.859  train=81.2%  val=80.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.856  train=80.1%  val=79.4%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.854  train=59.4%  val=58.8%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.852  train=82.6%  val=81.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.851  train=82.4%  val=81.2%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 91 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.849  train=82.4%  val=81.4%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.848  train=82.9%  val=81.7%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.847  train=82.3%  val=81.5%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.846  train=82.2%  val=81.0%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 137 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.844  train=83.2%  val=81.9%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.843  train=82.8%  val=82.0%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 157 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.842  train=83.2%  val=82.1%  (p=43)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.843  train=79.7%  val=78.4%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.842  train=82.3%  val=81.1%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.841  train=82.6%  val=81.2%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 194 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.840  train=82.4%  val=80.9%  (p=26)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.840  train=83.0%  val=81.8%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 214 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.838  train=81.6%  val=80.0%  (p=46)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.838  train=83.5%  val=82.2%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 234 (best_val=82.6%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=82.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=83.2%, calibrated=81.3%, linear=64.8%, mlp=82.7%, temperature=82.4%\n",
            "    test=83.16%  316s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.619/g-=0.346/sep=0.273/th=0.500 | L1:g+=0.550/g-=0.393/sep=0.157/th=0.500\n",
            "    Avg specialist: acc=91.4%, separation=0.3\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.847  train=80.1%  val=80.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.382  train=89.4%  val=88.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.294  train=90.4%  val=89.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.256  train=90.8%  val=90.1%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.236  train=90.8%  val=90.3%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.218  train=91.1%  val=90.3%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 52 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.206  train=91.2%  val=90.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.196  train=89.9%  val=89.0%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.189  train=90.5%  val=89.3%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.185  train=90.6%  val=89.4%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 99 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.178  train=91.5%  val=90.4%  (p=21)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.174  train=91.7%  val=90.7%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 119 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.173  train=91.1%  val=90.1%  (p=41)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.174  train=91.1%  val=89.7%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.163  train=91.2%  val=90.1%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 143 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.169  train=91.0%  val=90.0%  (p=27)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.165  train=89.4%  val=88.5%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 163 (reduction #5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.171  train=91.0%  val=90.0%  (p=47)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.179  train=90.2%  val=89.0%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 183 (best_val=90.8%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=90.8%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=90.8%, calibrated=90.8%, linear=90.6%, mlp=91.9%, temperature=90.9%\n",
            "    test=90.81%  247s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.956/g-=0.047/sep=0.909/th=0.500 | L1:g+=0.959/g-=0.053/sep=0.906/th=0.500\n",
            "    Avg specialist: acc=95.2%, separation=1.6\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.795  train=82.4%  val=82.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.270  train=89.5%  val=89.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.210  train=90.3%  val=89.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.182  train=90.9%  val=90.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.168  train=91.1%  val=90.3%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.156  train=91.4%  val=90.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.148  train=91.3%  val=90.5%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.142  train=91.0%  val=90.2%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.138  train=91.4%  val=90.3%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.136  train=91.4%  val=90.4%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.134  train=91.5%  val=90.4%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 106 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.131  train=91.9%  val=90.8%  (p=24)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.131  train=91.7%  val=90.6%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 126 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.131  train=91.7%  val=90.2%  (p=44)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.124  train=91.7%  val=90.5%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 146 (best_val=91.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=91.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=91.4%, calibrated=90.2%, linear=90.8%, mlp=92.0%, temperature=91.4%\n",
            "    test=91.43%  197s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.970/g-=0.027/sep=0.943/th=0.500 | L1:g+=0.971/g-=0.024/sep=0.947/th=0.500\n",
            "    Avg specialist: acc=95.5%, separation=1.7\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.917  train=72.3%  val=72.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.892  train=81.7%  val=81.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.884  train=82.6%  val=82.5%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.880  train=84.5%  val=84.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.878  train=83.0%  val=82.7%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 50 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.874  train=84.1%  val=83.2%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.873  train=80.0%  val=79.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.871  train=65.9%  val=65.4%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 78 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.872  train=85.6%  val=84.9%  (p=22)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.870  train=86.0%  val=85.3%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.870  train=83.4%  val=82.4%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.868  train=85.5%  val=85.0%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 111 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.869  train=86.0%  val=85.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.868  train=83.7%  val=82.5%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 140 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.867  train=83.7%  val=82.8%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.865  train=86.1%  val=85.2%  (p=30)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.867  train=85.6%  val=84.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.865  train=85.5%  val=84.6%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 176 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.865  train=86.0%  val=84.9%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.864  train=83.4%  val=82.1%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 198 (reduction #6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.863  train=84.4%  val=83.6%  (p=22)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.864  train=85.5%  val=84.9%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.862  train=78.1%  val=77.1%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 221 (reduction #7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.862  train=86.7%  val=85.9%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.862  train=83.7%  val=82.6%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 241 (reduction #8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.861  train=82.2%  val=81.7%  (p=49)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.862  train=82.5%  val=81.5%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 261 (best_val=86.0%, 8 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=86.0%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=86.6%, calibrated=85.6%, linear=68.1%, mlp=84.2%, temperature=86.5%\n",
            "    test=86.56%  362s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.550/g-=0.348/sep=0.202/th=0.500 | L1:g+=0.548/g-=0.406/sep=0.142/th=0.500\n",
            "    Avg specialist: acc=93.1%, separation=0.3\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.844  train=83.2%  val=82.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.371  train=89.7%  val=89.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.283  train=90.6%  val=90.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.245  train=91.1%  val=90.5%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.224  train=91.1%  val=90.7%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.207  train=91.5%  val=91.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.197  train=91.2%  val=90.8%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.187  train=90.0%  val=89.3%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 78 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.181  train=91.0%  val=90.1%  (p=22)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.179  train=90.8%  val=89.6%  (p=32)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.173  train=91.1%  val=90.2%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.170  train=91.7%  val=91.0%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 117 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.168  train=91.4%  val=90.9%  (p=23)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.173  train=91.8%  val=90.4%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 137 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.163  train=91.0%  val=90.1%  (p=43)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.166  train=91.3%  val=90.2%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 157 (best_val=91.1%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=91.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=91.1%, calibrated=90.9%, linear=91.2%, mlp=92.2%, temperature=91.3%\n",
            "    test=91.13%  218s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.983/g-=0.042/sep=0.941/th=0.500 | L1:g+=0.981/g-=0.048/sep=0.934/th=0.500\n",
            "    Avg specialist: acc=95.3%, separation=1.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.785  train=84.3%  val=84.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.260  train=89.7%  val=89.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.198  train=90.5%  val=90.0%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.172  train=91.1%  val=90.3%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.159  train=91.2%  val=90.5%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.149  train=91.7%  val=91.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.143  train=91.5%  val=91.1%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.138  train=91.1%  val=90.2%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 78 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.136  train=91.4%  val=90.3%  (p=22)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.134  train=91.6%  val=90.5%  (p=32)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.132  train=91.6%  val=90.5%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.131  train=92.0%  val=91.1%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 117 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.130  train=91.7%  val=90.8%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.130  train=91.6%  val=90.0%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 137 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.124  train=91.9%  val=90.7%  (p=43)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.131  train=91.4%  val=90.1%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 157 (best_val=91.3%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=91.3%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=91.4%, calibrated=90.5%, linear=91.2%, mlp=91.8%, temperature=91.4%\n",
            "    test=91.39%  217s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.989/g-=0.028/sep=0.961/th=0.500 | L1:g+=0.989/g-=0.033/sep=0.956/th=0.500\n",
            "    Avg specialist: acc=95.4%, separation=1.7\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/MNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- MNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 14.3% (648,000 params)\n",
            "  ClassicFF_SGD: 18.1% (648,000 params)\n",
            "  ModularFF_50_50_a0.0: 83.2% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 90.8% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 91.4% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 86.6% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 91.1% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 91.4% (886,000 params)\n",
            "\n",
            "==========================================================================================\n",
            " MNIST — 2-LAYER CROSS-ACTIVATION SUMMARY\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               98.2%    97.9%      97.6%      -0.3%\n",
            "tanh               97.8%    92.5%      92.4%      -0.0%\n",
            "hardlimit          89.5%    69.0%      47.6%     -21.4%\n",
            "perceptron           N/A    18.1%      91.4%     +73.3%\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n",
            "  DATASET: FashionMNIST\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: FashionMNIST | ACTIVATION: gelu\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  FashionMNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.765  train=41.5%  val=41.4%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.323  train=74.8%  val=74.5%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.052  train=82.5%  val=82.6%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  30  loss=0.845  train=84.6%  val=84.5%  (p=3)\n",
            "  [ClassicFF_ADAM] ep  40  loss=0.786  train=85.5%  val=85.1%  (p=3)\n",
            "  [ClassicFF_ADAM] ep  50  loss=0.783  train=86.1%  val=85.6%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  60  loss=0.829  train=86.5%  val=85.9%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.554  train=85.7%  val=84.8%  (p=11)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 79 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  80  loss=0.821  train=85.8%  val=85.1%  (p=21)\n",
            "  [ClassicFF_ADAM] ep  90  loss=0.877  train=84.6%  val=84.0%  (p=31)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 99 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 100  loss=0.823  train=85.6%  val=84.6%  (p=41)\n",
            "  [ClassicFF_ADAM] ep 110  loss=0.785  train=86.3%  val=85.4%  (p=51)\n",
            "  [ClassicFF_ADAM] ep 120  loss=0.829  train=86.8%  val=85.7%  (p=4)\n",
            "  [ClassicFF_ADAM] ep 130  loss=0.739  train=87.2%  val=86.0%  (p=5)\n",
            "  [ClassicFF_ADAM] ep 140  loss=0.734  train=86.9%  val=85.9%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 150  loss=0.719  train=86.9%  val=86.0%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 160  loss=0.726  train=87.4%  val=86.2%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 170  loss=0.714  train=87.2%  val=86.0%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 180  loss=0.711  train=87.5%  val=86.1%  (p=16)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.3e-03 at epoch 184 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 190  loss=0.695  train=87.5%  val=86.2%  (p=26)\n",
            "  [ClassicFF_ADAM] ep 200  loss=0.688  train=87.9%  val=86.3%  (p=36)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 6.3e-04 at epoch 204 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 210  loss=0.681  train=87.7%  val=86.2%  (p=46)\n",
            "  [ClassicFF_ADAM] ep 220  loss=0.685  train=87.9%  val=86.3%  (p=1)\n",
            "  [ClassicFF_ADAM] ep 230  loss=0.676  train=87.8%  val=86.3%  (p=11)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 3.1e-04 at epoch 239 (reduction #5)\n",
            "  [ClassicFF_ADAM] ep 240  loss=0.685  train=87.9%  val=86.3%  (p=21)\n",
            "  [ClassicFF_ADAM] ep 250  loss=0.680  train=87.9%  val=86.3%  (p=31)\n",
            "  [ClassicFF_ADAM] ep 260  loss=0.686  train=88.0%  val=86.4%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 270  loss=0.682  train=87.9%  val=86.5%  (p=13)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.6e-04 at epoch 277 (reduction #6)\n",
            "  [ClassicFF_ADAM] ep 280  loss=0.682  train=88.0%  val=86.4%  (p=23)\n",
            "  [ClassicFF_ADAM] ep 290  loss=0.693  train=88.0%  val=86.5%  (p=33)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 7.8e-05 at epoch 297 (reduction #7)\n",
            "  [ClassicFF_ADAM] ep 300  loss=0.670  train=88.0%  val=86.4%  (p=43)\n",
            "  [ClassicFF_ADAM] ep 310  loss=0.685  train=87.9%  val=86.4%  (p=53)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 317 (best_val=86.6%, 7 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=86.6%)\n",
            "  [ClassicFF_ADAM] DONE  test=85.53%  624s  648002 params  (LR=0.01, adam, act=gelu)\n",
            "    Goodness: L0:g+=19.579/g-=8.693/sep=10.885/th=11.384 | L1:g+=17.891/g-=11.311/sep=6.579/th=15.011\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.850  train=10.2%  val=10.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.773  train=16.9%  val=16.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.772  train=38.9%  val=38.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.771  train=48.5%  val=48.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.771  train=51.7%  val=51.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.770  train=52.4%  val=52.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.770  train=52.5%  val=52.2%  (p=4)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.769  train=52.4%  val=52.2%  (p=14)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 76 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.768  train=52.2%  val=52.2%  (p=24)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.768  train=52.1%  val=52.1%  (p=34)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 96 (reduction #2)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.768  train=52.0%  val=52.1%  (p=44)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.768  train=52.0%  val=52.0%  (p=54)\n",
            "  [ClassicFF_SGD] Early stop at epoch 116 (best_val=52.3%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=52.3%)\n",
            "  [ClassicFF_SGD] DONE  test=52.36%  218s  648002 params  (LR=0.03, sgd, act=gelu)\n",
            "    Goodness: L0:g+=0.204/g-=0.185/sep=0.019/th=0.123 | L1:g+=0.001/g-=0.001/sep=-0.000/th=0.001\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.814  train=27.8%  val=28.0%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.055  train=37.2%  val=37.3%  (p=1)\n",
            "  [ClassicFF-Embed] ep  20  loss=1.942  train=38.9%  val=39.0%  (p=7)\n",
            "  [ClassicFF-Embed] ep  30  loss=1.883  train=45.8%  val=45.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  40  loss=1.854  train=47.0%  val=46.9%  (p=2)\n",
            "  [ClassicFF-Embed] ep  50  loss=1.849  train=47.8%  val=47.6%  (p=7)\n",
            "  [ClassicFF-Embed] ep  60  loss=1.823  train=47.7%  val=47.5%  (p=17)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 63 (#1)\n",
            "  [ClassicFF-Embed] ep  70  loss=1.787  train=47.9%  val=47.5%  (p=27)\n",
            "  [ClassicFF-Embed] ep  80  loss=1.773  train=48.1%  val=47.6%  (p=37)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 83 (#2)\n",
            "  [ClassicFF-Embed] ep  90  loss=1.770  train=48.0%  val=47.7%  (p=47)\n",
            "  [ClassicFF-Embed] ep 100  loss=1.795  train=48.3%  val=47.9%  (p=57)\n",
            "  [ClassicFF-Embed] Early stop at epoch 103\n",
            "  [ClassicFF-Embed] DONE  test=47.72%  205s  644022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.224  train=58.6%  val=58.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=1.415  train=82.3%  val=82.4%  (p=1)\n",
            "  [ClassicFF-Additive] ep  20  loss=1.404  train=81.8%  val=81.7%  (p=7)\n",
            "  [ClassicFF-Additive] ep  30  loss=1.385  train=82.0%  val=81.4%  (p=17)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 33 (#1)\n",
            "  [ClassicFF-Additive] ep  40  loss=1.336  train=84.1%  val=84.0%  (p=4)\n",
            "  [ClassicFF-Additive] ep  50  loss=1.318  train=84.8%  val=84.5%  (p=2)\n",
            "  [ClassicFF-Additive] ep  60  loss=1.304  train=84.9%  val=84.3%  (p=1)\n",
            "  [ClassicFF-Additive] ep  70  loss=1.302  train=85.4%  val=85.1%  (p=11)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 79 (#2)\n",
            "  [ClassicFF-Additive] ep  80  loss=1.293  train=85.3%  val=85.0%  (p=21)\n",
            "  [ClassicFF-Additive] ep  90  loss=1.288  train=85.6%  val=85.1%  (p=9)\n",
            "  [ClassicFF-Additive] ep 100  loss=1.286  train=85.3%  val=85.0%  (p=19)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 101 (#3)\n",
            "  [ClassicFF-Additive] ep 110  loss=1.286  train=85.7%  val=85.1%  (p=5)\n",
            "  [ClassicFF-Additive] ep 120  loss=1.277  train=85.9%  val=85.3%  (p=15)\n",
            "  [ClassicFF-Additive] ep 130  loss=1.276  train=85.9%  val=85.4%  (p=9)\n",
            "  [ClassicFF-Additive] ep 140  loss=1.283  train=85.9%  val=85.2%  (p=19)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 141 (#4)\n",
            "  [ClassicFF-Additive] ep 150  loss=1.274  train=86.0%  val=85.5%  (p=29)\n",
            "  [ClassicFF-Additive] ep 160  loss=1.274  train=86.0%  val=85.4%  (p=8)\n",
            "  [ClassicFF-Additive] ep 170  loss=1.275  train=86.0%  val=85.4%  (p=5)\n",
            "  [ClassicFF-Additive] ep 180  loss=1.270  train=86.0%  val=85.4%  (p=3)\n",
            "  [ClassicFF-Additive] ep 190  loss=1.274  train=86.0%  val=85.5%  (p=13)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 197 (#5)\n",
            "  [ClassicFF-Additive] ep 200  loss=1.272  train=86.0%  val=85.6%  (p=23)\n",
            "  [ClassicFF-Additive] ep 210  loss=1.273  train=86.1%  val=85.5%  (p=33)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 217 (#6)\n",
            "  [ClassicFF-Additive] ep 220  loss=1.272  train=86.1%  val=85.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep 230  loss=1.276  train=86.1%  val=85.5%  (p=10)\n",
            "  [ClassicFF-Additive] ep 240  loss=1.273  train=86.1%  val=85.4%  (p=1)\n",
            "  [ClassicFF-Additive] ep 250  loss=1.269  train=86.1%  val=85.5%  (p=11)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 259 (#7)\n",
            "  [ClassicFF-Additive] ep 260  loss=1.274  train=86.0%  val=85.6%  (p=21)\n",
            "  [ClassicFF-Additive] ep 270  loss=1.274  train=86.1%  val=85.5%  (p=31)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 279 (#8)\n",
            "  [ClassicFF-Additive] ep 280  loss=1.274  train=86.1%  val=85.5%  (p=41)\n",
            "  [ClassicFF-Additive] ep 290  loss=1.266  train=86.1%  val=85.5%  (p=51)\n",
            "  [ClassicFF-Additive] Early stop at epoch 299\n",
            "  [ClassicFF-Additive] DONE  test=83.85%  569s  648002 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.888  train=37.4%  val=37.5%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.322  train=79.0%  val=79.3%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.782  train=53.9%  val=54.2%  (p=5)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.121  train=77.7%  val=78.2%  (p=3)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.067  train=84.5%  val=84.4%  (p=6)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.040  train=85.6%  val=85.0%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.084  train=86.5%  val=85.9%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.069  train=85.3%  val=84.4%  (p=10)\n",
            "  [ClassicFF+LA] LR reduced at epoch 80 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.029  train=86.4%  val=85.6%  (p=20)\n",
            "  [ClassicFF+LA a=0.5] ep  90  loss=2.012  train=86.4%  val=85.3%  (p=30)\n",
            "  [ClassicFF+LA] LR reduced at epoch 100 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep 100  loss=2.008  train=86.5%  val=85.3%  (p=40)\n",
            "  [ClassicFF+LA a=0.5] ep 110  loss=1.994  train=86.7%  val=85.6%  (p=50)\n",
            "  [ClassicFF+LA a=0.5] ep 120  loss=1.997  train=86.8%  val=85.7%  (p=1)\n",
            "  [ClassicFF+LA a=0.5] ep 130  loss=1.981  train=86.7%  val=85.6%  (p=11)\n",
            "  [ClassicFF+LA] LR reduced at epoch 139 (#3)\n",
            "  [ClassicFF+LA a=0.5] ep 140  loss=1.975  train=86.6%  val=85.5%  (p=21)\n",
            "  [ClassicFF+LA a=0.5] ep 150  loss=1.972  train=85.6%  val=84.6%  (p=31)\n",
            "  [ClassicFF+LA] LR reduced at epoch 159 (#4)\n",
            "  [ClassicFF+LA a=0.5] ep 160  loss=1.975  train=86.7%  val=85.6%  (p=41)\n",
            "  [ClassicFF+LA a=0.5] ep 170  loss=1.971  train=86.7%  val=85.5%  (p=51)\n",
            "  [ClassicFF+LA] Early stop at epoch 179\n",
            "  [ClassicFF+LA a=0.5] DONE  test=84.42%  393s  648002 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.644  train=82.2%  val=82.2%  (p=0)\n",
            "  [BP]       ep  10  loss=0.293  train=89.0%  val=87.9%  (p=0)\n",
            "  [BP]       ep  20  loss=0.284  train=79.2%  val=77.7%  (p=7)\n",
            "  [BP]       ep  30  loss=0.276  train=86.6%  val=84.4%  (p=17)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 33 (reduction #1)\n",
            "  [BP]       ep  40  loss=0.218  train=92.7%  val=88.8%  (p=5)\n",
            "  [BP]       ep  50  loss=0.191  train=93.0%  val=88.7%  (p=15)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 55 (reduction #2)\n",
            "  [BP]       ep  60  loss=0.152  train=94.4%  val=89.1%  (p=25)\n",
            "  [BP]       ep  70  loss=0.135  train=94.6%  val=89.0%  (p=35)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 75 (reduction #3)\n",
            "  [BP]       ep  80  loss=0.111  train=95.9%  val=89.3%  (p=0)\n",
            "  [BP]       ep  90  loss=0.102  train=96.2%  val=88.8%  (p=10)\n",
            "  [BP]       LR reduced (×0.5) -> 6.3e-04 at epoch 100 (reduction #4)\n",
            "  [BP]       ep 100  loss=0.094  train=96.6%  val=88.9%  (p=20)\n",
            "  [BP]       ep 110  loss=0.087  train=96.9%  val=88.7%  (p=30)\n",
            "  [BP]       LR reduced (×0.5) -> 3.1e-04 at epoch 120 (reduction #5)\n",
            "  [BP]       ep 120  loss=0.083  train=97.1%  val=89.0%  (p=40)\n",
            "  [BP]       ep 130  loss=0.076  train=97.3%  val=88.8%  (p=50)\n",
            "  [BP]       ep 140  loss=0.073  train=97.3%  val=88.8%  (p=60)\n",
            "  [BP]       Early stop at epoch 140 (best_val=89.3%, 5 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=89.3%)\n",
            "  [BP]       DONE  test=88.25%  111s  648010 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.265  train=74.1%  val=74.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.595  train=84.8%  val=85.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.564  train=86.6%  val=86.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.542  train=87.4%  val=87.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.529  train=88.2%  val=87.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.520  train=88.2%  val=87.6%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.525  train=89.1%  val=88.0%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.508  train=89.1%  val=88.1%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.509  train=89.7%  val=88.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.501  train=89.7%  val=88.5%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.504  train=90.0%  val=88.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.496  train=90.4%  val=88.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.495  train=90.3%  val=88.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.494  train=90.8%  val=88.9%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.491  train=90.8%  val=88.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.488  train=91.2%  val=89.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.487  train=91.0%  val=88.7%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.488  train=91.1%  val=89.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.484  train=91.5%  val=89.1%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 190 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.481  train=91.5%  val=89.1%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.480  train=91.7%  val=89.2%  (p=30)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.481  train=91.8%  val=89.3%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.476  train=91.8%  val=89.3%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.478  train=91.9%  val=89.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.477  train=92.0%  val=89.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.476  train=92.0%  val=89.2%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 257 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.474  train=92.1%  val=89.5%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.474  train=92.1%  val=89.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.474  train=92.2%  val=89.4%  (p=13)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.473  train=92.2%  val=89.4%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.475  train=92.3%  val=89.5%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 303 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.471  train=92.2%  val=89.5%  (p=27)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.473  train=92.3%  val=89.4%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 323 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.472  train=92.3%  val=89.5%  (p=47)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.473  train=92.4%  val=89.6%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 343 (best_val=89.6%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=89.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=88.2%, calibrated=85.2%, linear=88.0%, mlp=87.9%, temperature=88.0%\n",
            "    test=88.18%  548s  418000 params\n",
            "    Goodness (spec0): L0:g+=8.486/g-=0.132/sep=8.355/th=1.000 | L1:g+=4.278/g-=0.327/sep=3.951/th=1.000\n",
            "    Avg specialist: acc=89.5%, separation=20.8\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.422  train=71.5%  val=72.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.908  train=83.6%  val=84.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.831  train=85.8%  val=86.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.809  train=86.5%  val=86.5%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.797  train=87.2%  val=86.9%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.792  train=86.8%  val=86.7%  (p=11)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.796  train=79.2%  val=78.4%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.783  train=87.6%  val=86.9%  (p=12)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.779  train=88.1%  val=87.2%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.770  train=87.7%  val=87.1%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.774  train=87.9%  val=87.2%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 105 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.768  train=86.3%  val=85.6%  (p=25)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.771  train=87.3%  val=86.3%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 125 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.768  train=88.2%  val=87.2%  (p=45)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.764  train=88.4%  val=87.3%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 145 (best_val=87.5%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=87.5%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=85.9%, calibrated=82.8%, linear=85.5%, mlp=86.0%, temperature=85.9%\n",
            "    test=85.86%  268s  418000 params\n",
            "    Goodness (spec0): L0:g+=7.385/g-=0.164/sep=7.220/th=1.000 | L1:g+=3.479/g-=0.125/sep=3.353/th=1.000\n",
            "    Avg specialist: acc=88.7%, separation=16.2\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.533  train=72.7%  val=72.9%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.141  train=81.5%  val=81.4%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.102  train=84.1%  val=83.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.078  train=83.0%  val=82.7%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 40 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.060  train=82.2%  val=81.2%  (p=20)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.049  train=82.4%  val=82.1%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 60 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.042  train=84.3%  val=83.7%  (p=40)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.031  train=83.6%  val=83.3%  (p=50)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=1.031  train=84.3%  val=83.4%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=1.022  train=85.2%  val=84.2%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 92 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=1.021  train=84.7%  val=84.0%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=1.021  train=84.2%  val=83.3%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 112 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=1.020  train=84.1%  val=83.3%  (p=48)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=1.017  train=83.9%  val=83.0%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 132 (best_val=84.4%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=82.9%, calibrated=80.4%, linear=83.8%, mlp=84.6%, temperature=84.0%\n",
            "    test=82.88%  244s  418000 params\n",
            "    Goodness (spec0): L0:g+=5.139/g-=0.145/sep=4.994/th=1.000 | L1:g+=1.396/g-=0.184/sep=1.211/th=1.000\n",
            "    Avg specialist: acc=89.7%, separation=8.8\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.242  train=73.5%  val=74.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.589  train=84.7%  val=84.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.555  train=86.5%  val=86.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.533  train=87.2%  val=87.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.522  train=88.4%  val=87.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.512  train=88.5%  val=87.6%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.510  train=89.3%  val=88.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.500  train=89.5%  val=88.6%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.499  train=89.9%  val=88.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.491  train=90.0%  val=88.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.493  train=90.2%  val=88.5%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.487  train=90.5%  val=88.8%  (p=13)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.486  train=90.7%  val=88.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.483  train=91.0%  val=89.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.479  train=91.1%  val=88.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.478  train=91.4%  val=89.1%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 153 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.474  train=91.6%  val=89.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.474  train=91.6%  val=89.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.472  train=91.7%  val=89.3%  (p=13)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.472  train=91.8%  val=89.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.474  train=91.8%  val=89.1%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 205 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.472  train=92.1%  val=89.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.467  train=92.0%  val=89.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.470  train=92.1%  val=89.5%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 236 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.473  train=92.1%  val=89.4%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.476  train=91.5%  val=89.0%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.473  train=91.6%  val=89.1%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 264 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.471  train=91.8%  val=89.3%  (p=26)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.478  train=91.6%  val=89.0%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 284 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.475  train=91.7%  val=89.1%  (p=46)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.475  train=91.9%  val=89.1%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 304 (best_val=89.5%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=89.5%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=87.9%, calibrated=85.6%, linear=87.9%, mlp=87.9%, temperature=87.9%\n",
            "    test=87.95%  487s  886000 params\n",
            "    Goodness (spec0): L0:g+=8.666/g-=0.086/sep=8.581/th=1.000 | L1:g+=4.924/g-=0.229/sep=4.694/th=1.000\n",
            "    Avg specialist: acc=89.6%, separation=19.8\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.388  train=73.6%  val=74.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.858  train=84.8%  val=85.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.821  train=85.8%  val=86.0%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.800  train=87.1%  val=87.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.788  train=87.6%  val=87.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.783  train=87.6%  val=86.9%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.779  train=88.0%  val=87.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.771  train=88.4%  val=87.6%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.771  train=88.5%  val=87.3%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.785  train=86.2%  val=85.3%  (p=13)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.771  train=88.7%  val=87.6%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.770  train=80.5%  val=79.5%  (p=13)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.760  train=89.4%  val=87.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.754  train=89.4%  val=87.8%  (p=13)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.749  train=89.5%  val=87.9%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.755  train=89.4%  val=87.6%  (p=18)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.767  train=89.5%  val=87.4%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.764  train=89.6%  val=87.7%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 172 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.805  train=87.1%  val=85.5%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.774  train=88.9%  val=87.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 192 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.806  train=86.9%  val=85.4%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.884  train=82.4%  val=81.5%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 212 (best_val=88.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=88.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=86.6%, calibrated=82.5%, linear=86.4%, mlp=86.5%, temperature=86.6%\n",
            "    test=86.58%  395s  886000 params\n",
            "    Goodness (spec0): L0:g+=7.527/g-=0.102/sep=7.424/th=1.000 | L1:g+=4.122/g-=0.165/sep=3.957/th=1.000\n",
            "    Avg specialist: acc=88.4%, separation=20.5\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.517  train=71.5%  val=71.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.101  train=80.0%  val=79.9%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.062  train=83.6%  val=83.4%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.038  train=83.4%  val=83.2%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.017  train=83.3%  val=82.5%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.005  train=83.7%  val=83.3%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 51 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.005  train=83.8%  val=82.9%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.001  train=82.5%  val=81.6%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 71 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.998  train=84.0%  val=83.1%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.993  train=83.2%  val=82.3%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 91 (best_val=84.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.8%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=83.1%, calibrated=80.5%, linear=83.4%, mlp=84.2%, temperature=83.4%\n",
            "    test=83.13%  169s  886000 params\n",
            "    Goodness (spec0): L0:g+=4.960/g-=0.336/sep=4.624/th=1.000 | L1:g+=1.951/g-=0.176/sep=1.775/th=1.000\n",
            "    Avg specialist: acc=88.9%, separation=9.7\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/FashionMNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- FashionMNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 85.5% (648,002 params)\n",
            "  ClassicFF_SGD: 52.4% (648,002 params)\n",
            "  ClassicFF_Embed: 47.7% (644,022 params)\n",
            "  ClassicFF_Additive: 83.9% (648,002 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 84.4% (648,002 params)\n",
            "  BP: 88.2% (648,010 params)\n",
            "  ModularFF_50_50_a0.0: 88.2% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 85.9% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 82.9% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 87.9% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 86.6% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 83.1% (886,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: FashionMNIST | ACTIVATION: tanh\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  FashionMNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.458  train=49.0%  val=48.7%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.955  train=68.2%  val=68.1%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.927  train=74.9%  val=74.6%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.917  train=75.0%  val=74.7%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.903  train=75.6%  val=74.8%  (p=12)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 48 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.898  train=77.3%  val=76.1%  (p=22)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.895  train=78.1%  val=77.0%  (p=9)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.891  train=78.3%  val=77.1%  (p=19)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 71 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.888  train=77.9%  val=76.6%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.887  train=78.6%  val=77.0%  (p=15)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.891  train=78.3%  val=76.8%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.888  train=77.8%  val=76.4%  (p=17)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.3e-03 at epoch 113 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.889  train=78.2%  val=76.5%  (p=27)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.887  train=78.6%  val=76.8%  (p=37)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 6.3e-04 at epoch 133 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.883  train=78.6%  val=76.9%  (p=47)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.882  train=78.7%  val=76.8%  (p=57)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 153 (best_val=77.6%, 4 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=77.6%)\n",
            "  [ClassicFF_ADAM] DONE  test=76.77%  283s  648002 params  (LR=0.01, adam, act=tanh)\n",
            "    Goodness: L0:g+=0.194/g-=-0.345/sep=0.539/th=-0.076 | L1:g+=0.362/g-=-0.340/sep=0.702/th=0.034\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.773  train=10.0%  val=10.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.772  train=10.0%  val=10.0%  (p=9)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.772  train=10.0%  val=10.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.772  train=10.0%  val=10.0%  (p=8)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.772  train=10.3%  val=10.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.772  train=11.0%  val=10.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.772  train=12.4%  val=12.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.772  train=14.7%  val=14.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.772  train=20.5%  val=20.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.771  train=24.1%  val=24.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.771  train=25.6%  val=25.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.771  train=26.3%  val=26.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 120  loss=2.771  train=27.2%  val=27.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 130  loss=2.771  train=27.5%  val=27.3%  (p=6)\n",
            "  [ClassicFF_SGD] ep 140  loss=2.771  train=27.7%  val=27.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 150  loss=2.770  train=27.7%  val=27.6%  (p=5)\n",
            "  [ClassicFF_SGD] ep 160  loss=2.770  train=27.7%  val=27.6%  (p=5)\n",
            "  [ClassicFF_SGD] ep 170  loss=2.770  train=27.6%  val=27.5%  (p=15)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 175 (reduction #1)\n",
            "  [ClassicFF_SGD] ep 180  loss=2.770  train=27.7%  val=27.7%  (p=25)\n",
            "  [ClassicFF_SGD] ep 190  loss=2.770  train=27.8%  val=27.6%  (p=35)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 195 (reduction #2)\n",
            "  [ClassicFF_SGD] ep 200  loss=2.770  train=27.8%  val=27.6%  (p=45)\n",
            "  [ClassicFF_SGD] ep 210  loss=2.770  train=27.8%  val=27.6%  (p=55)\n",
            "  [ClassicFF_SGD] Early stop at epoch 215 (best_val=27.7%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=27.7%)\n",
            "  [ClassicFF_SGD] DONE  test=27.37%  382s  648002 params  (LR=0.03, sgd, act=tanh)\n",
            "    Goodness: L0:g+=0.001/g-=-0.001/sep=0.002/th=0.004 | L1:g+=0.001/g-=0.001/sep=-0.000/th=0.001\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.625  train=34.4%  val=34.2%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.142  train=68.4%  val=67.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.113  train=73.9%  val=73.0%  (p=0)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.091  train=76.0%  val=75.0%  (p=3)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.071  train=77.5%  val=76.3%  (p=6)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.058  train=77.4%  val=76.3%  (p=1)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.065  train=78.3%  val=76.9%  (p=7)\n",
            "  [ClassicFF-Embed] ep  70  loss=2.052  train=77.0%  val=75.5%  (p=17)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 73 (#1)\n",
            "  [ClassicFF-Embed] ep  80  loss=2.039  train=78.9%  val=77.1%  (p=27)\n",
            "  [ClassicFF-Embed] ep  90  loss=2.037  train=78.9%  val=77.0%  (p=37)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 93 (#2)\n",
            "  [ClassicFF-Embed] ep 100  loss=2.032  train=79.2%  val=77.1%  (p=47)\n",
            "  [ClassicFF-Embed] ep 110  loss=2.030  train=79.8%  val=77.5%  (p=57)\n",
            "  [ClassicFF-Embed] Early stop at epoch 113\n",
            "  [ClassicFF-Embed] DONE  test=77.11%  213s  644022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.416  train=80.6%  val=80.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.107  train=83.3%  val=82.9%  (p=0)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.071  train=84.2%  val=83.8%  (p=6)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.055  train=82.6%  val=82.0%  (p=16)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.046  train=83.4%  val=82.7%  (p=4)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.038  train=84.2%  val=83.4%  (p=14)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.028  train=85.0%  val=84.5%  (p=5)\n",
            "  [ClassicFF-Additive] ep  70  loss=2.034  train=84.2%  val=83.2%  (p=15)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 75 (#1)\n",
            "  [ClassicFF-Additive] ep  80  loss=2.022  train=85.1%  val=84.0%  (p=2)\n",
            "  [ClassicFF-Additive] ep  90  loss=2.022  train=85.3%  val=84.4%  (p=12)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 98 (#2)\n",
            "  [ClassicFF-Additive] ep 100  loss=2.012  train=86.4%  val=84.9%  (p=22)\n",
            "  [ClassicFF-Additive] ep 110  loss=2.011  train=86.3%  val=84.8%  (p=32)\n",
            "  [ClassicFF-Additive] ep 120  loss=2.010  train=86.4%  val=84.7%  (p=6)\n",
            "  [ClassicFF-Additive] ep 130  loss=2.010  train=86.7%  val=85.0%  (p=16)\n",
            "  [ClassicFF-Additive] ep 140  loss=2.013  train=86.5%  val=84.9%  (p=9)\n",
            "  [ClassicFF-Additive] ep 150  loss=2.005  train=86.9%  val=85.2%  (p=0)\n",
            "  [ClassicFF-Additive] ep 160  loss=2.007  train=86.2%  val=84.4%  (p=10)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 170 (#3)\n",
            "  [ClassicFF-Additive] ep 170  loss=2.006  train=87.0%  val=85.1%  (p=20)\n",
            "  [ClassicFF-Additive] ep 180  loss=2.002  train=86.7%  val=84.7%  (p=30)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 190 (#4)\n",
            "  [ClassicFF-Additive] ep 190  loss=2.000  train=86.8%  val=84.8%  (p=40)\n",
            "  [ClassicFF-Additive] ep 200  loss=2.002  train=86.8%  val=84.8%  (p=50)\n",
            "  [ClassicFF-Additive] ep 210  loss=2.000  train=86.9%  val=84.8%  (p=60)\n",
            "  [ClassicFF-Additive] Early stop at epoch 210\n",
            "  [ClassicFF-Additive] DONE  test=83.52%  390s  648002 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.643  train=53.9%  val=54.0%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.448  train=72.2%  val=72.6%  (p=4)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.439  train=69.7%  val=69.8%  (p=14)\n",
            "  [ClassicFF+LA] LR reduced at epoch 26 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.434  train=78.8%  val=78.6%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.427  train=73.5%  val=73.4%  (p=10)\n",
            "  [ClassicFF+LA] LR reduced at epoch 50 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.426  train=74.7%  val=74.6%  (p=20)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.421  train=77.5%  val=77.8%  (p=30)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.418  train=77.9%  val=77.9%  (p=5)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.418  train=74.9%  val=74.9%  (p=15)\n",
            "  [ClassicFF+LA] LR reduced at epoch 85 (#3)\n",
            "  [ClassicFF+LA a=0.5] ep  90  loss=2.416  train=76.0%  val=75.9%  (p=2)\n",
            "  [ClassicFF+LA a=0.5] ep 100  loss=2.418  train=78.6%  val=78.4%  (p=6)\n",
            "  [ClassicFF+LA a=0.5] ep 110  loss=2.417  train=78.0%  val=77.8%  (p=16)\n",
            "  [ClassicFF+LA] LR reduced at epoch 114 (#4)\n",
            "  [ClassicFF+LA a=0.5] ep 120  loss=2.416  train=77.6%  val=77.6%  (p=26)\n",
            "  [ClassicFF+LA a=0.5] ep 130  loss=2.415  train=78.6%  val=78.8%  (p=36)\n",
            "  [ClassicFF+LA] LR reduced at epoch 134 (#5)\n",
            "  [ClassicFF+LA a=0.5] ep 140  loss=2.412  train=78.6%  val=78.6%  (p=46)\n",
            "  [ClassicFF+LA a=0.5] ep 150  loss=2.411  train=78.4%  val=78.6%  (p=56)\n",
            "  [ClassicFF+LA] Early stop at epoch 154\n",
            "  [ClassicFF+LA a=0.5] DONE  test=78.04%  322s  648002 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.724  train=69.7%  val=69.8%  (p=0)\n",
            "  [BP]       ep  10  loss=0.677  train=77.7%  val=78.0%  (p=2)\n",
            "  [BP]       ep  20  loss=0.722  train=35.5%  val=35.8%  (p=12)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 28 (reduction #1)\n",
            "  [BP]       ep  30  loss=0.556  train=61.7%  val=61.4%  (p=1)\n",
            "  [BP]       ep  40  loss=0.709  train=75.4%  val=74.8%  (p=8)\n",
            "  [BP]       ep  50  loss=0.565  train=81.6%  val=80.8%  (p=18)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 52 (reduction #2)\n",
            "  [BP]       ep  60  loss=0.452  train=73.7%  val=73.1%  (p=1)\n",
            "  [BP]       ep  70  loss=0.418  train=86.7%  val=85.7%  (p=11)\n",
            "  [BP]       ep  80  loss=0.412  train=87.6%  val=86.2%  (p=0)\n",
            "  [BP]       ep  90  loss=0.369  train=88.4%  val=87.1%  (p=0)\n",
            "  [BP]       ep 100  loss=0.348  train=88.3%  val=86.2%  (p=10)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 110 (reduction #3)\n",
            "  [BP]       ep 110  loss=0.342  train=86.9%  val=84.9%  (p=20)\n",
            "  [BP]       ep 120  loss=0.281  train=90.1%  val=86.7%  (p=1)\n",
            "  [BP]       ep 130  loss=0.263  train=91.1%  val=87.6%  (p=1)\n",
            "  [BP]       ep 140  loss=0.255  train=91.9%  val=87.6%  (p=11)\n",
            "  [BP]       LR reduced (×0.5) -> 6.3e-04 at epoch 149 (reduction #4)\n",
            "  [BP]       ep 150  loss=0.215  train=93.2%  val=88.1%  (p=0)\n",
            "  [BP]       ep 160  loss=0.195  train=93.8%  val=87.8%  (p=10)\n",
            "  [BP]       ep 170  loss=0.185  train=94.2%  val=88.1%  (p=5)\n",
            "  [BP]       ep 180  loss=0.166  train=94.8%  val=88.2%  (p=0)\n",
            "  [BP]       ep 190  loss=0.153  train=95.3%  val=88.3%  (p=6)\n",
            "  [BP]       ep 200  loss=0.147  train=95.4%  val=88.1%  (p=16)\n",
            "  [BP]       LR reduced (×0.5) -> 3.1e-04 at epoch 204 (reduction #5)\n",
            "  [BP]       ep 210  loss=0.124  train=96.3%  val=88.2%  (p=26)\n",
            "  [BP]       ep 220  loss=0.116  train=96.5%  val=88.1%  (p=9)\n",
            "  [BP]       ep 230  loss=0.111  train=96.8%  val=88.2%  (p=19)\n",
            "  [BP]       LR reduced (×0.5) -> 1.6e-04 at epoch 231 (reduction #6)\n",
            "  [BP]       ep 240  loss=0.109  train=96.9%  val=88.4%  (p=29)\n",
            "  [BP]       ep 250  loss=0.099  train=97.1%  val=88.1%  (p=39)\n",
            "  [BP]       LR reduced (×0.5) -> 7.8e-05 at epoch 251 (reduction #7)\n",
            "  [BP]       ep 260  loss=0.096  train=97.2%  val=88.2%  (p=49)\n",
            "  [BP]       ep 270  loss=0.096  train=97.3%  val=88.2%  (p=59)\n",
            "  [BP]       Early stop at epoch 271 (best_val=88.4%, 7 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=88.4%)\n",
            "  [BP]       DONE  test=86.71%  215s  648010 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.026  train=66.8%  val=67.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.746  train=81.6%  val=82.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.734  train=83.2%  val=83.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.726  train=83.5%  val=83.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.721  train=84.9%  val=85.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.717  train=85.2%  val=84.8%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.716  train=85.4%  val=85.2%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 63 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.713  train=85.7%  val=85.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.712  train=85.7%  val=85.3%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.711  train=85.7%  val=85.4%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.712  train=85.9%  val=85.6%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.709  train=86.0%  val=85.5%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.709  train=86.0%  val=85.4%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.708  train=86.1%  val=85.6%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 137 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.705  train=86.2%  val=85.7%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.705  train=86.3%  val=85.7%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.705  train=86.3%  val=85.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.707  train=86.3%  val=85.7%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 171 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.705  train=86.4%  val=85.7%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.705  train=86.4%  val=85.7%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.706  train=86.4%  val=85.7%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 205 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.707  train=86.4%  val=85.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.704  train=86.4%  val=85.8%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 227 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.706  train=86.4%  val=85.7%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.705  train=86.5%  val=85.8%  (p=33)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.705  train=86.5%  val=85.8%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.705  train=86.5%  val=85.8%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.705  train=86.5%  val=85.9%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.6e-04 at epoch 277 (reduction #6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.704  train=86.5%  val=85.8%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.704  train=86.5%  val=85.8%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> 7.8e-05 at epoch 297 (reduction #7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.705  train=86.5%  val=85.8%  (p=43)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.703  train=86.5%  val=85.9%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 317 (best_val=85.9%, 7 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=85.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=84.0%, calibrated=81.8%, linear=82.2%, mlp=83.1%, temperature=82.7%\n",
            "    test=83.98%  469s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.977/g-=-0.903/sep=1.879/th=0.000 | L1:g+=0.992/g-=-0.913/sep=1.905/th=0.000\n",
            "    Avg specialist: acc=94.4%, separation=3.5\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.246  train=45.6%  val=46.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.090  train=75.3%  val=75.5%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.081  train=78.7%  val=78.7%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.080  train=78.0%  val=77.8%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.081  train=78.1%  val=78.1%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 42 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.075  train=79.6%  val=79.6%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.077  train=77.5%  val=77.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 62 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.075  train=79.3%  val=79.0%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.077  train=79.6%  val=79.2%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 82 (best_val=80.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=80.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=78.7%, calibrated=79.0%, linear=76.5%, mlp=80.3%, temperature=80.2%\n",
            "    test=78.74%  142s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.676/g-=-0.322/sep=0.998/th=0.000 | L1:g+=0.933/g-=-0.224/sep=1.157/th=0.000\n",
            "    Avg specialist: acc=91.4%, separation=2.2\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.337  train=24.5%  val=24.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.216  train=25.7%  val=25.4%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.219  train=25.2%  val=24.8%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.220  train=25.7%  val=25.2%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 32 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.218  train=26.1%  val=25.7%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.218  train=25.8%  val=25.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 52 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.218  train=25.7%  val=25.2%  (p=48)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.218  train=26.1%  val=25.6%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 72 (best_val=26.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=26.1%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=26.1%, calibrated=0.1%, linear=50.4%, mlp=63.2%, temperature=26.9%\n",
            "    test=26.06%  126s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.149/g-=0.013/sep=0.137/th=0.000 | L1:g+=0.000/g-=0.000/sep=-0.000/th=0.000\n",
            "    Avg specialist: acc=53.7%, separation=-0.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.999  train=68.9%  val=69.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.746  train=81.6%  val=82.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.735  train=83.0%  val=83.4%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.727  train=83.3%  val=83.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.723  train=84.0%  val=84.1%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.718  train=84.0%  val=83.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.717  train=85.3%  val=85.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.714  train=85.5%  val=85.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.714  train=85.6%  val=85.0%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.712  train=85.8%  val=85.2%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.711  train=85.8%  val=85.4%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.708  train=86.2%  val=85.4%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.709  train=86.0%  val=85.1%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.708  train=86.3%  val=85.4%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.705  train=86.4%  val=85.5%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.705  train=86.5%  val=85.5%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.704  train=86.5%  val=85.6%  (p=15)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.705  train=86.6%  val=85.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.704  train=86.4%  val=85.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.703  train=86.8%  val=85.8%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.704  train=86.9%  val=85.6%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 207 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.703  train=86.9%  val=85.7%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.700  train=87.1%  val=85.6%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 227 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.701  train=87.0%  val=85.7%  (p=43)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.700  train=87.1%  val=85.8%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 247 (best_val=85.9%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=85.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=84.0%, calibrated=82.1%, linear=82.7%, mlp=82.7%, temperature=82.9%\n",
            "    test=84.04%  370s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.961/g-=-0.888/sep=1.850/th=0.000 | L1:g+=0.969/g-=-0.896/sep=1.865/th=0.000\n",
            "    Avg specialist: acc=94.3%, separation=3.5\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.231  train=51.3%  val=51.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.088  train=76.9%  val=77.4%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.082  train=79.4%  val=80.0%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.081  train=79.1%  val=79.3%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.081  train=78.1%  val=78.7%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.079  train=79.8%  val=79.9%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.081  train=78.6%  val=79.2%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 69 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.077  train=79.9%  val=79.8%  (p=21)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.077  train=80.0%  val=79.9%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 89 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=1.076  train=79.8%  val=79.9%  (p=41)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=1.076  train=80.1%  val=80.2%  (p=51)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=1.075  train=79.4%  val=79.5%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=1.075  train=79.9%  val=79.9%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 124 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=1.074  train=80.0%  val=80.1%  (p=26)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=1.074  train=80.1%  val=80.1%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 144 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=1.073  train=79.7%  val=79.6%  (p=46)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=1.073  train=79.6%  val=79.6%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 164 (best_val=80.6%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=80.6%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=79.1%, calibrated=79.3%, linear=76.5%, mlp=80.4%, temperature=80.6%\n",
            "    test=79.12%  289s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.738/g-=-0.358/sep=1.097/th=0.000 | L1:g+=0.965/g-=-0.306/sep=1.271/th=0.000\n",
            "    Avg specialist: acc=91.7%, separation=2.2\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.325  train=14.6%  val=14.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.217  train=17.5%  val=18.2%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.219  train=13.7%  val=13.8%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.219  train=22.1%  val=22.5%  (p=16)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.220  train=19.3%  val=20.0%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.218  train=17.1%  val=18.0%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 51 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.220  train=17.4%  val=17.7%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.218  train=19.5%  val=20.1%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 71 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=1.219  train=16.3%  val=16.8%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=1.219  train=17.1%  val=17.8%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 91 (best_val=23.2%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=23.2%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=22.9%, calibrated=0.2%, linear=41.5%, mlp=63.8%, temperature=39.2%\n",
            "    test=22.93%  159s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.035/g-=0.004/sep=0.032/th=0.000 | L1:g+=0.070/g-=0.039/sep=0.030/th=0.000\n",
            "    Avg specialist: acc=51.6%, separation=0.0\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/FashionMNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- FashionMNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 76.8% (648,002 params)\n",
            "  ClassicFF_SGD: 27.4% (648,002 params)\n",
            "  ClassicFF_Embed: 77.1% (644,022 params)\n",
            "  ClassicFF_Additive: 83.5% (648,002 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 78.0% (648,002 params)\n",
            "  BP: 86.7% (648,010 params)\n",
            "  ModularFF_50_50_a0.0: 84.0% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 78.7% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 26.1% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 84.0% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 79.1% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 22.9% (886,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: FashionMNIST | ACTIVATION: hardlimit\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  FashionMNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.756  train=26.3%  val=26.8%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=2.716  train=18.3%  val=18.3%  (p=8)\n",
            "  [ClassicFF_ADAM] ep  20  loss=2.720  train=19.5%  val=19.4%  (p=18)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 22 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=2.717  train=19.5%  val=19.2%  (p=28)\n",
            "  [ClassicFF_ADAM] ep  40  loss=2.716  train=19.7%  val=19.1%  (p=38)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 42 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  50  loss=2.712  train=20.9%  val=20.9%  (p=48)\n",
            "  [ClassicFF_ADAM] ep  60  loss=2.714  train=21.4%  val=21.7%  (p=58)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 62 (best_val=28.2%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=28.2%)\n",
            "  [ClassicFF_ADAM] DONE  test=27.28%  139s  648002 params  (LR=0.01, adam, act=hardlimit)\n",
            "    Goodness: L0:g+=0.499/g-=0.506/sep=-0.007/th=0.500 | L1:g+=1.000/g-=0.002/sep=0.998/th=0.538\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.773  train=9.8%  val=10.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.773  train=10.0%  val=10.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.773  train=10.0%  val=10.3%  (p=4)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.773  train=10.2%  val=10.1%  (p=14)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 36 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.773  train=10.1%  val=10.1%  (p=24)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.773  train=10.2%  val=9.8%  (p=34)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 56 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.773  train=10.1%  val=10.1%  (p=44)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.773  train=10.2%  val=10.1%  (p=54)\n",
            "  [ClassicFF_SGD] Early stop at epoch 76 (best_val=10.6%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=10.6%)\n",
            "  [ClassicFF_SGD] DONE  test=10.40%  164s  648002 params  (LR=0.03, sgd, act=hardlimit)\n",
            "    Goodness: L0:g+=0.490/g-=0.483/sep=0.007/th=0.497 | L1:g+=0.491/g-=0.485/sep=0.006/th=0.487\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.751  train=24.7%  val=24.6%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.710  train=17.7%  val=17.5%  (p=8)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.713  train=12.1%  val=11.9%  (p=18)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 22 (#1)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.708  train=21.3%  val=21.0%  (p=28)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.709  train=16.3%  val=16.6%  (p=38)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 42 (#2)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.706  train=20.7%  val=21.2%  (p=48)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.707  train=20.1%  val=20.1%  (p=58)\n",
            "  [ClassicFF-Embed] Early stop at epoch 62\n",
            "  [ClassicFF-Embed] DONE  test=25.36%  141s  644022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.599  train=57.8%  val=58.1%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.610  train=28.8%  val=29.3%  (p=9)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.626  train=35.7%  val=36.0%  (p=19)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 21 (#1)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.629  train=33.4%  val=33.7%  (p=29)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.628  train=37.9%  val=38.0%  (p=39)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 41 (#2)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.629  train=41.3%  val=40.9%  (p=49)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.628  train=36.0%  val=36.2%  (p=59)\n",
            "  [ClassicFF-Additive] Early stop at epoch 61\n",
            "  [ClassicFF-Additive] DONE  test=56.44%  138s  648002 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.759  train=23.7%  val=23.3%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.684  train=36.4%  val=36.4%  (p=4)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.683  train=35.5%  val=35.9%  (p=14)\n",
            "  [ClassicFF+LA] LR reduced at epoch 26 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.684  train=31.2%  val=31.2%  (p=24)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.683  train=33.4%  val=33.4%  (p=34)\n",
            "  [ClassicFF+LA] LR reduced at epoch 46 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.681  train=32.0%  val=32.1%  (p=44)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.683  train=32.5%  val=32.5%  (p=54)\n",
            "  [ClassicFF+LA] Early stop at epoch 66\n",
            "  [ClassicFF+LA a=0.5] DONE  test=39.75%  165s  648002 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.862  train=69.9%  val=70.3%  (p=0)\n",
            "  [BP]       ep  10  loss=0.898  train=60.2%  val=59.9%  (p=9)\n",
            "  [BP]       ep  20  loss=0.864  train=64.7%  val=65.1%  (p=19)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [BP]       ep  30  loss=0.799  train=70.0%  val=70.3%  (p=5)\n",
            "  [BP]       ep  40  loss=0.945  train=61.9%  val=62.5%  (p=15)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 45 (reduction #2)\n",
            "  [BP]       ep  50  loss=0.828  train=69.2%  val=69.3%  (p=25)\n",
            "  [BP]       ep  60  loss=0.817  train=68.7%  val=68.8%  (p=35)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 65 (reduction #3)\n",
            "  [BP]       ep  70  loss=0.808  train=68.5%  val=68.2%  (p=45)\n",
            "  [BP]       ep  80  loss=0.809  train=67.9%  val=67.6%  (p=55)\n",
            "  [BP]       Early stop at epoch 85 (best_val=71.7%, 3 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=71.7%)\n",
            "  [BP]       DONE  test=70.24%  76s  648010 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.187  train=39.0%  val=39.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.132  train=34.7%  val=34.8%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.131  train=34.8%  val=35.0%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.132  train=34.6%  val=34.7%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.132  train=34.8%  val=35.0%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.130  train=34.6%  val=34.7%  (p=49)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.133  train=34.7%  val=34.8%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=39.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=39.1%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=38.9%, calibrated=37.6%, linear=55.1%, mlp=59.1%, temperature=40.5%\n",
            "    test=38.91%  106s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.974/g-=0.351/sep=0.623/th=0.500 | L1:g+=0.974/g-=0.364/sep=0.610/th=0.500\n",
            "    Avg specialist: acc=81.9%, separation=1.2\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.260  train=34.3%  val=34.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.204  train=21.6%  val=21.8%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.205  train=21.5%  val=21.7%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.206  train=21.3%  val=21.5%  (p=29)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.205  train=21.5%  val=21.8%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.205  train=21.3%  val=21.5%  (p=49)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.205  train=21.3%  val=21.6%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=34.5%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=34.5%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=34.2%, calibrated=19.1%, linear=48.9%, mlp=57.6%, temperature=26.7%\n",
            "    test=34.23%  123s  418000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.525/sep=0.475/th=0.500 | L1:g+=1.000/g-=0.558/sep=0.442/th=0.500\n",
            "    Avg specialist: acc=77.3%, separation=0.9\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.373  train=18.5%  val=18.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.356  train=19.7%  val=19.8%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.356  train=19.7%  val=19.8%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.356  train=19.7%  val=19.8%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 33 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.356  train=19.7%  val=19.8%  (p=27)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.355  train=19.7%  val=19.8%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 53 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.356  train=19.7%  val=19.8%  (p=47)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.355  train=19.7%  val=19.8%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 73 (best_val=19.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=19.8%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=19.7%, calibrated=15.6%, linear=43.5%, mlp=47.9%, temperature=24.5%\n",
            "    test=19.73%  148s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.700/g-=0.437/sep=0.263/th=0.500 | L1:g+=0.300/g-=0.191/sep=0.109/th=0.500\n",
            "    Avg specialist: acc=73.8%, separation=0.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.173  train=39.5%  val=39.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.132  train=34.7%  val=34.9%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.132  train=34.8%  val=35.0%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.133  train=34.6%  val=34.7%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.132  train=34.9%  val=35.0%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.130  train=34.6%  val=34.8%  (p=49)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.133  train=34.8%  val=34.9%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=39.6%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=39.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=39.2%, calibrated=37.9%, linear=53.8%, mlp=58.0%, temperature=40.8%\n",
            "    test=39.20%  112s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.974/g-=0.351/sep=0.623/th=0.500 | L1:g+=0.974/g-=0.351/sep=0.623/th=0.500\n",
            "    Avg specialist: acc=81.6%, separation=1.2\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.256  train=32.1%  val=32.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.205  train=24.8%  val=25.1%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.205  train=24.7%  val=24.9%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.205  train=24.4%  val=24.6%  (p=29)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.205  train=24.7%  val=24.9%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.205  train=24.5%  val=24.7%  (p=49)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.205  train=24.5%  val=24.7%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=32.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=32.4%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=32.2%, calibrated=27.5%, linear=49.9%, mlp=56.6%, temperature=26.1%\n",
            "    test=32.23%  128s  886000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.525/sep=0.475/th=0.500 | L1:g+=1.000/g-=0.558/sep=0.442/th=0.500\n",
            "    Avg specialist: acc=77.1%, separation=0.9\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.376  train=22.3%  val=22.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.357  train=20.1%  val=20.3%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.356  train=22.2%  val=22.8%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 23 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.357  train=22.2%  val=22.8%  (p=27)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.356  train=22.3%  val=22.9%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 43 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.356  train=22.3%  val=22.8%  (p=47)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.356  train=22.3%  val=22.9%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 63 (best_val=23.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=23.7%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=23.6%, calibrated=18.4%, linear=45.1%, mlp=50.3%, temperature=24.9%\n",
            "    test=23.62%  133s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.750/g-=0.448/sep=0.302/th=0.500 | L1:g+=0.280/g-=0.179/sep=0.101/th=0.500\n",
            "    Avg specialist: acc=73.3%, separation=0.3\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/FashionMNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- FashionMNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 27.3% (648,002 params)\n",
            "  ClassicFF_SGD: 10.4% (648,002 params)\n",
            "  ClassicFF_Embed: 25.4% (644,022 params)\n",
            "  ClassicFF_Additive: 56.4% (648,002 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 39.8% (648,002 params)\n",
            "  BP: 70.2% (648,010 params)\n",
            "  ModularFF_50_50_a0.0: 38.9% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 34.2% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 19.7% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 39.2% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 32.2% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 23.6% (886,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: FashionMNIST | ACTIVATION: perceptron\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  FashionMNIST | seed=42 | K=10 | dim=784\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [500, 500]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.000  train=9.1%  val=9.2%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.999  train=8.8%  val=9.3%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.998  train=10.6%  val=10.7%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.999  train=9.1%  val=8.8%  (p=9)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.997  train=14.2%  val=14.4%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.998  train=8.9%  val=8.8%  (p=7)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.998  train=14.1%  val=14.1%  (p=17)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 63 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.999  train=15.0%  val=15.2%  (p=27)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.998  train=13.2%  val=13.1%  (p=37)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 83 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.998  train=10.7%  val=11.1%  (p=47)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.998  train=12.2%  val=11.8%  (p=57)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 103 (best_val=16.1%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=16.1%)\n",
            "  [ClassicFF_ADAM] DONE  test=16.59%  182s  648000 params  (LR=0.01, adam, act=perceptron)\n",
            "    Goodness: L0:g+=0.747/g-=0.760/sep=-0.013/th=0.500 | L1:g+=0.537/g-=0.526/sep=0.011/th=0.500\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.000  train=8.8%  val=8.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=1.995  train=13.3%  val=13.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=1.992  train=8.2%  val=8.0%  (p=4)\n",
            "  [ClassicFF_SGD] ep  30  loss=1.994  train=6.8%  val=7.1%  (p=9)\n",
            "  [ClassicFF_SGD] ep  40  loss=1.994  train=18.0%  val=17.4%  (p=6)\n",
            "  [ClassicFF_SGD] ep  50  loss=1.992  train=15.6%  val=15.7%  (p=16)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 54 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  60  loss=1.991  train=13.6%  val=13.2%  (p=26)\n",
            "  [ClassicFF_SGD] ep  70  loss=1.992  train=14.2%  val=14.1%  (p=36)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 74 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  80  loss=1.990  train=11.4%  val=11.7%  (p=46)\n",
            "  [ClassicFF_SGD] ep  90  loss=1.988  train=20.2%  val=20.2%  (p=56)\n",
            "  [ClassicFF_SGD] Early stop at epoch 94 (best_val=23.2%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=23.2%)\n",
            "  [ClassicFF_SGD] DONE  test=22.95%  166s  648000 params  (LR=0.03, sgd, act=perceptron)\n",
            "    Goodness: L0:g+=0.969/g-=0.964/sep=0.005/th=0.500 | L1:g+=0.553/g-=0.565/sep=-0.012/th=0.500\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP] Skipped (perceptron activation is FF-specific)\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.875  train=35.1%  val=35.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.826  train=71.7%  val=72.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.817  train=69.2%  val=69.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.807  train=70.9%  val=71.6%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.811  train=56.7%  val=56.8%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.811  train=80.5%  val=80.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.808  train=75.2%  val=75.5%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 70 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.806  train=77.1%  val=76.7%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.806  train=80.2%  val=80.1%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 90 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.806  train=72.4%  val=72.2%  (p=40)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.800  train=65.5%  val=65.7%  (p=50)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.801  train=73.1%  val=72.9%  (p=60)\n",
            "  [ModularFF] Early stop at epoch 110 (best_val=80.9%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=80.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=79.3%, calibrated=79.1%, linear=54.9%, mlp=77.2%, temperature=79.5%\n",
            "    test=79.29%  146s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.622/g-=0.302/sep=0.320/th=0.500 | L1:g+=0.536/g-=0.440/sep=0.096/th=0.500\n",
            "    Avg specialist: acc=91.9%, separation=0.4\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.791  train=54.4%  val=54.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.377  train=79.6%  val=79.9%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.309  train=79.2%  val=79.7%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.281  train=75.2%  val=75.6%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.263  train=74.8%  val=74.6%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.256  train=83.5%  val=83.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.238  train=78.3%  val=78.9%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 70 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.236  train=79.3%  val=79.5%  (p=20)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.208  train=83.0%  val=83.0%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 90 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.219  train=77.0%  val=76.7%  (p=40)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.238  train=83.2%  val=82.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.210  train=76.1%  val=75.8%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.214  train=81.3%  val=81.6%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 126 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.214  train=78.6%  val=78.3%  (p=24)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.213  train=76.4%  val=75.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.198  train=81.6%  val=81.2%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 157 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.192  train=80.9%  val=80.8%  (p=23)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.203  train=72.3%  val=71.8%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 177 (reduction #5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.198  train=76.1%  val=75.4%  (p=43)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.199  train=76.3%  val=75.3%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 197 (best_val=84.2%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.2%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=82.5%, calibrated=81.8%, linear=82.8%, mlp=83.7%, temperature=83.0%\n",
            "    test=82.49%  260s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.979/g-=0.091/sep=0.888/th=0.500 | L1:g+=0.950/g-=0.125/sep=0.825/th=0.500\n",
            "    Avg specialist: acc=93.4%, separation=1.6\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.766  train=69.6%  val=69.9%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.297  train=79.0%  val=79.1%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.245  train=82.3%  val=82.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.220  train=81.6%  val=81.8%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.204  train=80.3%  val=80.1%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.191  train=83.3%  val=83.4%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 57 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.186  train=83.2%  val=83.4%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.182  train=84.7%  val=84.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.167  train=84.3%  val=84.4%  (p=10)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.171  train=78.9%  val=78.5%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.177  train=84.1%  val=83.8%  (p=13)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.169  train=80.2%  val=79.5%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.164  train=84.7%  val=84.4%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 122 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.166  train=84.8%  val=84.3%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.159  train=82.2%  val=81.8%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 142 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.157  train=85.7%  val=84.9%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.160  train=83.9%  val=83.4%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.161  train=84.1%  val=83.7%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 178 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.158  train=83.1%  val=82.4%  (p=22)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.157  train=85.0%  val=84.5%  (p=32)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 198 (reduction #5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.165  train=84.6%  val=84.3%  (p=42)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.165  train=77.8%  val=77.5%  (p=52)\n",
            "  [ModularFF] Early stop at epoch 218 (best_val=84.9%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.9%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=83.6%, calibrated=82.5%, linear=83.0%, mlp=83.6%, temperature=83.4%\n",
            "    test=83.57%  290s  418000 params\n",
            "    Goodness (spec0): L0:g+=0.963/g-=0.126/sep=0.838/th=0.500 | L1:g+=0.959/g-=0.118/sep=0.841/th=0.500\n",
            "    Avg specialist: acc=93.8%, separation=1.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.873  train=63.3%  val=63.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.832  train=73.2%  val=73.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.825  train=79.7%  val=80.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.830  train=66.1%  val=66.5%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.816  train=59.4%  val=59.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.814  train=69.9%  val=70.3%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 56 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.821  train=71.3%  val=71.7%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.821  train=73.7%  val=74.4%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 76 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.820  train=68.4%  val=68.4%  (p=44)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.810  train=75.6%  val=75.9%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 96 (best_val=81.9%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=81.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=80.4%, calibrated=80.2%, linear=62.1%, mlp=78.8%, temperature=80.6%\n",
            "    test=80.41%  132s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.606/g-=0.298/sep=0.308/th=0.500 | L1:g+=0.530/g-=0.446/sep=0.083/th=0.500\n",
            "    Avg specialist: acc=92.2%, separation=0.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.783  train=66.4%  val=66.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.375  train=78.7%  val=78.9%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.305  train=74.6%  val=75.0%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.271  train=75.9%  val=76.1%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 33 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.258  train=75.1%  val=75.3%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.244  train=83.5%  val=83.6%  (p=13)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.232  train=79.7%  val=80.3%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.236  train=80.0%  val=79.9%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 72 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.215  train=77.7%  val=77.5%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.218  train=76.7%  val=76.7%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 92 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.226  train=83.4%  val=83.5%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.219  train=76.2%  val=75.9%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 112 (best_val=84.3%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.3%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=82.4%, calibrated=82.4%, linear=82.0%, mlp=83.6%, temperature=82.9%\n",
            "    test=82.41%  155s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.946/g-=0.093/sep=0.853/th=0.500 | L1:g+=0.935/g-=0.152/sep=0.783/th=0.500\n",
            "    Avg specialist: acc=93.4%, separation=1.5\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.751  train=70.9%  val=71.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.291  train=80.0%  val=80.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.234  train=82.6%  val=82.8%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.208  train=81.0%  val=81.7%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.196  train=80.1%  val=80.1%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.183  train=84.2%  val=84.2%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 57 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.180  train=84.0%  val=84.3%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.175  train=85.0%  val=85.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.165  train=84.5%  val=84.4%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 90 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.169  train=78.7%  val=78.6%  (p=20)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.175  train=84.4%  val=84.0%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 110 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.167  train=80.7%  val=80.0%  (p=40)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.164  train=84.7%  val=83.9%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.166  train=84.7%  val=84.5%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 135 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.158  train=83.2%  val=82.7%  (p=25)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.155  train=85.7%  val=84.9%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 155 (reduction #5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.160  train=83.9%  val=83.3%  (p=45)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.161  train=84.9%  val=84.3%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 175 (best_val=85.1%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=85.1%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=83.2%, calibrated=82.5%, linear=82.7%, mlp=83.7%, temperature=83.5%\n",
            "    test=83.25%  242s  886000 params\n",
            "    Goodness (spec0): L0:g+=0.901/g-=0.022/sep=0.879/th=0.500 | L1:g+=0.883/g-=0.020/sep=0.863/th=0.500\n",
            "    Avg specialist: acc=93.2%, separation=1.6\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/FashionMNIST/results_2layer_seed42.json\n",
            "\n",
            "  --- FashionMNIST 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 16.6% (648,000 params)\n",
            "  ClassicFF_SGD: 22.9% (648,000 params)\n",
            "  ModularFF_50_50_a0.0: 79.3% (418,000 params)\n",
            "  ModularFF_50_50_a0.5: 82.5% (418,000 params)\n",
            "  ModularFF_50_50_a1.0: 83.6% (418,000 params)\n",
            "  ModularFF_100_100_a0.0: 80.4% (886,000 params)\n",
            "  ModularFF_100_100_a0.5: 82.4% (886,000 params)\n",
            "  ModularFF_100_100_a1.0: 83.2% (886,000 params)\n",
            "\n",
            "==========================================================================================\n",
            " FashionMNIST — 2-LAYER CROSS-ACTIVATION SUMMARY\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               88.2%    85.5%      88.2%      +2.7%\n",
            "tanh               86.7%    83.5%      84.0%      +0.5%\n",
            "hardlimit          70.2%    56.4%      39.2%     -17.2%\n",
            "perceptron           N/A    22.9%      83.6%     +60.6%\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n",
            "  DATASET: Pendigits\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: Pendigits | ACTIVATION: gelu\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  Pendigits | seed=42 | K=10 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [200, 200]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.797  train=80.3%  val=80.3%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.325  train=90.4%  val=90.1%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  20  loss=0.681  train=92.9%  val=92.8%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  30  loss=0.455  train=94.9%  val=94.4%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  40  loss=0.355  train=95.8%  val=96.1%  (p=4)\n",
            "  [ClassicFF_ADAM] ep  50  loss=0.288  train=96.5%  val=96.7%  (p=3)\n",
            "  [ClassicFF_ADAM] ep  60  loss=0.272  train=97.0%  val=97.5%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  70  loss=0.230  train=97.1%  val=97.8%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  80  loss=0.217  train=97.4%  val=97.8%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  90  loss=0.201  train=97.7%  val=98.0%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 100  loss=0.190  train=98.1%  val=98.6%  (p=2)\n",
            "  [ClassicFF_ADAM] ep 110  loss=0.186  train=98.3%  val=98.5%  (p=5)\n",
            "  [ClassicFF_ADAM] ep 120  loss=0.164  train=98.3%  val=98.6%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 130  loss=0.166  train=98.5%  val=98.9%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 140  loss=0.158  train=98.5%  val=98.6%  (p=13)\n",
            "  [ClassicFF_ADAM] ep 150  loss=0.153  train=98.6%  val=98.8%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 160  loss=0.156  train=98.8%  val=98.9%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 170  loss=0.141  train=98.8%  val=99.0%  (p=17)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 173 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep 180  loss=0.135  train=98.9%  val=98.9%  (p=27)\n",
            "  [ClassicFF_ADAM] ep 190  loss=0.123  train=98.9%  val=98.9%  (p=37)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 193 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 200  loss=0.133  train=98.8%  val=98.9%  (p=47)\n",
            "  [ClassicFF_ADAM] ep 210  loss=0.125  train=99.0%  val=98.9%  (p=57)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 213 (best_val=99.2%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=99.2%)\n",
            "  [ClassicFF_ADAM] DONE  test=95.68%  51s  45602 params  (LR=0.01, adam, act=gelu)\n",
            "    Goodness: L0:g+=21.120/g-=6.951/sep=14.169/th=14.110 | L1:g+=17.814/g-=5.516/sep=12.298/th=13.001\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.993  train=8.7%  val=9.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.787  train=13.0%  val=13.8%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.781  train=19.1%  val=20.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.776  train=26.1%  val=26.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.773  train=32.1%  val=31.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.770  train=36.8%  val=36.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.766  train=40.6%  val=40.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.764  train=43.4%  val=42.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.761  train=45.4%  val=44.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.759  train=47.4%  val=45.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.756  train=48.8%  val=47.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.753  train=50.4%  val=48.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 120  loss=2.751  train=52.1%  val=50.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 130  loss=2.749  train=53.9%  val=52.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 140  loss=2.747  train=55.5%  val=54.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 150  loss=2.745  train=56.9%  val=55.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 160  loss=2.742  train=58.4%  val=57.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 170  loss=2.740  train=59.4%  val=58.7%  (p=1)\n",
            "  [ClassicFF_SGD] ep 180  loss=2.737  train=60.5%  val=60.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 190  loss=2.735  train=61.7%  val=61.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 200  loss=2.732  train=62.6%  val=61.7%  (p=1)\n",
            "  [ClassicFF_SGD] ep 210  loss=2.731  train=63.4%  val=62.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 220  loss=2.728  train=64.1%  val=62.8%  (p=2)\n",
            "  [ClassicFF_SGD] ep 230  loss=2.726  train=64.7%  val=63.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 240  loss=2.723  train=65.2%  val=63.9%  (p=1)\n",
            "  [ClassicFF_SGD] ep 250  loss=2.721  train=65.6%  val=64.2%  (p=2)\n",
            "  [ClassicFF_SGD] ep 260  loss=2.719  train=66.0%  val=64.6%  (p=3)\n",
            "  [ClassicFF_SGD] ep 270  loss=2.717  train=66.4%  val=64.9%  (p=1)\n",
            "  [ClassicFF_SGD] ep 280  loss=2.714  train=66.8%  val=65.3%  (p=3)\n",
            "  [ClassicFF_SGD] ep 290  loss=2.712  train=67.0%  val=65.8%  (p=0)\n",
            "  [ClassicFF_SGD] ep 300  loss=2.710  train=67.3%  val=66.3%  (p=1)\n",
            "  [ClassicFF_SGD] ep 310  loss=2.708  train=67.4%  val=66.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep 320  loss=2.705  train=67.6%  val=66.7%  (p=3)\n",
            "  [ClassicFF_SGD] ep 330  loss=2.702  train=67.8%  val=67.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 340  loss=2.700  train=68.0%  val=67.6%  (p=2)\n",
            "  [ClassicFF_SGD] ep 350  loss=2.698  train=68.2%  val=67.6%  (p=5)\n",
            "  [ClassicFF_SGD] ep 360  loss=2.695  train=68.4%  val=67.7%  (p=9)\n",
            "  [ClassicFF_SGD] ep 370  loss=2.694  train=68.6%  val=67.9%  (p=2)\n",
            "  [ClassicFF_SGD] ep 380  loss=2.691  train=68.7%  val=68.2%  (p=5)\n",
            "  [ClassicFF_SGD] ep 390  loss=2.688  train=68.8%  val=68.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep 400  loss=2.687  train=68.9%  val=68.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 410  loss=2.684  train=69.1%  val=68.7%  (p=2)\n",
            "  [ClassicFF_SGD] ep 420  loss=2.681  train=69.2%  val=68.9%  (p=5)\n",
            "  [ClassicFF_SGD] ep 430  loss=2.678  train=69.4%  val=68.9%  (p=15)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 435 (reduction #1)\n",
            "  [ClassicFF_SGD] ep 440  loss=2.678  train=69.5%  val=68.9%  (p=25)\n",
            "  [ClassicFF_SGD] ep 450  loss=2.675  train=69.6%  val=69.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 460  loss=2.675  train=69.6%  val=69.1%  (p=3)\n",
            "  [ClassicFF_SGD] ep 470  loss=2.673  train=69.6%  val=69.2%  (p=8)\n",
            "  [ClassicFF_SGD] ep 480  loss=2.672  train=69.7%  val=69.2%  (p=5)\n",
            "  [ClassicFF_SGD] ep 490  loss=2.671  train=69.8%  val=69.3%  (p=8)\n",
            "  [ClassicFF_SGD] ep 500  loss=2.668  train=69.8%  val=69.2%  (p=18)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=69.3%)\n",
            "  [ClassicFF_SGD] DONE  test=65.38%  115s  45602 params  (LR=0.03, sgd, act=gelu)\n",
            "    Goodness: L0:g+=1.040/g-=0.752/sep=0.288/th=0.874 | L1:g+=0.003/g-=0.003/sep=-0.000/th=0.003\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.816  train=27.1%  val=27.5%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.025  train=40.4%  val=39.5%  (p=0)\n",
            "  [ClassicFF-Embed] ep  20  loss=1.762  train=46.4%  val=47.3%  (p=0)\n",
            "  [ClassicFF-Embed] ep  30  loss=1.679  train=47.5%  val=48.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  40  loss=1.561  train=48.0%  val=49.1%  (p=0)\n",
            "  [ClassicFF-Embed] ep  50  loss=1.531  train=48.9%  val=49.7%  (p=1)\n",
            "  [ClassicFF-Embed] ep  60  loss=1.443  train=49.8%  val=50.0%  (p=1)\n",
            "  [ClassicFF-Embed] ep  70  loss=1.446  train=49.9%  val=50.5%  (p=0)\n",
            "  [ClassicFF-Embed] ep  80  loss=1.377  train=50.7%  val=51.2%  (p=5)\n",
            "  [ClassicFF-Embed] ep  90  loss=1.374  train=51.2%  val=51.2%  (p=15)\n",
            "  [ClassicFF-Embed] ep 100  loss=1.331  train=51.5%  val=51.5%  (p=2)\n",
            "  [ClassicFF-Embed] ep 110  loss=1.304  train=52.1%  val=52.1%  (p=5)\n",
            "  [ClassicFF-Embed] ep 120  loss=1.291  train=52.6%  val=52.5%  (p=2)\n",
            "  [ClassicFF-Embed] ep 130  loss=1.318  train=52.8%  val=52.8%  (p=12)\n",
            "  [ClassicFF-Embed] ep 140  loss=1.263  train=52.9%  val=52.7%  (p=7)\n",
            "  [ClassicFF-Embed] ep 150  loss=1.259  train=53.3%  val=53.4%  (p=3)\n",
            "  [ClassicFF-Embed] ep 160  loss=1.229  train=53.6%  val=53.3%  (p=6)\n",
            "  [ClassicFF-Embed] ep 170  loss=1.210  train=53.9%  val=54.0%  (p=0)\n",
            "  [ClassicFF-Embed] ep 180  loss=1.221  train=53.7%  val=53.4%  (p=4)\n",
            "  [ClassicFF-Embed] ep 190  loss=1.220  train=53.5%  val=53.2%  (p=14)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 196 (#1)\n",
            "  [ClassicFF-Embed] ep 200  loss=1.172  train=54.0%  val=53.6%  (p=24)\n",
            "  [ClassicFF-Embed] ep 210  loss=1.188  train=54.3%  val=54.3%  (p=0)\n",
            "  [ClassicFF-Embed] ep 220  loss=1.169  train=54.4%  val=53.9%  (p=10)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 230 (#2)\n",
            "  [ClassicFF-Embed] ep 230  loss=1.160  train=54.1%  val=53.4%  (p=20)\n",
            "  [ClassicFF-Embed] ep 240  loss=1.171  train=54.4%  val=53.8%  (p=30)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 250 (#3)\n",
            "  [ClassicFF-Embed] ep 250  loss=1.152  train=54.4%  val=53.8%  (p=40)\n",
            "  [ClassicFF-Embed] ep 260  loss=1.182  train=54.5%  val=53.9%  (p=50)\n",
            "  [ClassicFF-Embed] ep 270  loss=1.148  train=54.6%  val=53.9%  (p=60)\n",
            "  [ClassicFF-Embed] Early stop at epoch 270\n",
            "  [ClassicFF-Embed] DONE  test=53.29%  66s  44022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.858  train=62.6%  val=62.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=1.983  train=87.2%  val=86.5%  (p=2)\n",
            "  [ClassicFF-Additive] ep  20  loss=1.734  train=93.4%  val=93.2%  (p=0)\n",
            "  [ClassicFF-Additive] ep  30  loss=1.510  train=94.2%  val=93.6%  (p=5)\n",
            "  [ClassicFF-Additive] ep  40  loss=1.378  train=94.3%  val=93.8%  (p=1)\n",
            "  [ClassicFF-Additive] ep  50  loss=1.282  train=95.3%  val=94.9%  (p=1)\n",
            "  [ClassicFF-Additive] ep  60  loss=1.235  train=96.2%  val=96.1%  (p=0)\n",
            "  [ClassicFF-Additive] ep  70  loss=1.186  train=96.3%  val=96.0%  (p=3)\n",
            "  [ClassicFF-Additive] ep  80  loss=1.176  train=96.2%  val=96.0%  (p=13)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 87 (#1)\n",
            "  [ClassicFF-Additive] ep  90  loss=1.145  train=96.2%  val=95.4%  (p=23)\n",
            "  [ClassicFF-Additive] ep 100  loss=1.137  train=96.6%  val=96.3%  (p=33)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 107 (#2)\n",
            "  [ClassicFF-Additive] ep 110  loss=1.130  train=96.3%  val=95.9%  (p=43)\n",
            "  [ClassicFF-Additive] ep 120  loss=1.134  train=96.5%  val=96.1%  (p=53)\n",
            "  [ClassicFF-Additive] ep 130  loss=1.122  train=96.5%  val=95.8%  (p=5)\n",
            "  [ClassicFF-Additive] ep 140  loss=1.116  train=96.5%  val=96.1%  (p=15)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 145 (#3)\n",
            "  [ClassicFF-Additive] ep 150  loss=1.114  train=96.5%  val=96.1%  (p=25)\n",
            "  [ClassicFF-Additive] ep 160  loss=1.121  train=96.7%  val=96.2%  (p=35)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 165 (#4)\n",
            "  [ClassicFF-Additive] ep 170  loss=1.110  train=96.5%  val=96.2%  (p=45)\n",
            "  [ClassicFF-Additive] ep 180  loss=1.108  train=96.5%  val=96.0%  (p=55)\n",
            "  [ClassicFF-Additive] Early stop at epoch 185\n",
            "  [ClassicFF-Additive] DONE  test=92.45%  43s  45602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=3.005  train=68.7%  val=68.6%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.112  train=88.3%  val=88.0%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=1.875  train=89.8%  val=90.2%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=1.817  train=88.7%  val=88.4%  (p=6)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=1.740  train=89.9%  val=89.0%  (p=16)\n",
            "  [ClassicFF+LA] LR reduced at epoch 44 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=1.679  train=90.1%  val=89.4%  (p=26)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=1.683  train=89.9%  val=89.0%  (p=36)\n",
            "  [ClassicFF+LA] LR reduced at epoch 64 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=1.643  train=89.8%  val=89.0%  (p=46)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=1.633  train=89.8%  val=88.9%  (p=56)\n",
            "  [ClassicFF+LA] Early stop at epoch 84\n",
            "  [ClassicFF+LA a=0.5] DONE  test=84.33%  23s  45602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.439  train=97.2%  val=96.7%  (p=0)\n",
            "  [BP]       ep  10  loss=0.010  train=99.9%  val=99.4%  (p=4)\n",
            "  [BP]       ep  20  loss=0.022  train=99.3%  val=98.6%  (p=14)\n",
            "  [BP]       ep  30  loss=0.004  train=99.9%  val=99.5%  (p=5)\n",
            "  [BP]       ep  40  loss=0.003  train=99.8%  val=99.4%  (p=15)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 45 (reduction #1)\n",
            "  [BP]       ep  50  loss=0.000  train=100.0%  val=99.5%  (p=25)\n",
            "  [BP]       ep  60  loss=0.000  train=100.0%  val=99.5%  (p=35)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 65 (reduction #2)\n",
            "  [BP]       ep  70  loss=0.000  train=100.0%  val=99.5%  (p=45)\n",
            "  [BP]       ep  80  loss=0.000  train=100.0%  val=99.5%  (p=55)\n",
            "  [BP]       Early stop at epoch 85 (best_val=99.5%, 2 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=99.5%)\n",
            "  [BP]       DONE  test=96.66%  7s  45610 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.507  train=51.3%  val=50.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.784  train=87.2%  val=87.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.674  train=91.2%  val=91.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.623  train=92.9%  val=92.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.585  train=93.8%  val=93.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.549  train=94.1%  val=93.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.519  train=94.6%  val=94.2%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.508  train=95.3%  val=94.6%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.484  train=95.8%  val=95.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.488  train=95.8%  val=95.3%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.481  train=96.2%  val=95.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.470  train=96.3%  val=96.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.462  train=96.5%  val=96.1%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.459  train=96.4%  val=96.1%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.459  train=96.7%  val=96.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.447  train=96.5%  val=96.5%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.452  train=96.8%  val=96.9%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.443  train=96.7%  val=96.6%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 175 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.439  train=96.9%  val=97.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.447  train=97.1%  val=97.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.449  train=96.9%  val=97.1%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.432  train=97.0%  val=97.3%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 213 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.440  train=97.0%  val=97.1%  (p=27)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.434  train=97.2%  val=97.2%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 233 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.434  train=97.0%  val=97.2%  (p=47)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.432  train=97.1%  val=97.3%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 253 (best_val=97.5%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=97.5%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=93.1%, calibrated=93.3%, linear=94.2%, mlp=94.8%, temperature=93.4%\n",
            "    test=93.11%  50s  34000 params\n",
            "    Goodness (spec0): L0:g+=7.495/g-=0.068/sep=7.427/th=1.000 | L1:g+=4.992/g-=0.208/sep=4.784/th=1.000\n",
            "    Avg specialist: acc=96.1%, separation=10.0\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.643  train=48.6%  val=48.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.979  train=86.0%  val=86.5%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.845  train=90.2%  val=90.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.792  train=92.3%  val=91.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.768  train=92.9%  val=92.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.742  train=93.1%  val=92.7%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.714  train=93.5%  val=93.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.708  train=93.8%  val=93.7%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.687  train=94.4%  val=94.2%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.691  train=94.3%  val=94.6%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.683  train=95.0%  val=94.9%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.675  train=95.0%  val=95.1%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.663  train=95.1%  val=95.2%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.661  train=95.3%  val=95.2%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.661  train=95.5%  val=95.5%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.649  train=95.9%  val=95.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.652  train=95.9%  val=95.6%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 170 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.648  train=95.7%  val=95.4%  (p=20)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.644  train=96.0%  val=95.7%  (p=30)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.646  train=96.1%  val=95.8%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.648  train=95.9%  val=95.5%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 208 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.633  train=96.0%  val=95.7%  (p=22)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 220  loss=0.640  train=96.0%  val=95.5%  (p=32)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 228 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 230  loss=0.637  train=96.1%  val=95.7%  (p=42)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 240  loss=0.637  train=96.0%  val=95.5%  (p=52)\n",
            "  [ModularFF] Early stop at epoch 248 (best_val=96.0%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=96.0%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=91.1%, calibrated=89.1%, linear=91.3%, mlp=92.7%, temperature=91.5%\n",
            "    test=91.08%  59s  34000 params\n",
            "    Goodness (spec0): L0:g+=9.289/g-=0.060/sep=9.229/th=1.000 | L1:g+=3.289/g-=0.152/sep=3.138/th=1.000\n",
            "    Avg specialist: acc=94.5%, separation=9.0\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.774  train=28.5%  val=28.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.125  train=85.8%  val=86.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.962  train=89.9%  val=89.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.906  train=91.0%  val=90.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.875  train=92.4%  val=91.3%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.848  train=92.5%  val=91.5%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.824  train=92.9%  val=92.1%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.823  train=93.5%  val=92.4%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.808  train=93.7%  val=93.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.811  train=93.5%  val=92.9%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.805  train=94.5%  val=94.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.799  train=94.4%  val=94.3%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.791  train=94.4%  val=94.0%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.788  train=94.7%  val=94.6%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.787  train=94.8%  val=94.9%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 145 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.780  train=95.1%  val=94.8%  (p=25)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.782  train=95.5%  val=95.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.780  train=95.2%  val=94.9%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 180 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.779  train=95.3%  val=95.2%  (p=20)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.780  train=95.3%  val=95.3%  (p=30)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.781  train=95.1%  val=94.9%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.772  train=95.2%  val=94.9%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 211 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 220  loss=0.777  train=95.2%  val=95.2%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 230  loss=0.776  train=95.2%  val=95.2%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 231 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 240  loss=0.775  train=95.2%  val=95.2%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 250  loss=0.775  train=95.2%  val=95.1%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 251 (best_val=95.4%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=95.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=90.3%, calibrated=88.5%, linear=90.8%, mlp=92.0%, temperature=90.7%\n",
            "    test=90.28%  60s  34000 params\n",
            "    Goodness (spec0): L0:g+=7.986/g-=0.083/sep=7.903/th=1.000 | L1:g+=2.016/g-=0.129/sep=1.888/th=1.000\n",
            "    Avg specialist: acc=93.8%, separation=7.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.494  train=57.2%  val=58.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.750  train=87.6%  val=88.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.662  train=91.4%  val=91.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.599  train=93.3%  val=92.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.553  train=94.2%  val=93.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.519  train=94.6%  val=94.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.491  train=95.1%  val=94.9%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.485  train=95.8%  val=95.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.464  train=96.2%  val=95.5%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.470  train=96.1%  val=96.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.461  train=96.5%  val=96.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.452  train=96.6%  val=96.7%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.445  train=96.7%  val=96.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.441  train=96.7%  val=97.0%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.440  train=97.1%  val=97.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.432  train=97.0%  val=97.1%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.435  train=97.3%  val=97.9%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.429  train=97.1%  val=97.5%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.426  train=97.3%  val=97.9%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 181 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.432  train=97.5%  val=98.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.433  train=97.2%  val=98.0%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 209 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.420  train=97.3%  val=98.0%  (p=21)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.425  train=97.4%  val=98.0%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 229 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.421  train=97.5%  val=98.3%  (p=41)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.423  train=97.4%  val=98.0%  (p=51)\n",
            "  [ModularFF] Early stop at epoch 249 (best_val=98.3%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=98.3%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=93.7%, calibrated=93.6%, linear=93.6%, mlp=94.3%, temperature=93.8%\n",
            "    test=93.74%  50s  118000 params\n",
            "    Goodness (spec0): L0:g+=7.399/g-=0.029/sep=7.369/th=1.000 | L1:g+=5.071/g-=0.090/sep=4.981/th=1.000\n",
            "    Avg specialist: acc=96.0%, separation=10.3\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.630  train=57.9%  val=58.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.991  train=86.0%  val=87.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.850  train=90.8%  val=90.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.800  train=92.7%  val=91.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.774  train=93.5%  val=93.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.752  train=93.6%  val=93.0%  (p=10)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.722  train=93.9%  val=93.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.716  train=94.3%  val=93.9%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.699  train=94.8%  val=94.4%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.702  train=94.6%  val=94.6%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.697  train=95.2%  val=95.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.687  train=95.3%  val=95.1%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.677  train=95.3%  val=95.4%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.675  train=95.5%  val=95.6%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.672  train=95.7%  val=95.6%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.661  train=96.1%  val=95.7%  (p=14)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.664  train=96.1%  val=95.8%  (p=9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.656  train=95.8%  val=95.5%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 171 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.657  train=96.3%  val=95.8%  (p=29)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.657  train=96.4%  val=95.9%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 191 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.661  train=96.1%  val=95.7%  (p=49)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.647  train=96.2%  val=95.8%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 211 (best_val=96.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=96.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=91.3%, calibrated=89.4%, linear=91.0%, mlp=92.7%, temperature=91.3%\n",
            "    test=91.28%  51s  118000 params\n",
            "    Goodness (spec0): L0:g+=9.782/g-=0.035/sep=9.747/th=1.000 | L1:g+=3.296/g-=0.107/sep=3.189/th=1.000\n",
            "    Avg specialist: acc=94.7%, separation=8.8\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.760  train=44.2%  val=44.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.144  train=86.2%  val=87.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.981  train=89.8%  val=89.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.925  train=91.8%  val=91.6%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.897  train=92.8%  val=92.1%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.878  train=92.9%  val=91.8%  (p=11)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.852  train=93.1%  val=92.6%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.852  train=94.0%  val=93.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.837  train=94.1%  val=93.5%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.840  train=93.9%  val=93.4%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.833  train=95.2%  val=94.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.829  train=94.8%  val=94.6%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.821  train=94.8%  val=94.6%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.818  train=95.1%  val=94.9%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.818  train=95.2%  val=94.9%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.811  train=95.7%  val=95.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.811  train=95.9%  val=95.7%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.809  train=95.2%  val=95.2%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 172 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.808  train=95.6%  val=95.0%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.809  train=96.0%  val=95.5%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 192 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.809  train=95.7%  val=95.2%  (p=48)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.800  train=95.7%  val=95.4%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 212 (best_val=95.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=95.7%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=90.5%, calibrated=88.5%, linear=90.2%, mlp=92.0%, temperature=90.5%\n",
            "    test=90.51%  50s  118000 params\n",
            "    Goodness (spec0): L0:g+=8.915/g-=0.082/sep=8.833/th=1.000 | L1:g+=1.996/g-=0.166/sep=1.830/th=1.000\n",
            "    Avg specialist: acc=93.8%, separation=7.1\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/Pendigits/results_2layer_seed42.json\n",
            "\n",
            "  --- Pendigits 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 95.7% (45,602 params)\n",
            "  ClassicFF_SGD: 65.4% (45,602 params)\n",
            "  ClassicFF_Embed: 53.3% (44,022 params)\n",
            "  ClassicFF_Additive: 92.5% (45,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 84.3% (45,602 params)\n",
            "  BP: 96.7% (45,610 params)\n",
            "  ModularFF_50_50_a0.0: 93.1% (34,000 params)\n",
            "  ModularFF_50_50_a0.5: 91.1% (34,000 params)\n",
            "  ModularFF_50_50_a1.0: 90.3% (34,000 params)\n",
            "  ModularFF_100_100_a0.0: 93.7% (118,000 params)\n",
            "  ModularFF_100_100_a0.5: 91.3% (118,000 params)\n",
            "  ModularFF_100_100_a1.0: 90.5% (118,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: Pendigits | ACTIVATION: tanh\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  Pendigits | seed=42 | K=10 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [200, 200]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.711  train=70.3%  val=69.0%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.937  train=88.5%  val=89.4%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.876  train=89.3%  val=90.6%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.853  train=90.0%  val=91.4%  (p=4)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.829  train=90.7%  val=92.1%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.810  train=91.0%  val=92.1%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.807  train=91.4%  val=92.3%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.793  train=91.4%  val=92.3%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.788  train=91.6%  val=92.4%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.785  train=91.8%  val=92.7%  (p=0)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.781  train=91.7%  val=92.2%  (p=10)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 110 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.782  train=91.6%  val=92.1%  (p=20)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.778  train=91.8%  val=92.5%  (p=30)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 130 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.778  train=91.9%  val=92.6%  (p=40)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.778  train=91.9%  val=92.5%  (p=50)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.779  train=91.9%  val=92.5%  (p=60)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 150 (best_val=92.7%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=92.7%)\n",
            "  [ClassicFF_ADAM] DONE  test=88.19%  34s  45602 params  (LR=0.01, adam, act=tanh)\n",
            "    Goodness: L0:g+=0.343/g-=-0.167/sep=0.510/th=0.073 | L1:g+=0.879/g-=-0.956/sep=1.836/th=-0.001\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.772  train=14.8%  val=14.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.772  train=15.1%  val=15.1%  (p=3)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.772  train=15.4%  val=15.3%  (p=3)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.771  train=15.9%  val=15.5%  (p=7)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.771  train=16.3%  val=15.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.771  train=16.7%  val=16.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.771  train=17.0%  val=16.5%  (p=2)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.771  train=17.4%  val=17.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.771  train=17.8%  val=17.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.771  train=18.2%  val=18.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.770  train=18.6%  val=18.6%  (p=3)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.770  train=19.1%  val=19.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 120  loss=2.770  train=19.3%  val=19.7%  (p=1)\n",
            "  [ClassicFF_SGD] ep 130  loss=2.770  train=19.7%  val=20.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 140  loss=2.770  train=20.0%  val=20.5%  (p=1)\n",
            "  [ClassicFF_SGD] ep 150  loss=2.770  train=20.5%  val=20.7%  (p=3)\n",
            "  [ClassicFF_SGD] ep 160  loss=2.770  train=20.9%  val=20.9%  (p=5)\n",
            "  [ClassicFF_SGD] ep 170  loss=2.770  train=21.3%  val=21.2%  (p=4)\n",
            "  [ClassicFF_SGD] ep 180  loss=2.769  train=21.7%  val=22.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 190  loss=2.769  train=22.2%  val=22.6%  (p=4)\n",
            "  [ClassicFF_SGD] ep 200  loss=2.769  train=22.5%  val=23.1%  (p=1)\n",
            "  [ClassicFF_SGD] ep 210  loss=2.769  train=23.1%  val=23.8%  (p=0)\n",
            "  [ClassicFF_SGD] ep 220  loss=2.769  train=23.6%  val=24.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 230  loss=2.769  train=24.3%  val=24.8%  (p=2)\n",
            "  [ClassicFF_SGD] ep 240  loss=2.768  train=24.9%  val=25.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 250  loss=2.768  train=25.7%  val=26.2%  (p=1)\n",
            "  [ClassicFF_SGD] ep 260  loss=2.768  train=26.3%  val=27.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 270  loss=2.768  train=26.9%  val=27.8%  (p=2)\n",
            "  [ClassicFF_SGD] ep 280  loss=2.768  train=27.6%  val=28.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 290  loss=2.768  train=28.2%  val=29.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 300  loss=2.768  train=29.1%  val=29.9%  (p=5)\n",
            "  [ClassicFF_SGD] ep 310  loss=2.768  train=29.9%  val=30.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 320  loss=2.767  train=30.6%  val=31.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 330  loss=2.767  train=31.3%  val=32.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 340  loss=2.767  train=32.1%  val=33.3%  (p=4)\n",
            "  [ClassicFF_SGD] ep 350  loss=2.767  train=32.9%  val=33.8%  (p=1)\n",
            "  [ClassicFF_SGD] ep 360  loss=2.767  train=33.6%  val=34.3%  (p=1)\n",
            "  [ClassicFF_SGD] ep 370  loss=2.767  train=34.2%  val=34.5%  (p=4)\n",
            "  [ClassicFF_SGD] ep 380  loss=2.767  train=34.9%  val=35.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep 390  loss=2.767  train=35.7%  val=36.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 400  loss=2.766  train=36.4%  val=37.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 410  loss=2.766  train=37.2%  val=38.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 420  loss=2.766  train=37.9%  val=38.8%  (p=1)\n",
            "  [ClassicFF_SGD] ep 430  loss=2.766  train=38.7%  val=39.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 440  loss=2.766  train=39.4%  val=40.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 450  loss=2.766  train=40.2%  val=41.1%  (p=3)\n",
            "  [ClassicFF_SGD] ep 460  loss=2.765  train=40.9%  val=42.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 470  loss=2.765  train=41.7%  val=42.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep 480  loss=2.765  train=42.6%  val=43.8%  (p=2)\n",
            "  [ClassicFF_SGD] ep 490  loss=2.765  train=43.2%  val=44.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 500  loss=2.765  train=44.2%  val=45.0%  (p=1)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=45.0%)\n",
            "  [ClassicFF_SGD] DONE  test=41.71%  108s  45602 params  (LR=0.03, sgd, act=tanh)\n",
            "    Goodness: L0:g+=0.011/g-=-0.008/sep=0.019/th=0.004 | L1:g+=0.001/g-=-0.001/sep=0.001/th=0.000\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.714  train=42.5%  val=43.3%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.099  train=74.4%  val=75.0%  (p=0)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.020  train=78.9%  val=78.9%  (p=0)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.008  train=80.6%  val=80.6%  (p=0)\n",
            "  [ClassicFF-Embed] ep  40  loss=1.985  train=82.1%  val=82.4%  (p=0)\n",
            "  [ClassicFF-Embed] ep  50  loss=1.978  train=83.6%  val=84.1%  (p=3)\n",
            "  [ClassicFF-Embed] ep  60  loss=1.963  train=85.1%  val=84.9%  (p=7)\n",
            "  [ClassicFF-Embed] ep  70  loss=1.954  train=85.5%  val=85.4%  (p=3)\n",
            "  [ClassicFF-Embed] ep  80  loss=1.955  train=85.9%  val=85.6%  (p=5)\n",
            "  [ClassicFF-Embed] ep  90  loss=1.948  train=86.3%  val=86.4%  (p=5)\n",
            "  [ClassicFF-Embed] ep 100  loss=1.946  train=86.9%  val=86.5%  (p=4)\n",
            "  [ClassicFF-Embed] ep 110  loss=1.935  train=87.0%  val=87.1%  (p=0)\n",
            "  [ClassicFF-Embed] ep 120  loss=1.934  train=86.8%  val=86.9%  (p=6)\n",
            "  [ClassicFF-Embed] ep 130  loss=1.945  train=86.7%  val=86.7%  (p=16)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 134 (#1)\n",
            "  [ClassicFF-Embed] ep 140  loss=1.928  train=87.0%  val=86.6%  (p=26)\n",
            "  [ClassicFF-Embed] ep 150  loss=1.939  train=86.9%  val=86.3%  (p=36)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 154 (#2)\n",
            "  [ClassicFF-Embed] ep 160  loss=1.934  train=87.2%  val=86.3%  (p=46)\n",
            "  [ClassicFF-Embed] ep 170  loss=1.934  train=87.1%  val=86.8%  (p=56)\n",
            "  [ClassicFF-Embed] Early stop at epoch 174\n",
            "  [ClassicFF-Embed] DONE  test=80.30%  40s  44022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.686  train=72.2%  val=72.7%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.343  train=89.7%  val=90.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.316  train=91.3%  val=91.3%  (p=0)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.255  train=91.7%  val=91.7%  (p=4)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.235  train=92.1%  val=91.7%  (p=1)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.222  train=92.9%  val=92.2%  (p=5)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.217  train=93.7%  val=93.0%  (p=0)\n",
            "  [ClassicFF-Additive] ep  70  loss=2.207  train=93.7%  val=92.6%  (p=10)\n",
            "  [ClassicFF-Additive] ep  80  loss=2.210  train=94.4%  val=93.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  90  loss=2.205  train=94.0%  val=93.5%  (p=6)\n",
            "  [ClassicFF-Additive] ep 100  loss=2.196  train=94.8%  val=94.0%  (p=3)\n",
            "  [ClassicFF-Additive] ep 110  loss=2.194  train=95.1%  val=94.3%  (p=2)\n",
            "  [ClassicFF-Additive] ep 120  loss=2.196  train=95.3%  val=94.2%  (p=5)\n",
            "  [ClassicFF-Additive] ep 130  loss=2.183  train=95.4%  val=94.6%  (p=6)\n",
            "  [ClassicFF-Additive] ep 140  loss=2.181  train=95.3%  val=94.4%  (p=6)\n",
            "  [ClassicFF-Additive] ep 150  loss=2.181  train=95.5%  val=94.6%  (p=16)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 154 (#1)\n",
            "  [ClassicFF-Additive] ep 160  loss=2.185  train=95.6%  val=94.6%  (p=26)\n",
            "  [ClassicFF-Additive] ep 170  loss=2.175  train=95.6%  val=94.5%  (p=36)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 174 (#2)\n",
            "  [ClassicFF-Additive] ep 180  loss=2.172  train=95.6%  val=94.6%  (p=46)\n",
            "  [ClassicFF-Additive] ep 190  loss=2.173  train=95.6%  val=94.5%  (p=56)\n",
            "  [ClassicFF-Additive] Early stop at epoch 194\n",
            "  [ClassicFF-Additive] DONE  test=89.82%  43s  45602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.775  train=18.7%  val=19.7%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.453  train=86.8%  val=87.7%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.430  train=87.7%  val=88.4%  (p=7)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.418  train=87.9%  val=88.1%  (p=6)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.410  train=88.5%  val=88.8%  (p=1)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.403  train=87.8%  val=88.4%  (p=11)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.404  train=88.6%  val=89.1%  (p=1)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.395  train=88.7%  val=88.9%  (p=11)\n",
            "  [ClassicFF+LA] LR reduced at epoch 79 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.394  train=88.0%  val=88.6%  (p=21)\n",
            "  [ClassicFF+LA a=0.5] ep  90  loss=2.397  train=88.3%  val=88.7%  (p=31)\n",
            "  [ClassicFF+LA] LR reduced at epoch 99 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep 100  loss=2.394  train=88.2%  val=88.5%  (p=41)\n",
            "  [ClassicFF+LA a=0.5] ep 110  loss=2.393  train=88.2%  val=88.7%  (p=51)\n",
            "  [ClassicFF+LA] Early stop at epoch 119\n",
            "  [ClassicFF+LA a=0.5] DONE  test=83.88%  30s  45602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.416  train=97.2%  val=97.1%  (p=0)\n",
            "  [BP]       ep  10  loss=0.007  train=99.9%  val=99.5%  (p=3)\n",
            "  [BP]       ep  20  loss=0.007  train=100.0%  val=99.3%  (p=13)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 27 (reduction #1)\n",
            "  [BP]       ep  30  loss=0.002  train=100.0%  val=99.3%  (p=23)\n",
            "  [BP]       ep  40  loss=0.002  train=100.0%  val=99.5%  (p=33)\n",
            "  [BP]       ep  50  loss=0.002  train=100.0%  val=99.6%  (p=4)\n",
            "  [BP]       ep  60  loss=0.001  train=100.0%  val=99.6%  (p=14)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 66 (reduction #2)\n",
            "  [BP]       ep  70  loss=0.001  train=100.0%  val=99.6%  (p=24)\n",
            "  [BP]       ep  80  loss=0.000  train=100.0%  val=99.5%  (p=34)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 86 (reduction #3)\n",
            "  [BP]       ep  90  loss=0.000  train=100.0%  val=99.5%  (p=44)\n",
            "  [BP]       ep 100  loss=0.000  train=100.0%  val=99.5%  (p=54)\n",
            "  [BP]       Early stop at epoch 106 (best_val=99.6%, 3 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=99.6%)\n",
            "  [BP]       DONE  test=97.03%  9s  45610 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.367  train=61.2%  val=60.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.010  train=84.2%  val=84.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.871  train=88.7%  val=87.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.812  train=90.8%  val=90.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.782  train=91.6%  val=91.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.760  train=91.8%  val=91.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.743  train=92.1%  val=91.9%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.735  train=92.3%  val=92.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.724  train=92.4%  val=92.4%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.722  train=92.5%  val=92.5%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.719  train=92.8%  val=92.6%  (p=18)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.716  train=92.6%  val=92.4%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.713  train=92.7%  val=92.6%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 121 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.708  train=92.8%  val=92.7%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.710  train=92.9%  val=92.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.705  train=92.9%  val=92.7%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.709  train=92.9%  val=92.8%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.706  train=92.8%  val=92.6%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 172 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.704  train=93.0%  val=92.7%  (p=28)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.706  train=93.0%  val=92.8%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 192 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.708  train=93.0%  val=92.7%  (p=48)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.698  train=93.0%  val=92.7%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 212 (best_val=92.9%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=92.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=86.9%, calibrated=86.8%, linear=86.0%, mlp=90.9%, temperature=85.2%\n",
            "    test=86.91%  39s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.949/g-=-0.828/sep=1.777/th=0.000 | L1:g+=0.981/g-=-0.926/sep=1.907/th=0.000\n",
            "    Avg specialist: acc=94.4%, separation=3.4\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.399  train=57.9%  val=56.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.198  train=80.9%  val=80.7%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.144  train=86.5%  val=86.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.124  train=87.2%  val=87.1%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.113  train=87.3%  val=87.2%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.103  train=87.2%  val=87.5%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.093  train=87.7%  val=88.0%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.091  train=87.6%  val=87.9%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.087  train=87.5%  val=88.1%  (p=15)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=1.088  train=87.7%  val=88.3%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=1.087  train=87.8%  val=88.3%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=1.087  train=87.9%  val=88.8%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 116 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=1.087  train=87.7%  val=88.4%  (p=24)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=1.083  train=87.9%  val=88.7%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 136 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=1.086  train=87.9%  val=88.6%  (p=44)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=1.082  train=87.9%  val=88.5%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 156 (best_val=88.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=88.8%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=82.4%, calibrated=82.4%, linear=78.6%, mlp=91.0%, temperature=84.8%\n",
            "    test=82.39%  35s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.888/g-=-0.327/sep=1.214/th=0.000 | L1:g+=0.946/g-=-0.171/sep=1.117/th=0.000\n",
            "    Avg specialist: acc=90.1%, separation=2.2\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.423  train=8.7%  val=8.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.273  train=15.8%  val=15.1%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.249  train=15.9%  val=15.3%  (p=12)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.245  train=16.0%  val=15.3%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.243  train=17.0%  val=15.7%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.242  train=16.7%  val=15.7%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.241  train=17.5%  val=16.1%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.242  train=17.8%  val=16.6%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 75 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=1.241  train=16.6%  val=15.7%  (p=25)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=1.242  train=16.9%  val=15.8%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 95 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=1.242  train=16.9%  val=15.8%  (p=45)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=1.241  train=17.1%  val=15.9%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=1.243  train=17.2%  val=15.9%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 124 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=1.242  train=17.5%  val=16.3%  (p=26)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=1.242  train=17.3%  val=16.1%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 144 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=1.242  train=17.3%  val=16.0%  (p=46)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=1.243  train=17.4%  val=16.0%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 164 (best_val=16.6%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=16.6%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=16.3%, calibrated=0.0%, linear=49.9%, mlp=69.2%, temperature=23.6%\n",
            "    test=16.32%  36s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.110/g-=0.016/sep=0.094/th=0.000 | L1:g+=-0.056/g-=-0.035/sep=-0.021/th=0.000\n",
            "    Avg specialist: acc=40.8%, separation=-0.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.361  train=72.8%  val=73.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.989  train=84.2%  val=84.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.864  train=88.6%  val=88.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.808  train=90.9%  val=90.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.780  train=91.5%  val=91.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.758  train=91.7%  val=91.5%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.741  train=92.1%  val=92.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.734  train=92.0%  val=92.4%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.723  train=92.2%  val=92.4%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.721  train=92.4%  val=92.5%  (p=15)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.717  train=92.5%  val=92.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.715  train=92.6%  val=92.5%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.712  train=92.7%  val=92.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.706  train=92.7%  val=92.9%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 138 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.707  train=92.8%  val=92.9%  (p=22)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.703  train=92.8%  val=92.9%  (p=32)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 158 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.707  train=92.9%  val=93.0%  (p=42)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.704  train=92.9%  val=92.7%  (p=52)\n",
            "  [ModularFF] Early stop at epoch 178 (best_val=93.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=93.1%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=86.9%, calibrated=86.7%, linear=86.1%, mlp=90.9%, temperature=85.2%\n",
            "    test=86.91%  33s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.917/g-=-0.893/sep=1.810/th=0.000 | L1:g+=0.960/g-=-0.984/sep=1.944/th=0.000\n",
            "    Avg specialist: acc=94.9%, separation=3.4\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.394  train=70.5%  val=71.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.198  train=79.2%  val=80.1%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.145  train=84.6%  val=85.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.125  train=86.1%  val=86.2%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.112  train=87.2%  val=87.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.101  train=86.9%  val=87.4%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.092  train=87.4%  val=88.0%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.089  train=87.5%  val=88.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.086  train=87.2%  val=88.1%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=1.088  train=87.4%  val=88.4%  (p=18)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=1.087  train=87.3%  val=88.1%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=1.087  train=87.4%  val=88.0%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 112 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=1.086  train=87.4%  val=88.3%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=1.083  train=87.6%  val=88.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 132 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=1.086  train=87.3%  val=88.3%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=1.082  train=87.5%  val=88.3%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 152 (best_val=88.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=88.7%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=82.0%, calibrated=82.3%, linear=79.2%, mlp=91.2%, temperature=84.6%\n",
            "    test=82.02%  34s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.841/g-=-0.417/sep=1.258/th=0.000 | L1:g+=0.953/g-=-0.253/sep=1.207/th=0.000\n",
            "    Avg specialist: acc=90.2%, separation=2.2\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.419  train=2.9%  val=3.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.268  train=14.9%  val=15.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.246  train=14.6%  val=14.8%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.244  train=10.8%  val=11.6%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 38 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.243  train=8.2%  val=8.4%  (p=22)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.240  train=8.0%  val=8.2%  (p=32)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 58 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.240  train=6.9%  val=7.4%  (p=42)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.242  train=7.1%  val=7.4%  (p=52)\n",
            "  [ModularFF] Early stop at epoch 78 (best_val=15.6%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=15.6%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=16.2%, calibrated=0.6%, linear=58.8%, mlp=76.0%, temperature=12.7%\n",
            "    test=16.18%  17s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.035/g-=0.006/sep=0.029/th=0.000 | L1:g+=0.059/g-=0.033/sep=0.026/th=0.000\n",
            "    Avg specialist: acc=50.4%, separation=-0.0\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/Pendigits/results_2layer_seed42.json\n",
            "\n",
            "  --- Pendigits 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 88.2% (45,602 params)\n",
            "  ClassicFF_SGD: 41.7% (45,602 params)\n",
            "  ClassicFF_Embed: 80.3% (44,022 params)\n",
            "  ClassicFF_Additive: 89.8% (45,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 83.9% (45,602 params)\n",
            "  BP: 97.0% (45,610 params)\n",
            "  ModularFF_50_50_a0.0: 86.9% (34,000 params)\n",
            "  ModularFF_50_50_a0.5: 82.4% (34,000 params)\n",
            "  ModularFF_50_50_a1.0: 16.3% (34,000 params)\n",
            "  ModularFF_100_100_a0.0: 86.9% (118,000 params)\n",
            "  ModularFF_100_100_a0.5: 82.0% (118,000 params)\n",
            "  ModularFF_100_100_a1.0: 16.2% (118,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: Pendigits | ACTIVATION: hardlimit\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  Pendigits | seed=42 | K=10 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [200, 200]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.730  train=33.1%  val=33.7%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=2.629  train=20.2%  val=21.2%  (p=7)\n",
            "  [ClassicFF_ADAM] ep  20  loss=2.666  train=11.6%  val=12.6%  (p=17)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 23 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=2.674  train=15.2%  val=14.7%  (p=27)\n",
            "  [ClassicFF_ADAM] ep  40  loss=2.677  train=13.9%  val=14.5%  (p=37)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 43 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  50  loss=2.674  train=12.9%  val=13.7%  (p=47)\n",
            "  [ClassicFF_ADAM] ep  60  loss=2.683  train=13.4%  val=13.6%  (p=57)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 63 (best_val=35.5%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=35.5%)\n",
            "  [ClassicFF_ADAM] DONE  test=33.73%  16s  45602 params  (LR=0.01, adam, act=hardlimit)\n",
            "    Goodness: L0:g+=0.475/g-=0.527/sep=-0.052/th=0.505 | L1:g+=0.924/g-=0.861/sep=0.063/th=0.838\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.771  train=12.1%  val=11.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.771  train=12.2%  val=11.6%  (p=4)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.771  train=12.2%  val=11.6%  (p=9)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.771  train=12.3%  val=12.0%  (p=2)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.771  train=12.2%  val=12.3%  (p=4)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.771  train=12.3%  val=12.2%  (p=9)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.771  train=12.4%  val=12.2%  (p=19)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 61 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.771  train=12.4%  val=12.5%  (p=1)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.771  train=12.5%  val=12.5%  (p=2)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.771  train=12.5%  val=12.4%  (p=12)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 98 (reduction #2)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.771  train=12.7%  val=12.4%  (p=22)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.771  train=12.8%  val=12.5%  (p=32)\n",
            "  [ClassicFF_SGD] ep 120  loss=2.771  train=12.8%  val=12.7%  (p=2)\n",
            "  [ClassicFF_SGD] ep 130  loss=2.771  train=12.8%  val=12.8%  (p=12)\n",
            "  [ClassicFF_SGD] ep 140  loss=2.771  train=12.8%  val=12.8%  (p=2)\n",
            "  [ClassicFF_SGD] ep 150  loss=2.771  train=12.8%  val=12.8%  (p=12)\n",
            "  [ClassicFF_SGD] ep 160  loss=2.771  train=12.8%  val=12.8%  (p=6)\n",
            "  [ClassicFF_SGD] ep 170  loss=2.771  train=12.8%  val=12.8%  (p=16)\n",
            "  [ClassicFF_SGD] ep 180  loss=2.771  train=12.9%  val=13.0%  (p=7)\n",
            "  [ClassicFF_SGD] ep 190  loss=2.771  train=12.9%  val=12.9%  (p=17)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 3.7e-03 at epoch 193 (reduction #3)\n",
            "  [ClassicFF_SGD] ep 200  loss=2.771  train=12.8%  val=13.1%  (p=2)\n",
            "  [ClassicFF_SGD] ep 210  loss=2.771  train=12.8%  val=12.8%  (p=12)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.9e-03 at epoch 218 (reduction #4)\n",
            "  [ClassicFF_SGD] ep 220  loss=2.771  train=12.8%  val=12.8%  (p=22)\n",
            "  [ClassicFF_SGD] ep 230  loss=2.771  train=12.8%  val=12.8%  (p=32)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 9.4e-04 at epoch 238 (reduction #5)\n",
            "  [ClassicFF_SGD] ep 240  loss=2.771  train=12.8%  val=13.0%  (p=42)\n",
            "  [ClassicFF_SGD] ep 250  loss=2.771  train=12.8%  val=13.0%  (p=52)\n",
            "  [ClassicFF_SGD] Early stop at epoch 258 (best_val=13.1%, 5 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=13.1%)\n",
            "  [ClassicFF_SGD] DONE  test=11.86%  65s  45602 params  (LR=0.03, sgd, act=hardlimit)\n",
            "    Goodness: L0:g+=0.499/g-=0.501/sep=-0.002/th=0.502 | L1:g+=0.516/g-=0.522/sep=-0.006/th=0.515\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.735  train=32.1%  val=32.0%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.647  train=33.1%  val=32.6%  (p=5)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.644  train=28.5%  val=27.8%  (p=15)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 25 (#1)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.650  train=27.5%  val=26.4%  (p=25)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.657  train=26.9%  val=25.7%  (p=35)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 45 (#2)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.648  train=28.0%  val=26.2%  (p=45)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.647  train=26.5%  val=25.5%  (p=55)\n",
            "  [ClassicFF-Embed] Early stop at epoch 65\n",
            "  [ClassicFF-Embed] DONE  test=32.28%  17s  44022 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.723  train=45.5%  val=46.0%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.495  train=69.7%  val=69.2%  (p=3)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.480  train=62.6%  val=63.5%  (p=13)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 27 (#1)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.487  train=68.5%  val=68.5%  (p=23)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.500  train=64.5%  val=66.3%  (p=33)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 47 (#2)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.501  train=65.3%  val=65.8%  (p=43)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.510  train=63.4%  val=64.0%  (p=53)\n",
            "  [ClassicFF-Additive] Early stop at epoch 67\n",
            "  [ClassicFF-Additive] DONE  test=69.81%  18s  45602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.815  train=27.8%  val=26.8%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.708  train=39.6%  val=39.5%  (p=2)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.695  train=35.5%  val=34.3%  (p=12)\n",
            "  [ClassicFF+LA] LR reduced at epoch 28 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.687  train=40.0%  val=38.3%  (p=22)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.692  train=42.5%  val=41.1%  (p=32)\n",
            "  [ClassicFF+LA] LR reduced at epoch 48 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.692  train=43.5%  val=43.2%  (p=42)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.687  train=43.1%  val=42.8%  (p=52)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.694  train=43.6%  val=43.8%  (p=3)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.687  train=44.9%  val=44.3%  (p=13)\n",
            "  [ClassicFF+LA] LR reduced at epoch 87 (#3)\n",
            "  [ClassicFF+LA a=0.5] ep  90  loss=2.698  train=44.7%  val=44.3%  (p=23)\n",
            "  [ClassicFF+LA a=0.5] ep 100  loss=2.694  train=44.6%  val=44.7%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep 110  loss=2.691  train=45.2%  val=44.4%  (p=3)\n",
            "  [ClassicFF+LA a=0.5] ep 120  loss=2.695  train=44.7%  val=44.3%  (p=13)\n",
            "  [ClassicFF+LA] LR reduced at epoch 127 (#4)\n",
            "  [ClassicFF+LA a=0.5] ep 130  loss=2.687  train=45.6%  val=44.5%  (p=23)\n",
            "  [ClassicFF+LA a=0.5] ep 140  loss=2.697  train=46.1%  val=45.2%  (p=33)\n",
            "  [ClassicFF+LA] LR reduced at epoch 147 (#5)\n",
            "  [ClassicFF+LA a=0.5] ep 150  loss=2.694  train=45.6%  val=44.8%  (p=43)\n",
            "  [ClassicFF+LA a=0.5] ep 160  loss=2.697  train=45.8%  val=44.9%  (p=53)\n",
            "  [ClassicFF+LA] Early stop at epoch 167\n",
            "  [ClassicFF+LA a=0.5] DONE  test=42.97%  49s  45602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.804  train=91.3%  val=91.3%  (p=0)\n",
            "  [BP]       ep  10  loss=0.080  train=97.4%  val=95.8%  (p=3)\n",
            "  [BP]       ep  20  loss=0.060  train=98.6%  val=97.5%  (p=2)\n",
            "  [BP]       ep  30  loss=0.039  train=99.2%  val=98.1%  (p=12)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 38 (reduction #1)\n",
            "  [BP]       ep  40  loss=0.028  train=99.5%  val=98.6%  (p=0)\n",
            "  [BP]       ep  50  loss=0.020  train=99.6%  val=98.4%  (p=7)\n",
            "  [BP]       ep  60  loss=0.021  train=99.3%  val=98.3%  (p=17)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 63 (reduction #2)\n",
            "  [BP]       ep  70  loss=0.018  train=99.7%  val=98.3%  (p=27)\n",
            "  [BP]       ep  80  loss=0.017  train=99.7%  val=98.7%  (p=8)\n",
            "  [BP]       ep  90  loss=0.018  train=99.7%  val=98.3%  (p=7)\n",
            "  [BP]       ep 100  loss=0.021  train=99.7%  val=98.4%  (p=17)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 103 (reduction #3)\n",
            "  [BP]       ep 110  loss=0.013  train=99.7%  val=98.3%  (p=27)\n",
            "  [BP]       ep 120  loss=0.012  train=99.8%  val=98.0%  (p=37)\n",
            "  [BP]       LR reduced (×0.5) -> 6.3e-04 at epoch 123 (reduction #4)\n",
            "  [BP]       ep 130  loss=0.009  train=99.9%  val=98.1%  (p=47)\n",
            "  [BP]       ep 140  loss=0.010  train=99.8%  val=98.3%  (p=57)\n",
            "  [BP]       Early stop at epoch 143 (best_val=98.9%, 4 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=98.9%)\n",
            "  [BP]       DONE  test=94.54%  13s  45610 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.369  train=56.0%  val=57.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.187  train=71.4%  val=72.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.167  train=67.5%  val=67.0%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.168  train=52.6%  val=53.2%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 33 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.173  train=46.6%  val=46.7%  (p=27)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.166  train=42.9%  val=43.8%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 53 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.158  train=41.0%  val=42.3%  (p=47)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=1.165  train=39.6%  val=40.2%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 73 (best_val=75.5%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=75.5%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=72.1%, calibrated=56.7%, linear=65.1%, mlp=84.2%, temperature=72.3%\n",
            "    test=72.13%  16s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.999/g-=0.299/sep=0.700/th=0.500 | L1:g+=1.000/g-=0.385/sep=0.615/th=0.500\n",
            "    Avg specialist: acc=79.2%, separation=1.0\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.402  train=44.3%  val=43.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.259  train=48.8%  val=49.5%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.236  train=35.4%  val=34.6%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 25 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.229  train=31.5%  val=32.5%  (p=25)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.233  train=30.4%  val=31.8%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 45 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.225  train=29.9%  val=31.4%  (p=45)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.218  train=27.2%  val=28.6%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 65 (best_val=63.5%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=63.5%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=58.5%, calibrated=49.7%, linear=65.6%, mlp=83.8%, temperature=66.6%\n",
            "    test=58.52%  17s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.996/g-=0.410/sep=0.586/th=0.500 | L1:g+=1.000/g-=0.559/sep=0.441/th=0.500\n",
            "    Avg specialist: acc=78.4%, separation=0.6\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.435  train=36.6%  val=37.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.310  train=41.9%  val=40.3%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.284  train=30.8%  val=31.8%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 26 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.281  train=26.5%  val=26.0%  (p=24)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.285  train=25.4%  val=25.1%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 46 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.280  train=24.7%  val=24.5%  (p=44)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.276  train=24.4%  val=24.4%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 66 (best_val=44.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=44.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=43.8%, calibrated=39.7%, linear=62.8%, mlp=82.3%, temperature=52.7%\n",
            "    test=43.77%  17s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.986/g-=0.406/sep=0.580/th=0.500 | L1:g+=0.540/g-=0.326/sep=0.214/th=0.500\n",
            "    Avg specialist: acc=77.5%, separation=0.5\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.354  train=73.3%  val=73.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.181  train=72.7%  val=72.6%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.172  train=67.5%  val=66.7%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 22 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.174  train=61.6%  val=61.5%  (p=28)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.178  train=56.4%  val=56.6%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 42 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.173  train=54.0%  val=53.8%  (p=48)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.165  train=51.4%  val=51.5%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 62 (best_val=77.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=77.4%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=71.8%, calibrated=62.9%, linear=72.8%, mlp=83.7%, temperature=75.5%\n",
            "    test=71.81%  14s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.993/g-=0.247/sep=0.746/th=0.500 | L1:g+=1.000/g-=0.356/sep=0.644/th=0.500\n",
            "    Avg specialist: acc=81.9%, separation=0.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.386  train=65.4%  val=67.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.253  train=62.8%  val=63.0%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.230  train=36.7%  val=36.7%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 23 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.230  train=29.4%  val=30.3%  (p=27)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.234  train=31.4%  val=31.6%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 43 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.226  train=30.7%  val=31.2%  (p=47)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.219  train=28.2%  val=29.6%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 63 (best_val=69.6%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=69.6%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=65.8%, calibrated=52.8%, linear=69.8%, mlp=84.4%, temperature=72.2%\n",
            "    test=65.81%  16s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.995/g-=0.359/sep=0.636/th=0.500 | L1:g+=1.000/g-=0.514/sep=0.486/th=0.500\n",
            "    Avg specialist: acc=79.3%, separation=0.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.423  train=54.8%  val=55.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.309  train=44.2%  val=46.0%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.284  train=33.1%  val=33.9%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.281  train=28.0%  val=28.4%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.284  train=23.6%  val=24.0%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.278  train=22.4%  val=22.0%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.273  train=22.1%  val=21.8%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=55.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=55.7%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=50.0%, calibrated=52.7%, linear=61.3%, mlp=78.7%, temperature=64.2%\n",
            "    test=49.97%  16s  118000 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.465/sep=0.535/th=0.500 | L1:g+=0.620/g-=0.395/sep=0.225/th=0.500\n",
            "    Avg specialist: acc=81.9%, separation=0.2\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/Pendigits/results_2layer_seed42.json\n",
            "\n",
            "  --- Pendigits 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 33.7% (45,602 params)\n",
            "  ClassicFF_SGD: 11.9% (45,602 params)\n",
            "  ClassicFF_Embed: 32.3% (44,022 params)\n",
            "  ClassicFF_Additive: 69.8% (45,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 43.0% (45,602 params)\n",
            "  BP: 94.5% (45,610 params)\n",
            "  ModularFF_50_50_a0.0: 72.1% (34,000 params)\n",
            "  ModularFF_50_50_a0.5: 58.5% (34,000 params)\n",
            "  ModularFF_50_50_a1.0: 43.8% (34,000 params)\n",
            "  ModularFF_100_100_a0.0: 71.8% (118,000 params)\n",
            "  ModularFF_100_100_a0.5: 65.8% (118,000 params)\n",
            "  ModularFF_100_100_a1.0: 50.0% (118,000 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: Pendigits | ACTIVATION: perceptron\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  Pendigits | seed=42 | K=10 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [200, 200]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=1.996  train=11.8%  val=14.0%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.992  train=14.2%  val=16.1%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.990  train=15.9%  val=17.8%  (p=4)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.988  train=17.2%  val=19.4%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.987  train=19.2%  val=20.9%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.985  train=19.3%  val=19.9%  (p=3)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.986  train=19.9%  val=21.6%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.984  train=20.7%  val=21.1%  (p=11)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.985  train=21.1%  val=21.8%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.985  train=21.3%  val=22.6%  (p=9)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.984  train=21.2%  val=22.9%  (p=19)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 101 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.984  train=20.7%  val=22.8%  (p=29)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.984  train=21.2%  val=22.4%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.984  train=21.5%  val=24.6%  (p=9)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.983  train=21.1%  val=22.2%  (p=19)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 141 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.984  train=21.2%  val=23.1%  (p=29)\n",
            "  [ClassicFF_ADAM] ep 160  loss=1.984  train=21.8%  val=23.7%  (p=39)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 161 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 170  loss=1.984  train=21.0%  val=22.0%  (p=49)\n",
            "  [ClassicFF_ADAM] ep 180  loss=1.983  train=21.6%  val=23.7%  (p=1)\n",
            "  [ClassicFF_ADAM] ep 190  loss=1.983  train=21.6%  val=22.7%  (p=11)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 199 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 200  loss=1.984  train=21.3%  val=22.9%  (p=21)\n",
            "  [ClassicFF_ADAM] ep 210  loss=1.984  train=21.6%  val=21.2%  (p=31)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 219 (reduction #5)\n",
            "  [ClassicFF_ADAM] ep 220  loss=1.983  train=22.0%  val=23.7%  (p=41)\n",
            "  [ClassicFF_ADAM] ep 230  loss=1.983  train=21.1%  val=22.2%  (p=51)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 239 (best_val=25.1%, 5 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=25.1%)\n",
            "  [ClassicFF_ADAM] DONE  test=21.41%  48s  45600 params  (LR=0.01, adam, act=perceptron)\n",
            "    Goodness: L0:g+=0.509/g-=0.497/sep=0.012/th=0.500 | L1:g+=0.496/g-=0.477/sep=0.019/th=0.500\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=1.995  train=12.9%  val=13.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=1.980  train=23.2%  val=23.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=1.969  train=28.4%  val=29.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=1.960  train=32.4%  val=32.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  40  loss=1.954  train=35.0%  val=36.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=1.949  train=37.1%  val=37.3%  (p=3)\n",
            "  [ClassicFF_SGD] ep  60  loss=1.949  train=38.9%  val=38.0%  (p=1)\n",
            "  [ClassicFF_SGD] ep  70  loss=1.945  train=37.1%  val=36.7%  (p=2)\n",
            "  [ClassicFF_SGD] ep  80  loss=1.946  train=38.3%  val=39.5%  (p=6)\n",
            "  [ClassicFF_SGD] ep  90  loss=1.944  train=38.4%  val=39.6%  (p=16)\n",
            "  [ClassicFF_SGD] ep 100  loss=1.943  train=39.0%  val=42.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 110  loss=1.944  train=38.2%  val=39.4%  (p=10)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 120 (reduction #1)\n",
            "  [ClassicFF_SGD] ep 120  loss=1.942  train=38.4%  val=39.8%  (p=20)\n",
            "  [ClassicFF_SGD] ep 130  loss=1.943  train=38.9%  val=40.4%  (p=30)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 140 (reduction #2)\n",
            "  [ClassicFF_SGD] ep 140  loss=1.940  train=38.2%  val=40.8%  (p=40)\n",
            "  [ClassicFF_SGD] ep 150  loss=1.942  train=38.1%  val=39.3%  (p=50)\n",
            "  [ClassicFF_SGD] ep 160  loss=1.942  train=37.5%  val=39.6%  (p=60)\n",
            "  [ClassicFF_SGD] Early stop at epoch 160 (best_val=42.0%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=42.0%)\n",
            "  [ClassicFF_SGD] DONE  test=38.08%  32s  45600 params  (LR=0.03, sgd, act=perceptron)\n",
            "    Goodness: L0:g+=0.499/g-=0.494/sep=0.004/th=0.500 | L1:g+=0.569/g-=0.523/sep=0.045/th=0.500\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP] Skipped (perceptron activation is FF-specific)\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.998  train=13.4%  val=14.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.951  train=40.4%  val=39.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.934  train=53.9%  val=54.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.922  train=62.9%  val=64.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.915  train=68.7%  val=69.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.907  train=72.9%  val=72.3%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.901  train=75.3%  val=75.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.899  train=77.4%  val=77.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.894  train=78.7%  val=78.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.891  train=80.2%  val=80.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.887  train=81.1%  val=81.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.883  train=81.6%  val=81.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.883  train=82.5%  val=82.6%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.880  train=83.3%  val=83.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.877  train=83.9%  val=83.7%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.876  train=84.1%  val=84.7%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.873  train=84.9%  val=85.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.871  train=85.3%  val=85.5%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.869  train=85.7%  val=85.5%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.868  train=85.9%  val=86.1%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.867  train=86.6%  val=85.9%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.864  train=86.9%  val=85.9%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 216 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.864  train=87.1%  val=86.1%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.862  train=87.1%  val=86.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.861  train=87.5%  val=86.3%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.858  train=87.7%  val=87.0%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.856  train=87.7%  val=87.2%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.857  train=87.9%  val=86.7%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.856  train=88.2%  val=87.3%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.856  train=88.2%  val=87.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.855  train=88.6%  val=88.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.852  train=88.5%  val=87.1%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 320 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.852  train=88.7%  val=87.3%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.850  train=88.8%  val=87.5%  (p=30)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.851  train=88.9%  val=88.1%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 350  loss=0.850  train=89.1%  val=87.8%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 360  loss=0.848  train=89.3%  val=88.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 370  loss=0.849  train=89.3%  val=88.4%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 380  loss=0.846  train=89.3%  val=88.4%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 390  loss=0.847  train=89.5%  val=88.9%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 400  loss=0.845  train=89.5%  val=88.6%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 404 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 410  loss=0.844  train=89.8%  val=88.7%  (p=26)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 420  loss=0.844  train=89.8%  val=88.5%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 424 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 430  loss=0.844  train=89.7%  val=88.7%  (p=46)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 440  loss=0.844  train=89.7%  val=88.9%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 450  loss=0.842  train=89.8%  val=89.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 460  loss=0.841  train=89.8%  val=89.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 470  loss=0.840  train=90.1%  val=89.1%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 479 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 480  loss=0.838  train=90.0%  val=89.1%  (p=21)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 490  loss=0.839  train=90.2%  val=89.1%  (p=31)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 500  loss=0.840  train=90.2%  val=89.2%  (p=7)\n",
            "  [ModularFF] Restored best checkpoint (val=89.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=86.0%, calibrated=84.3%, linear=57.6%, mlp=82.8%, temperature=84.0%\n",
            "    test=85.99%  78s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.566/g-=0.352/sep=0.214/th=0.500 | L1:g+=0.553/g-=0.384/sep=0.169/th=0.500\n",
            "    Avg specialist: acc=91.8%, separation=0.3\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.998  train=16.4%  val=16.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.875  train=70.9%  val=69.9%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.802  train=81.3%  val=82.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.750  train=84.8%  val=84.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.703  train=86.8%  val=86.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.651  train=88.0%  val=88.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.602  train=88.8%  val=88.1%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.560  train=89.3%  val=89.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.514  train=89.9%  val=89.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.473  train=90.4%  val=90.2%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.438  train=91.1%  val=90.3%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.401  train=91.6%  val=90.8%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.379  train=91.9%  val=90.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.347  train=92.1%  val=91.8%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.324  train=92.5%  val=91.8%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.303  train=92.4%  val=91.7%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 157 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.288  train=92.7%  val=92.1%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.274  train=92.9%  val=92.5%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.260  train=93.0%  val=92.7%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.245  train=93.4%  val=92.7%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.238  train=93.4%  val=93.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.222  train=93.5%  val=92.8%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 220 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 220  loss=0.215  train=93.6%  val=93.0%  (p=20)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 230  loss=0.207  train=93.8%  val=93.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 240  loss=0.198  train=93.8%  val=93.7%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 250  loss=0.188  train=93.8%  val=93.7%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 260  loss=0.182  train=94.2%  val=93.7%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 267 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 270  loss=0.178  train=94.2%  val=93.6%  (p=23)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 280  loss=0.181  train=94.3%  val=93.7%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 287 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 290  loss=0.170  train=94.3%  val=93.7%  (p=43)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 300  loss=0.171  train=94.3%  val=94.1%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 310  loss=0.163  train=94.5%  val=94.0%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 320  loss=0.160  train=94.6%  val=94.1%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 330  loss=0.149  train=94.7%  val=94.3%  (p=12)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 340  loss=0.156  train=94.5%  val=94.3%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 350  loss=0.151  train=94.6%  val=94.3%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 352 (reduction #5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 360  loss=0.146  train=94.7%  val=94.3%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 370  loss=0.149  train=94.7%  val=94.3%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 372 (reduction #6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 380  loss=0.138  train=94.9%  val=94.3%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 390  loss=0.145  train=94.8%  val=94.6%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 394 (reduction #7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 400  loss=0.136  train=94.9%  val=94.7%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 410  loss=0.131  train=95.0%  val=94.6%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 420  loss=0.133  train=95.0%  val=94.6%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 425 (reduction #8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 430  loss=0.129  train=95.0%  val=94.8%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 440  loss=0.131  train=94.9%  val=94.7%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 449 (reduction #9)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 450  loss=0.126  train=95.1%  val=94.8%  (p=21)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 460  loss=0.126  train=94.9%  val=94.8%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 470  loss=0.122  train=95.2%  val=94.7%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 474 (reduction #10)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 480  loss=0.122  train=95.0%  val=94.9%  (p=26)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 490  loss=0.117  train=95.0%  val=94.9%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 494 (reduction #11)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 500  loss=0.121  train=95.1%  val=94.8%  (p=46)\n",
            "  [ModularFF] Restored best checkpoint (val=94.9%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=89.4%, calibrated=86.7%, linear=87.7%, mlp=91.0%, temperature=89.4%\n",
            "    test=89.37%  78s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.952/g-=0.032/sep=0.920/th=0.500 | L1:g+=0.940/g-=0.049/sep=0.891/th=0.500\n",
            "    Avg specialist: acc=94.6%, separation=1.7\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.997  train=21.5%  val=20.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.794  train=78.1%  val=77.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.691  train=83.5%  val=82.4%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.619  train=84.8%  val=83.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.553  train=87.1%  val=87.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.476  train=88.8%  val=88.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.405  train=89.7%  val=89.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.352  train=90.3%  val=89.9%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.303  train=91.2%  val=90.2%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.273  train=91.6%  val=90.9%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.250  train=92.0%  val=91.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.223  train=92.2%  val=90.9%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.211  train=92.4%  val=91.4%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.192  train=92.6%  val=91.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.180  train=93.0%  val=91.9%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.169  train=93.1%  val=91.7%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.162  train=93.3%  val=92.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.155  train=93.5%  val=92.4%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.147  train=93.6%  val=92.8%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.142  train=93.8%  val=92.9%  (p=13)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.142  train=93.9%  val=93.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.128  train=93.9%  val=93.3%  (p=10)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 220  loss=0.128  train=94.1%  val=93.5%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 230  loss=0.125  train=94.0%  val=93.6%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 240  loss=0.121  train=94.1%  val=93.7%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 250  loss=0.114  train=94.0%  val=93.4%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 260  loss=0.112  train=94.2%  val=93.7%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 262 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 270  loss=0.111  train=94.2%  val=93.7%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 280  loss=0.116  train=94.4%  val=93.7%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 289 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 290  loss=0.107  train=94.2%  val=93.7%  (p=21)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 300  loss=0.111  train=94.2%  val=93.8%  (p=31)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 310  loss=0.106  train=94.3%  val=94.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 320  loss=0.104  train=94.4%  val=93.9%  (p=11)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 330  loss=0.098  train=94.6%  val=94.0%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 340  loss=0.104  train=94.3%  val=94.2%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 345 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 350  loss=0.102  train=94.2%  val=94.0%  (p=25)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 360  loss=0.098  train=94.4%  val=94.1%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 365 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 370  loss=0.101  train=94.3%  val=94.0%  (p=45)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 380  loss=0.093  train=94.4%  val=93.8%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 385 (best_val=94.3%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=94.3%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=88.9%, calibrated=86.4%, linear=86.8%, mlp=90.2%, temperature=88.3%\n",
            "    test=88.91%  60s  34000 params\n",
            "    Goodness (spec0): L0:g+=0.975/g-=0.029/sep=0.946/th=0.500 | L1:g+=0.965/g-=0.038/sep=0.927/th=0.500\n",
            "    Avg specialist: acc=94.4%, separation=1.7\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.993  train=18.2%  val=17.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.953  train=53.9%  val=54.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.938  train=67.4%  val=68.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.930  train=73.4%  val=73.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.924  train=77.3%  val=76.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.917  train=79.5%  val=79.1%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.913  train=81.1%  val=80.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.911  train=82.3%  val=81.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.906  train=83.2%  val=82.8%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.904  train=84.3%  val=84.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.901  train=84.9%  val=85.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.899  train=85.7%  val=85.8%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.899  train=86.4%  val=86.5%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.895  train=86.7%  val=86.3%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.893  train=87.0%  val=87.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.892  train=87.4%  val=88.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.891  train=88.0%  val=88.7%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.889  train=88.4%  val=89.0%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.887  train=88.7%  val=89.3%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.886  train=88.9%  val=89.5%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.886  train=89.0%  val=90.0%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.882  train=89.5%  val=90.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.884  train=89.7%  val=90.6%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.881  train=89.9%  val=90.4%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.880  train=90.0%  val=90.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.878  train=89.8%  val=91.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.877  train=90.4%  val=91.4%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.876  train=90.6%  val=91.2%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 276 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.877  train=90.4%  val=91.2%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.876  train=90.7%  val=91.9%  (p=34)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.875  train=90.8%  val=91.9%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.873  train=91.0%  val=91.8%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.874  train=91.4%  val=92.2%  (p=18)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.872  train=91.2%  val=92.6%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.873  train=91.5%  val=92.5%  (p=14)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 350  loss=0.871  train=91.5%  val=92.3%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 360  loss=0.869  train=91.6%  val=92.4%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 364 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 370  loss=0.871  train=91.7%  val=92.6%  (p=26)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 380  loss=0.869  train=91.8%  val=92.7%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 384 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 390  loss=0.869  train=91.8%  val=92.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 400  loss=0.867  train=92.1%  val=92.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 410  loss=0.866  train=92.2%  val=92.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 420  loss=0.866  train=92.2%  val=93.0%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 427 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 430  loss=0.866  train=92.4%  val=93.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 440  loss=0.866  train=92.5%  val=93.1%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 449 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 450  loss=0.865  train=92.4%  val=93.3%  (p=21)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 460  loss=0.864  train=92.7%  val=93.2%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 469 (reduction #6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 470  loss=0.863  train=92.5%  val=93.6%  (p=41)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 480  loss=0.862  train=92.7%  val=93.9%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 490  loss=0.862  train=92.8%  val=94.0%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 497 (reduction #7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 500  loss=0.863  train=92.8%  val=93.8%  (p=23)\n",
            "  [ModularFF] Restored best checkpoint (val=94.0%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=87.5%, calibrated=86.0%, linear=60.5%, mlp=79.7%, temperature=85.8%\n",
            "    test=87.54%  78s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.562/g-=0.405/sep=0.157/th=0.500 | L1:g+=0.559/g-=0.398/sep=0.160/th=0.500\n",
            "    Avg specialist: acc=93.6%, separation=0.3\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.992  train=22.1%  val=21.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.877  train=74.3%  val=74.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.804  train=83.1%  val=82.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.751  train=86.8%  val=86.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.702  train=87.9%  val=88.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.646  train=89.3%  val=88.9%  (p=3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.593  train=89.9%  val=90.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.548  train=90.8%  val=90.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.500  train=91.1%  val=91.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.460  train=91.4%  val=91.8%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.424  train=91.9%  val=91.5%  (p=11)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.388  train=92.2%  val=92.1%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.365  train=92.7%  val=92.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.334  train=93.0%  val=92.4%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.311  train=93.1%  val=92.4%  (p=14)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.288  train=93.3%  val=93.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.275  train=93.4%  val=93.3%  (p=10)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.258  train=93.2%  val=93.3%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.245  train=93.6%  val=93.5%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.232  train=93.6%  val=93.4%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.223  train=93.9%  val=93.4%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 202 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.209  train=94.1%  val=93.5%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 220  loss=0.204  train=94.1%  val=93.9%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 222 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 230  loss=0.194  train=94.4%  val=94.0%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 240  loss=0.189  train=94.4%  val=94.1%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 250  loss=0.179  train=94.3%  val=94.1%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 252 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 260  loss=0.172  train=94.6%  val=94.1%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 270  loss=0.169  train=94.7%  val=94.3%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 276 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 280  loss=0.173  train=94.7%  val=94.0%  (p=24)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 290  loss=0.161  train=94.6%  val=94.1%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 296 (reduction #5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 300  loss=0.162  train=94.6%  val=94.0%  (p=44)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 310  loss=0.155  train=94.8%  val=94.1%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 316 (best_val=94.3%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=94.3%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=88.9%, calibrated=86.4%, linear=86.8%, mlp=91.3%, temperature=89.2%\n",
            "    test=88.94%  49s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.944/g-=0.074/sep=0.870/th=0.500 | L1:g+=0.939/g-=0.100/sep=0.839/th=0.500\n",
            "    Avg specialist: acc=94.9%, separation=1.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.991  train=25.6%  val=25.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.793  train=80.9%  val=80.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.687  train=85.8%  val=85.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.609  train=87.7%  val=86.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.535  train=89.1%  val=87.9%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.459  train=89.8%  val=89.3%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.390  train=90.5%  val=90.0%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.337  train=91.2%  val=90.3%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.291  train=91.8%  val=91.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.258  train=92.1%  val=91.6%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.234  train=92.3%  val=91.8%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.211  train=92.7%  val=92.0%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.198  train=92.8%  val=92.3%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.180  train=93.0%  val=92.3%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.170  train=93.1%  val=92.4%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.157  train=93.4%  val=93.0%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.152  train=93.6%  val=93.1%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 168 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.146  train=93.7%  val=93.1%  (p=22)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.139  train=93.7%  val=93.3%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.135  train=93.9%  val=93.4%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.133  train=94.2%  val=93.4%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 201 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.121  train=94.2%  val=93.5%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 220  loss=0.123  train=94.1%  val=93.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 230  loss=0.118  train=94.2%  val=93.7%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 240  loss=0.115  train=94.2%  val=93.8%  (p=16)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 250  loss=0.110  train=94.4%  val=93.8%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 260  loss=0.107  train=94.2%  val=93.9%  (p=18)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 270  loss=0.107  train=94.3%  val=94.2%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 280  loss=0.111  train=94.3%  val=94.3%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 290  loss=0.102  train=94.3%  val=94.2%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 294 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 300  loss=0.107  train=94.4%  val=94.1%  (p=26)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 310  loss=0.101  train=94.5%  val=94.1%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 314 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 320  loss=0.101  train=94.4%  val=94.0%  (p=46)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 330  loss=0.094  train=94.4%  val=94.0%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 334 (best_val=94.4%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=94.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=88.9%, calibrated=86.5%, linear=86.4%, mlp=90.3%, temperature=88.4%\n",
            "    test=88.85%  52s  118000 params\n",
            "    Goodness (spec0): L0:g+=0.968/g-=0.030/sep=0.938/th=0.500 | L1:g+=0.967/g-=0.034/sep=0.934/th=0.500\n",
            "    Avg specialist: acc=94.7%, separation=1.7\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/Pendigits/results_2layer_seed42.json\n",
            "\n",
            "  --- Pendigits 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 21.4% (45,600 params)\n",
            "  ClassicFF_SGD: 38.1% (45,600 params)\n",
            "  ModularFF_50_50_a0.0: 86.0% (34,000 params)\n",
            "  ModularFF_50_50_a0.5: 89.4% (34,000 params)\n",
            "  ModularFF_50_50_a1.0: 88.9% (34,000 params)\n",
            "  ModularFF_100_100_a0.0: 87.5% (118,000 params)\n",
            "  ModularFF_100_100_a0.5: 88.9% (118,000 params)\n",
            "  ModularFF_100_100_a1.0: 88.9% (118,000 params)\n",
            "\n",
            "==========================================================================================\n",
            " Pendigits — 2-LAYER CROSS-ACTIVATION SUMMARY\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               96.7%    95.7%      93.7%      -1.9%\n",
            "tanh               97.0%    89.8%      86.9%      -2.9%\n",
            "hardlimit          94.5%    69.8%      72.1%      +2.3%\n",
            "perceptron           N/A    38.1%      89.4%     +51.3%\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n",
            "  DATASET: LetterRecog\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: LetterRecog | ACTIVATION: gelu\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  LetterRecog | seed=42 | K=26 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [400, 400]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.779  train=49.7%  val=48.9%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.625  train=59.2%  val=58.1%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.345  train=65.4%  val=63.9%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.205  train=67.7%  val=66.3%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.127  train=67.9%  val=66.6%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.066  train=70.0%  val=69.1%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.026  train=70.4%  val=69.9%  (p=9)\n",
            "  [ClassicFF_ADAM] ep  70  loss=0.995  train=72.4%  val=71.3%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  80  loss=0.972  train=72.6%  val=71.7%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  90  loss=0.943  train=72.6%  val=71.8%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 100  loss=0.940  train=71.7%  val=70.4%  (p=16)\n",
            "  [ClassicFF_ADAM] ep 110  loss=0.924  train=72.8%  val=72.2%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 120  loss=0.906  train=73.9%  val=73.2%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 130  loss=0.906  train=74.2%  val=73.1%  (p=1)\n",
            "  [ClassicFF_ADAM] ep 140  loss=0.910  train=74.7%  val=73.5%  (p=4)\n",
            "  [ClassicFF_ADAM] ep 150  loss=0.877  train=75.8%  val=74.0%  (p=5)\n",
            "  [ClassicFF_ADAM] ep 160  loss=0.884  train=75.9%  val=74.9%  (p=15)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 165 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep 170  loss=0.844  train=76.1%  val=74.7%  (p=4)\n",
            "  [ClassicFF_ADAM] ep 180  loss=0.861  train=76.9%  val=76.0%  (p=2)\n",
            "  [ClassicFF_ADAM] ep 190  loss=0.831  train=76.6%  val=75.6%  (p=12)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 198 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 200  loss=0.835  train=76.8%  val=75.4%  (p=22)\n",
            "  [ClassicFF_ADAM] ep 210  loss=0.836  train=77.2%  val=75.4%  (p=32)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.3e-03 at epoch 218 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 220  loss=0.840  train=77.1%  val=75.7%  (p=42)\n",
            "  [ClassicFF_ADAM] ep 230  loss=0.843  train=77.0%  val=75.9%  (p=52)\n",
            "  [ClassicFF_ADAM] ep 240  loss=0.829  train=77.3%  val=75.9%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 250  loss=0.837  train=77.2%  val=75.9%  (p=1)\n",
            "  [ClassicFF_ADAM] ep 260  loss=0.843  train=77.5%  val=76.1%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 270  loss=0.844  train=77.1%  val=75.9%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 280  loss=0.849  train=77.2%  val=76.4%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 290  loss=0.829  train=77.4%  val=76.3%  (p=13)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 6.3e-04 at epoch 297 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 300  loss=0.827  train=77.6%  val=76.3%  (p=23)\n",
            "  [ClassicFF_ADAM] ep 310  loss=0.832  train=77.6%  val=76.6%  (p=0)\n",
            "  [ClassicFF_ADAM] ep 320  loss=0.825  train=77.6%  val=76.3%  (p=10)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 3.1e-04 at epoch 330 (reduction #5)\n",
            "  [ClassicFF_ADAM] ep 330  loss=0.820  train=77.5%  val=76.2%  (p=20)\n",
            "  [ClassicFF_ADAM] ep 340  loss=0.832  train=77.7%  val=76.3%  (p=30)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.6e-04 at epoch 350 (reduction #6)\n",
            "  [ClassicFF_ADAM] ep 350  loss=0.821  train=77.6%  val=76.4%  (p=40)\n",
            "  [ClassicFF_ADAM] ep 360  loss=0.825  train=77.6%  val=76.5%  (p=50)\n",
            "  [ClassicFF_ADAM] ep 370  loss=0.830  train=77.7%  val=76.8%  (p=0)\n",
            "  [ClassicFF_ADAM] ep 380  loss=0.827  train=77.6%  val=76.5%  (p=10)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 7.8e-05 at epoch 390 (reduction #7)\n",
            "  [ClassicFF_ADAM] ep 390  loss=0.806  train=77.5%  val=76.4%  (p=20)\n",
            "  [ClassicFF_ADAM] ep 400  loss=0.817  train=77.6%  val=76.5%  (p=30)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 3.9e-05 at epoch 410 (reduction #8)\n",
            "  [ClassicFF_ADAM] ep 410  loss=0.821  train=77.7%  val=76.3%  (p=40)\n",
            "  [ClassicFF_ADAM] ep 420  loss=0.822  train=77.6%  val=76.4%  (p=50)\n",
            "  [ClassicFF_ADAM] ep 430  loss=0.815  train=77.6%  val=76.5%  (p=60)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 430 (best_val=76.8%, 8 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=76.8%)\n",
            "  [ClassicFF_ADAM] DONE  test=74.85%  320s  177602 params  (LR=0.01, adam, act=gelu)\n",
            "    Goodness: L0:g+=18.218/g-=11.684/sep=6.533/th=14.978 | L1:g+=27.370/g-=19.661/sep=7.710/th=24.422\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.971  train=2.9%  val=3.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.783  train=3.0%  val=4.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.782  train=3.3%  val=4.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.781  train=3.6%  val=4.6%  (p=1)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.781  train=4.1%  val=5.1%  (p=2)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.780  train=4.6%  val=5.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.780  train=5.0%  val=5.9%  (p=1)\n",
            "  [ClassicFF_SGD] ep  70  loss=2.779  train=5.4%  val=6.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep  80  loss=2.779  train=5.9%  val=6.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  90  loss=2.778  train=6.5%  val=7.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 100  loss=2.778  train=7.1%  val=8.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 110  loss=2.777  train=7.6%  val=8.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 120  loss=2.777  train=8.2%  val=9.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 130  loss=2.776  train=8.8%  val=9.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep 140  loss=2.776  train=9.6%  val=10.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 150  loss=2.776  train=10.4%  val=10.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 160  loss=2.775  train=11.1%  val=11.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 170  loss=2.775  train=11.9%  val=12.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 180  loss=2.774  train=12.7%  val=13.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 190  loss=2.774  train=13.4%  val=14.2%  (p=1)\n",
            "  [ClassicFF_SGD] ep 200  loss=2.774  train=14.3%  val=15.2%  (p=1)\n",
            "  [ClassicFF_SGD] ep 210  loss=2.773  train=15.2%  val=16.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 220  loss=2.773  train=15.9%  val=16.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 230  loss=2.773  train=17.0%  val=17.3%  (p=1)\n",
            "  [ClassicFF_SGD] ep 240  loss=2.773  train=17.8%  val=18.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 250  loss=2.772  train=18.5%  val=19.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 260  loss=2.772  train=19.2%  val=19.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 270  loss=2.772  train=20.1%  val=20.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 280  loss=2.771  train=20.8%  val=21.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 290  loss=2.771  train=21.6%  val=22.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 300  loss=2.771  train=22.4%  val=22.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 310  loss=2.771  train=23.1%  val=23.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep 320  loss=2.770  train=23.6%  val=24.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 330  loss=2.770  train=24.2%  val=24.4%  (p=1)\n",
            "  [ClassicFF_SGD] ep 340  loss=2.770  train=24.7%  val=24.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep 350  loss=2.769  train=25.2%  val=25.4%  (p=0)\n",
            "  [ClassicFF_SGD] ep 360  loss=2.769  train=25.7%  val=26.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 370  loss=2.769  train=26.0%  val=26.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 380  loss=2.769  train=26.6%  val=27.4%  (p=2)\n",
            "  [ClassicFF_SGD] ep 390  loss=2.768  train=27.1%  val=27.8%  (p=4)\n",
            "  [ClassicFF_SGD] ep 400  loss=2.768  train=27.6%  val=28.3%  (p=1)\n",
            "  [ClassicFF_SGD] ep 410  loss=2.768  train=28.0%  val=29.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 420  loss=2.768  train=28.5%  val=29.5%  (p=1)\n",
            "  [ClassicFF_SGD] ep 430  loss=2.767  train=29.1%  val=30.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep 440  loss=2.767  train=29.6%  val=30.6%  (p=0)\n",
            "  [ClassicFF_SGD] ep 450  loss=2.767  train=30.1%  val=31.1%  (p=0)\n",
            "  [ClassicFF_SGD] ep 460  loss=2.766  train=30.4%  val=31.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep 470  loss=2.766  train=31.0%  val=31.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep 480  loss=2.766  train=31.4%  val=32.5%  (p=0)\n",
            "  [ClassicFF_SGD] ep 490  loss=2.766  train=31.8%  val=32.7%  (p=0)\n",
            "  [ClassicFF_SGD] ep 500  loss=2.765  train=32.2%  val=33.0%  (p=0)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=33.0%)\n",
            "  [ClassicFF_SGD] DONE  test=31.30%  363s  177602 params  (LR=0.03, sgd, act=gelu)\n",
            "    Goodness: L0:g+=0.261/g-=0.237/sep=0.024/th=0.249 | L1:g+=0.001/g-=0.001/sep=0.000/th=0.001\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.785  train=9.7%  val=9.5%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.287  train=17.6%  val=17.1%  (p=1)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.170  train=18.9%  val=18.7%  (p=1)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.096  train=19.3%  val=18.2%  (p=11)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 39 (#1)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.068  train=19.6%  val=18.5%  (p=21)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.045  train=20.0%  val=19.1%  (p=1)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.052  train=21.1%  val=19.9%  (p=0)\n",
            "  [ClassicFF-Embed] ep  70  loss=2.040  train=21.5%  val=20.8%  (p=0)\n",
            "  [ClassicFF-Embed] ep  80  loss=2.040  train=21.9%  val=20.8%  (p=2)\n",
            "  [ClassicFF-Embed] ep  90  loss=2.009  train=22.2%  val=21.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep 100  loss=2.014  train=22.4%  val=21.6%  (p=10)\n",
            "  [ClassicFF-Embed] ep 110  loss=1.999  train=22.7%  val=22.0%  (p=8)\n",
            "  [ClassicFF-Embed] ep 120  loss=1.999  train=23.3%  val=22.1%  (p=1)\n",
            "  [ClassicFF-Embed] ep 130  loss=1.997  train=23.8%  val=23.1%  (p=0)\n",
            "  [ClassicFF-Embed] ep 140  loss=1.985  train=23.6%  val=23.1%  (p=1)\n",
            "  [ClassicFF-Embed] ep 150  loss=1.972  train=23.8%  val=23.2%  (p=5)\n",
            "  [ClassicFF-Embed] ep 160  loss=1.969  train=23.8%  val=23.2%  (p=2)\n",
            "  [ClassicFF-Embed] ep 170  loss=1.963  train=24.2%  val=23.8%  (p=4)\n",
            "  [ClassicFF-Embed] ep 180  loss=1.965  train=24.4%  val=23.8%  (p=14)\n",
            "  [ClassicFF-Embed] ep 190  loss=1.967  train=24.8%  val=24.1%  (p=1)\n",
            "  [ClassicFF-Embed] ep 200  loss=1.950  train=25.2%  val=24.4%  (p=6)\n",
            "  [ClassicFF-Embed] ep 210  loss=1.930  train=25.2%  val=24.3%  (p=6)\n",
            "  [ClassicFF-Embed] ep 220  loss=1.929  train=25.3%  val=24.5%  (p=7)\n",
            "  [ClassicFF-Embed] ep 230  loss=1.923  train=25.8%  val=25.2%  (p=1)\n",
            "  [ClassicFF-Embed] ep 240  loss=1.916  train=26.0%  val=25.5%  (p=2)\n",
            "  [ClassicFF-Embed] ep 250  loss=1.926  train=26.0%  val=25.8%  (p=8)\n",
            "  [ClassicFF-Embed] ep 260  loss=1.928  train=26.4%  val=25.5%  (p=18)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 262 (#2)\n",
            "  [ClassicFF-Embed] ep 270  loss=1.908  train=26.7%  val=25.6%  (p=28)\n",
            "  [ClassicFF-Embed] ep 280  loss=1.922  train=26.7%  val=25.5%  (p=9)\n",
            "  [ClassicFF-Embed] ep 290  loss=1.906  train=26.8%  val=25.8%  (p=19)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 291 (#3)\n",
            "  [ClassicFF-Embed] ep 300  loss=1.893  train=26.8%  val=25.9%  (p=5)\n",
            "  [ClassicFF-Embed] ep 310  loss=1.898  train=27.1%  val=26.2%  (p=15)\n",
            "  [ClassicFF-Embed] ep 320  loss=1.906  train=27.2%  val=26.2%  (p=7)\n",
            "  [ClassicFF-Embed] ep 330  loss=1.893  train=27.3%  val=26.3%  (p=4)\n",
            "  [ClassicFF-Embed] ep 340  loss=1.893  train=27.4%  val=26.3%  (p=14)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 346 (#4)\n",
            "  [ClassicFF-Embed] ep 350  loss=1.893  train=27.2%  val=26.4%  (p=24)\n",
            "  [ClassicFF-Embed] ep 360  loss=1.871  train=27.4%  val=26.4%  (p=34)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 366 (#5)\n",
            "  [ClassicFF-Embed] ep 370  loss=1.896  train=27.3%  val=26.3%  (p=44)\n",
            "  [ClassicFF-Embed] ep 380  loss=1.886  train=27.5%  val=26.6%  (p=0)\n",
            "  [ClassicFF-Embed] ep 390  loss=1.889  train=27.5%  val=26.5%  (p=3)\n",
            "  [ClassicFF-Embed] ep 400  loss=1.879  train=27.6%  val=26.5%  (p=13)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 407 (#6)\n",
            "  [ClassicFF-Embed] ep 410  loss=1.884  train=27.6%  val=26.4%  (p=23)\n",
            "  [ClassicFF-Embed] ep 420  loss=1.880  train=27.6%  val=26.5%  (p=33)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 427 (#7)\n",
            "  [ClassicFF-Embed] ep 430  loss=1.874  train=27.6%  val=26.4%  (p=43)\n",
            "  [ClassicFF-Embed] ep 440  loss=1.875  train=27.7%  val=26.4%  (p=53)\n",
            "  [ClassicFF-Embed] Early stop at epoch 447\n",
            "  [ClassicFF-Embed] DONE  test=25.97%  333s  168054 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.781  train=40.8%  val=40.2%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.088  train=33.7%  val=33.1%  (p=8)\n",
            "  [ClassicFF-Additive] ep  20  loss=1.883  train=44.6%  val=44.2%  (p=18)\n",
            "  [ClassicFF-Additive] ep  30  loss=1.774  train=57.5%  val=56.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  40  loss=1.694  train=61.6%  val=60.1%  (p=0)\n",
            "  [ClassicFF-Additive] ep  50  loss=1.645  train=64.8%  val=62.9%  (p=0)\n",
            "  [ClassicFF-Additive] ep  60  loss=1.610  train=66.7%  val=64.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  70  loss=1.585  train=66.1%  val=63.9%  (p=1)\n",
            "  [ClassicFF-Additive] ep  80  loss=1.562  train=66.7%  val=65.0%  (p=9)\n",
            "  [ClassicFF-Additive] ep  90  loss=1.550  train=68.6%  val=66.9%  (p=4)\n",
            "  [ClassicFF-Additive] ep 100  loss=1.530  train=68.7%  val=66.6%  (p=14)\n",
            "  [ClassicFF-Additive] ep 110  loss=1.527  train=69.1%  val=66.7%  (p=2)\n",
            "  [ClassicFF-Additive] ep 120  loss=1.516  train=69.9%  val=68.3%  (p=2)\n",
            "  [ClassicFF-Additive] ep 130  loss=1.498  train=71.6%  val=69.3%  (p=4)\n",
            "  [ClassicFF-Additive] ep 140  loss=1.494  train=72.1%  val=70.5%  (p=3)\n",
            "  [ClassicFF-Additive] ep 150  loss=1.481  train=73.1%  val=70.9%  (p=13)\n",
            "  [ClassicFF-Additive] ep 160  loss=1.468  train=72.9%  val=71.2%  (p=3)\n",
            "  [ClassicFF-Additive] ep 170  loss=1.469  train=74.4%  val=72.9%  (p=13)\n",
            "  [ClassicFF-Additive] ep 180  loss=1.456  train=76.2%  val=73.8%  (p=4)\n",
            "  [ClassicFF-Additive] ep 190  loss=1.457  train=74.6%  val=72.2%  (p=1)\n",
            "  [ClassicFF-Additive] ep 200  loss=1.460  train=76.4%  val=74.1%  (p=4)\n",
            "  [ClassicFF-Additive] ep 210  loss=1.441  train=77.8%  val=76.3%  (p=0)\n",
            "  [ClassicFF-Additive] ep 220  loss=1.438  train=75.9%  val=74.4%  (p=10)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 230 (#1)\n",
            "  [ClassicFF-Additive] ep 230  loss=1.447  train=77.2%  val=75.7%  (p=20)\n",
            "  [ClassicFF-Additive] ep 240  loss=1.426  train=77.4%  val=75.3%  (p=2)\n",
            "  [ClassicFF-Additive] ep 250  loss=1.420  train=77.9%  val=76.0%  (p=12)\n",
            "  [ClassicFF-Additive] ep 260  loss=1.424  train=78.0%  val=76.7%  (p=9)\n",
            "  [ClassicFF-Additive] ep 270  loss=1.420  train=78.5%  val=76.6%  (p=19)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 271 (#2)\n",
            "  [ClassicFF-Additive] ep 280  loss=1.422  train=78.3%  val=77.1%  (p=0)\n",
            "  [ClassicFF-Additive] ep 290  loss=1.412  train=78.2%  val=76.6%  (p=10)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 300 (#3)\n",
            "  [ClassicFF-Additive] ep 300  loss=1.402  train=78.6%  val=76.8%  (p=20)\n",
            "  [ClassicFF-Additive] ep 310  loss=1.406  train=78.6%  val=77.1%  (p=30)\n",
            "  [ClassicFF-Additive] ep 320  loss=1.407  train=78.9%  val=76.6%  (p=4)\n",
            "  [ClassicFF-Additive] ep 330  loss=1.411  train=78.6%  val=76.7%  (p=8)\n",
            "  [ClassicFF-Additive] ep 340  loss=1.399  train=78.2%  val=76.5%  (p=18)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 342 (#4)\n",
            "  [ClassicFF-Additive] ep 350  loss=1.407  train=78.6%  val=76.8%  (p=28)\n",
            "  [ClassicFF-Additive] ep 360  loss=1.410  train=78.9%  val=77.3%  (p=38)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 362 (#5)\n",
            "  [ClassicFF-Additive] ep 370  loss=1.412  train=79.0%  val=77.2%  (p=48)\n",
            "  [ClassicFF-Additive] ep 380  loss=1.406  train=78.9%  val=77.4%  (p=1)\n",
            "  [ClassicFF-Additive] ep 390  loss=1.409  train=78.8%  val=77.2%  (p=11)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 399 (#6)\n",
            "  [ClassicFF-Additive] ep 400  loss=1.410  train=78.9%  val=77.3%  (p=21)\n",
            "  [ClassicFF-Additive] ep 410  loss=1.404  train=78.8%  val=77.1%  (p=31)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 419 (#7)\n",
            "  [ClassicFF-Additive] ep 420  loss=1.404  train=78.9%  val=77.3%  (p=41)\n",
            "  [ClassicFF-Additive] ep 430  loss=1.396  train=78.9%  val=77.3%  (p=51)\n",
            "  [ClassicFF-Additive] Early stop at epoch 439\n",
            "  [ClassicFF-Additive] DONE  test=76.88%  306s  177602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.948  train=34.3%  val=33.5%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.383  train=56.1%  val=54.0%  (p=1)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.318  train=56.1%  val=55.1%  (p=2)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.300  train=60.5%  val=58.5%  (p=2)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.288  train=54.0%  val=52.7%  (p=12)\n",
            "  [ClassicFF+LA] LR reduced at epoch 48 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.258  train=55.0%  val=54.3%  (p=22)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.263  train=56.7%  val=55.4%  (p=32)\n",
            "  [ClassicFF+LA] LR reduced at epoch 68 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.241  train=58.0%  val=56.4%  (p=42)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.237  train=56.8%  val=55.0%  (p=52)\n",
            "  [ClassicFF+LA] Early stop at epoch 88\n",
            "  [ClassicFF+LA a=0.5] DONE  test=58.03%  71s  177602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.831  train=89.2%  val=86.2%  (p=0)\n",
            "  [BP]       ep  10  loss=0.078  train=97.7%  val=94.3%  (p=4)\n",
            "  [BP]       ep  20  loss=0.081  train=98.5%  val=94.8%  (p=5)\n",
            "  [BP]       ep  30  loss=0.065  train=98.3%  val=94.7%  (p=8)\n",
            "  [BP]       ep  40  loss=0.041  train=99.2%  val=95.0%  (p=18)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 42 (reduction #1)\n",
            "  [BP]       ep  50  loss=0.000  train=100.0%  val=96.4%  (p=4)\n",
            "  [BP]       ep  60  loss=0.000  train=100.0%  val=96.4%  (p=4)\n",
            "  [BP]       ep  70  loss=0.000  train=100.0%  val=96.5%  (p=14)\n",
            "  [BP]       ep  80  loss=0.000  train=100.0%  val=96.5%  (p=6)\n",
            "  [BP]       ep  90  loss=0.000  train=100.0%  val=96.5%  (p=16)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 94 (reduction #2)\n",
            "  [BP]       ep 100  loss=0.000  train=100.0%  val=96.5%  (p=26)\n",
            "  [BP]       ep 110  loss=0.000  train=100.0%  val=96.4%  (p=36)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 114 (reduction #3)\n",
            "  [BP]       ep 120  loss=0.000  train=100.0%  val=96.4%  (p=46)\n",
            "  [BP]       ep 130  loss=0.000  train=100.0%  val=96.4%  (p=56)\n",
            "  [BP]       Early stop at epoch 134 (best_val=96.5%, 3 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=96.5%)\n",
            "  [BP]       DONE  test=97.00%  24s  177626 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.534  train=13.5%  val=13.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.085  train=51.0%  val=49.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.893  train=66.4%  val=64.6%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.842  train=70.2%  val=69.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.815  train=71.3%  val=69.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.783  train=71.6%  val=70.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.770  train=72.9%  val=71.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.759  train=73.0%  val=71.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.742  train=73.7%  val=72.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.751  train=74.2%  val=72.7%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.740  train=74.7%  val=72.8%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.723  train=75.3%  val=73.3%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.731  train=76.0%  val=74.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.713  train=76.0%  val=74.6%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.721  train=76.0%  val=74.8%  (p=15)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.709  train=76.6%  val=74.8%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.710  train=76.6%  val=74.6%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 165 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.708  train=77.0%  val=75.4%  (p=25)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.707  train=77.5%  val=76.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.695  train=77.8%  val=76.4%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.700  train=77.2%  val=76.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.707  train=77.2%  val=75.8%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 215 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.692  train=77.6%  val=76.1%  (p=25)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.698  train=77.8%  val=76.6%  (p=35)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.691  train=77.9%  val=76.6%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.700  train=78.1%  val=76.8%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 251 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.690  train=78.4%  val=76.7%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.703  train=78.5%  val=76.5%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 276 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.695  train=78.3%  val=76.6%  (p=24)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.691  train=78.4%  val=76.5%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 296 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.690  train=78.4%  val=76.7%  (p=44)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.712  train=78.5%  val=76.7%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 316 (best_val=77.0%, 5 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=77.0%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=75.8%, calibrated=62.2%, linear=79.5%, mlp=82.8%, temperature=76.8%\n",
            "    test=75.83%  191s  88400 params\n",
            "    Goodness (spec0): L0:g+=6.388/g-=0.172/sep=6.216/th=1.000 | L1:g+=3.830/g-=0.189/sep=3.641/th=1.000\n",
            "    Avg specialist: acc=88.3%, separation=6.7\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.663  train=12.5%  val=12.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.230  train=49.3%  val=48.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.056  train=66.7%  val=65.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.989  train=70.3%  val=69.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.954  train=70.1%  val=69.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.925  train=70.6%  val=69.1%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.913  train=71.5%  val=70.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.905  train=71.9%  val=69.9%  (p=10)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.892  train=71.2%  val=70.5%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.902  train=72.4%  val=71.1%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.889  train=72.0%  val=71.1%  (p=12)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.880  train=72.6%  val=71.1%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.886  train=73.2%  val=71.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.869  train=72.4%  val=70.6%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 140 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.877  train=72.0%  val=70.8%  (p=20)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.868  train=72.8%  val=71.2%  (p=30)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.872  train=73.2%  val=71.2%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.875  train=72.9%  val=71.3%  (p=12)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.872  train=72.9%  val=71.2%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.862  train=73.1%  val=71.5%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 195 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.861  train=73.2%  val=71.7%  (p=25)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.870  train=73.1%  val=71.6%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 215 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 220  loss=0.861  train=73.1%  val=71.9%  (p=45)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 230  loss=0.872  train=73.2%  val=71.7%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 235 (best_val=71.9%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=71.9%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=71.0%, calibrated=55.1%, linear=75.6%, mlp=80.4%, temperature=70.5%\n",
            "    test=70.95%  161s  88400 params\n",
            "    Goodness (spec0): L0:g+=6.356/g-=0.013/sep=6.343/th=1.000 | L1:g+=3.285/g-=0.142/sep=3.143/th=1.000\n",
            "    Avg specialist: acc=87.4%, separation=5.5\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.789  train=8.6%  val=8.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.341  train=46.0%  val=45.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.160  train=66.1%  val=64.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.085  train=69.1%  val=68.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.043  train=69.7%  val=68.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.014  train=70.1%  val=68.4%  (p=10)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.998  train=71.4%  val=69.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.989  train=71.8%  val=69.5%  (p=5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.974  train=71.6%  val=70.0%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.980  train=71.9%  val=70.6%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.969  train=71.9%  val=70.6%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 106 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.961  train=72.1%  val=71.2%  (p=24)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.969  train=72.3%  val=71.0%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 126 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.955  train=72.5%  val=70.8%  (p=44)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.963  train=72.2%  val=70.7%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 146 (best_val=71.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=71.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=70.8%, calibrated=54.5%, linear=74.0%, mlp=79.8%, temperature=69.8%\n",
            "    test=70.80%  101s  88400 params\n",
            "    Goodness (spec0): L0:g+=5.086/g-=0.083/sep=5.003/th=1.000 | L1:g+=2.051/g-=0.175/sep=1.875/th=1.000\n",
            "    Avg specialist: acc=87.2%, separation=4.6\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.524  train=22.7%  val=23.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.024  train=48.3%  val=47.5%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.880  train=67.5%  val=66.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.830  train=71.2%  val=70.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.800  train=71.6%  val=70.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.766  train=72.4%  val=70.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.749  train=73.8%  val=72.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.739  train=74.0%  val=72.2%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.723  train=74.5%  val=72.7%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.733  train=74.9%  val=72.8%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.724  train=75.2%  val=73.3%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.708  train=75.6%  val=73.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.715  train=76.6%  val=75.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.698  train=76.6%  val=74.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.708  train=76.3%  val=75.2%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.694  train=77.4%  val=75.6%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.696  train=77.1%  val=75.6%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 164 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.693  train=77.6%  val=76.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.691  train=78.1%  val=77.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.681  train=78.4%  val=77.1%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.685  train=78.0%  val=77.3%  (p=14)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.693  train=77.7%  val=76.6%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.680  train=78.2%  val=76.8%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 221 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.683  train=78.5%  val=77.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.677  train=78.6%  val=77.4%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.686  train=78.7%  val=77.4%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 255 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.676  train=79.0%  val=77.5%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.689  train=78.9%  val=77.6%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 277 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.679  train=79.0%  val=77.3%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.678  train=79.0%  val=77.6%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 297 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.677  train=79.2%  val=77.7%  (p=43)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.700  train=79.2%  val=77.8%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.688  train=79.2%  val=77.9%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.687  train=79.2%  val=77.8%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.6e-04 at epoch 332 (reduction #6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.674  train=79.3%  val=77.8%  (p=28)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 350  loss=0.682  train=79.2%  val=77.8%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 7.8e-05 at epoch 352 (reduction #7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 360  loss=0.685  train=79.1%  val=77.7%  (p=48)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 370  loss=0.687  train=79.1%  val=77.7%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 372 (best_val=77.9%, 7 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=77.9%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=76.5%, calibrated=63.0%, linear=80.7%, mlp=83.8%, temperature=77.3%\n",
            "    test=76.48%  226s  306800 params\n",
            "    Goodness (spec0): L0:g+=6.315/g-=0.144/sep=6.170/th=1.000 | L1:g+=4.094/g-=0.293/sep=3.801/th=1.000\n",
            "    Avg specialist: acc=88.1%, separation=6.9\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.658  train=21.1%  val=21.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.191  train=49.1%  val=49.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.040  train=69.4%  val=68.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.982  train=70.6%  val=69.3%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.949  train=69.9%  val=68.7%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 50 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.922  train=70.5%  val=68.7%  (p=20)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.917  train=71.0%  val=69.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.911  train=71.1%  val=69.6%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.901  train=71.1%  val=69.5%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 82 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.912  train=71.5%  val=69.8%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.902  train=71.4%  val=69.9%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.894  train=71.9%  val=70.2%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.905  train=71.8%  val=70.0%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 126 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.887  train=72.1%  val=69.5%  (p=24)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.896  train=72.1%  val=70.3%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 146 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.886  train=72.0%  val=70.0%  (p=44)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.892  train=72.3%  val=70.3%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 166 (best_val=70.6%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=70.6%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=69.5%, calibrated=54.9%, linear=74.7%, mlp=81.2%, temperature=70.4%\n",
            "    test=69.53%  114s  306800 params\n",
            "    Goodness (spec0): L0:g+=4.719/g-=0.041/sep=4.677/th=1.000 | L1:g+=2.495/g-=0.294/sep=2.200/th=1.000\n",
            "    Avg specialist: acc=87.4%, separation=5.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.788  train=15.1%  val=14.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.313  train=49.8%  val=49.8%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.148  train=69.0%  val=67.5%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.078  train=70.4%  val=68.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.039  train=69.9%  val=68.7%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.010  train=70.2%  val=68.6%  (p=10)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.996  train=71.3%  val=69.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.987  train=71.1%  val=68.8%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.971  train=70.4%  val=69.1%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.977  train=71.4%  val=70.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.966  train=71.4%  val=69.8%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.960  train=71.4%  val=69.8%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 111 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.967  train=71.9%  val=70.0%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.952  train=71.6%  val=69.8%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 140  loss=0.961  train=71.7%  val=70.1%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 147 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 150  loss=0.951  train=71.8%  val=69.8%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 160  loss=0.959  train=72.4%  val=70.6%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 170  loss=0.961  train=71.9%  val=69.7%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 179 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 180  loss=0.956  train=72.3%  val=70.3%  (p=21)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 190  loss=0.950  train=72.3%  val=70.7%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 200  loss=0.948  train=72.2%  val=70.1%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 6.3e-04 at epoch 207 (reduction #4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 210  loss=0.956  train=72.2%  val=70.6%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 220  loss=0.948  train=72.3%  val=70.5%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 230  loss=0.960  train=72.3%  val=70.4%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> 3.1e-04 at epoch 236 (reduction #5)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 240  loss=0.950  train=72.3%  val=70.3%  (p=24)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 250  loss=0.953  train=72.4%  val=70.3%  (p=34)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.6e-04 at epoch 256 (reduction #6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 260  loss=0.952  train=72.5%  val=70.2%  (p=44)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 270  loss=0.956  train=72.3%  val=70.1%  (p=54)\n",
            "  [ModularFF] Early stop at epoch 276 (best_val=70.9%, 6 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=70.9%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=gelu] DONE\n",
            "    Meta: argmax=70.2%, calibrated=54.9%, linear=75.8%, mlp=80.6%, temperature=70.9%\n",
            "    test=70.15%  190s  306800 params\n",
            "    Goodness (spec0): L0:g+=5.581/g-=0.143/sep=5.438/th=1.000 | L1:g+=2.221/g-=0.187/sep=2.035/th=1.000\n",
            "    Avg specialist: acc=87.4%, separation=4.9\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/LetterRecog/results_2layer_seed42.json\n",
            "\n",
            "  --- LetterRecog 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 74.9% (177,602 params)\n",
            "  ClassicFF_SGD: 31.3% (177,602 params)\n",
            "  ClassicFF_Embed: 26.0% (168,054 params)\n",
            "  ClassicFF_Additive: 76.9% (177,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 58.0% (177,602 params)\n",
            "  BP: 97.0% (177,626 params)\n",
            "  ModularFF_50_50_a0.0: 75.8% (88,400 params)\n",
            "  ModularFF_50_50_a0.5: 71.0% (88,400 params)\n",
            "  ModularFF_50_50_a1.0: 70.8% (88,400 params)\n",
            "  ModularFF_100_100_a0.0: 76.5% (306,800 params)\n",
            "  ModularFF_100_100_a0.5: 69.5% (306,800 params)\n",
            "  ModularFF_100_100_a1.0: 70.2% (306,800 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: LetterRecog | ACTIVATION: tanh\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  LetterRecog | seed=42 | K=26 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [400, 400]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.697  train=12.4%  val=12.7%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=2.044  train=66.4%  val=65.9%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.988  train=68.0%  val=66.7%  (p=6)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.967  train=68.8%  val=68.1%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.955  train=68.9%  val=67.8%  (p=3)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.944  train=69.3%  val=68.1%  (p=13)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 57 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.944  train=69.9%  val=68.8%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.940  train=69.8%  val=68.5%  (p=10)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 80 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.938  train=69.8%  val=68.4%  (p=20)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.933  train=70.2%  val=68.8%  (p=30)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.938  train=70.1%  val=68.4%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.935  train=70.2%  val=68.8%  (p=18)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 1.3e-03 at epoch 112 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.931  train=70.2%  val=68.8%  (p=6)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.931  train=70.2%  val=68.8%  (p=2)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.934  train=70.3%  val=68.7%  (p=12)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 6.3e-04 at epoch 148 (reduction #4)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.933  train=70.3%  val=68.8%  (p=22)\n",
            "  [ClassicFF_ADAM] ep 160  loss=1.934  train=70.3%  val=68.9%  (p=32)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 3.1e-04 at epoch 168 (reduction #5)\n",
            "  [ClassicFF_ADAM] ep 170  loss=1.933  train=70.3%  val=68.8%  (p=42)\n",
            "  [ClassicFF_ADAM] ep 180  loss=1.933  train=70.3%  val=68.9%  (p=52)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 188 (best_val=69.0%, 5 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=69.0%)\n",
            "  [ClassicFF_ADAM] DONE  test=68.60%  131s  177602 params  (LR=0.01, adam, act=tanh)\n",
            "    Goodness: L0:g+=0.433/g-=-0.078/sep=0.512/th=0.196 | L1:g+=0.833/g-=-0.804/sep=1.637/th=0.014\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.773  train=3.9%  val=3.8%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.773  train=3.9%  val=3.8%  (p=9)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.773  train=3.9%  val=3.8%  (p=19)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 21 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.773  train=3.9%  val=3.8%  (p=29)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.773  train=3.9%  val=3.8%  (p=39)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 41 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.773  train=3.9%  val=3.8%  (p=49)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.773  train=3.9%  val=3.8%  (p=59)\n",
            "  [ClassicFF_SGD] Early stop at epoch 61 (best_val=3.8%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=3.8%)\n",
            "  [ClassicFF_SGD] DONE  test=3.50%  42s  177602 params  (LR=0.03, sgd, act=tanh)\n",
            "    Goodness: L0:g+=0.003/g-=0.001/sep=0.002/th=0.000 | L1:g+=0.000/g-=0.000/sep=0.000/th=-0.000\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.711  train=20.6%  val=20.4%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.276  train=39.8%  val=38.2%  (p=1)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.244  train=44.3%  val=42.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.224  train=45.9%  val=43.5%  (p=3)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.220  train=46.7%  val=44.1%  (p=4)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.213  train=47.9%  val=45.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.209  train=48.6%  val=45.6%  (p=2)\n",
            "  [ClassicFF-Embed] ep  70  loss=2.201  train=48.9%  val=46.7%  (p=0)\n",
            "  [ClassicFF-Embed] ep  80  loss=2.204  train=49.5%  val=46.4%  (p=2)\n",
            "  [ClassicFF-Embed] ep  90  loss=2.197  train=49.7%  val=46.1%  (p=12)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 98 (#1)\n",
            "  [ClassicFF-Embed] ep 100  loss=2.196  train=49.8%  val=46.3%  (p=22)\n",
            "  [ClassicFF-Embed] ep 110  loss=2.195  train=50.3%  val=47.0%  (p=6)\n",
            "  [ClassicFF-Embed] ep 120  loss=2.197  train=50.1%  val=47.0%  (p=16)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 124 (#2)\n",
            "  [ClassicFF-Embed] ep 130  loss=2.195  train=50.2%  val=46.8%  (p=26)\n",
            "  [ClassicFF-Embed] ep 140  loss=2.196  train=50.5%  val=47.2%  (p=0)\n",
            "  [ClassicFF-Embed] ep 150  loss=2.191  train=50.2%  val=46.8%  (p=10)\n",
            "  [ClassicFF-Embed] ep 160  loss=2.188  train=50.4%  val=47.2%  (p=1)\n",
            "  [ClassicFF-Embed] ep 170  loss=2.194  train=50.6%  val=47.0%  (p=7)\n",
            "  [ClassicFF-Embed] ep 180  loss=2.194  train=50.4%  val=47.4%  (p=17)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 183 (#3)\n",
            "  [ClassicFF-Embed] ep 190  loss=2.193  train=50.4%  val=47.2%  (p=27)\n",
            "  [ClassicFF-Embed] ep 200  loss=2.195  train=50.7%  val=47.2%  (p=37)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 203 (#4)\n",
            "  [ClassicFF-Embed] ep 210  loss=2.190  train=50.7%  val=47.1%  (p=47)\n",
            "  [ClassicFF-Embed] ep 220  loss=2.191  train=50.7%  val=47.3%  (p=57)\n",
            "  [ClassicFF-Embed] Early stop at epoch 223\n",
            "  [ClassicFF-Embed] DONE  test=46.60%  156s  168054 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.699  train=52.9%  val=51.4%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.412  train=65.8%  val=64.1%  (p=3)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.365  train=68.2%  val=66.7%  (p=0)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.336  train=69.3%  val=67.7%  (p=7)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.321  train=70.4%  val=68.6%  (p=0)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.312  train=70.5%  val=68.1%  (p=4)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.310  train=69.1%  val=66.3%  (p=14)\n",
            "  [ClassicFF-Additive] ep  70  loss=2.309  train=71.4%  val=69.6%  (p=9)\n",
            "  [ClassicFF-Additive] ep  80  loss=2.305  train=72.0%  val=68.7%  (p=1)\n",
            "  [ClassicFF-Additive] ep  90  loss=2.307  train=72.6%  val=71.2%  (p=11)\n",
            "  [ClassicFF-Additive] ep 100  loss=2.300  train=72.0%  val=70.3%  (p=3)\n",
            "  [ClassicFF-Additive] ep 110  loss=2.299  train=72.9%  val=71.5%  (p=13)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 117 (#1)\n",
            "  [ClassicFF-Additive] ep 120  loss=2.300  train=73.5%  val=71.3%  (p=1)\n",
            "  [ClassicFF-Additive] ep 130  loss=2.296  train=72.3%  val=70.4%  (p=2)\n",
            "  [ClassicFF-Additive] ep 140  loss=2.298  train=74.0%  val=71.6%  (p=3)\n",
            "  [ClassicFF-Additive] ep 150  loss=2.293  train=72.8%  val=70.4%  (p=5)\n",
            "  [ClassicFF-Additive] ep 160  loss=2.293  train=72.3%  val=70.5%  (p=8)\n",
            "  [ClassicFF-Additive] ep 170  loss=2.294  train=73.0%  val=71.0%  (p=18)\n",
            "  [ClassicFF-Additive] ep 180  loss=2.293  train=74.8%  val=72.3%  (p=8)\n",
            "  [ClassicFF-Additive] ep 190  loss=2.292  train=72.2%  val=70.7%  (p=18)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 192 (#2)\n",
            "  [ClassicFF-Additive] ep 200  loss=2.295  train=74.3%  val=71.6%  (p=5)\n",
            "  [ClassicFF-Additive] ep 210  loss=2.290  train=75.1%  val=72.9%  (p=15)\n",
            "  [ClassicFF-Additive] ep 220  loss=2.288  train=75.1%  val=73.2%  (p=8)\n",
            "  [ClassicFF-Additive] ep 230  loss=2.294  train=74.5%  val=72.3%  (p=18)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 232 (#3)\n",
            "  [ClassicFF-Additive] ep 240  loss=2.291  train=75.1%  val=73.2%  (p=6)\n",
            "  [ClassicFF-Additive] ep 250  loss=2.289  train=73.9%  val=71.8%  (p=16)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 254 (#4)\n",
            "  [ClassicFF-Additive] ep 260  loss=2.290  train=74.7%  val=72.4%  (p=26)\n",
            "  [ClassicFF-Additive] ep 270  loss=2.293  train=74.5%  val=72.4%  (p=36)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 274 (#5)\n",
            "  [ClassicFF-Additive] ep 280  loss=2.296  train=74.4%  val=72.3%  (p=46)\n",
            "  [ClassicFF-Additive] ep 290  loss=2.291  train=74.1%  val=71.5%  (p=56)\n",
            "  [ClassicFF-Additive] Early stop at epoch 294\n",
            "  [ClassicFF-Additive] DONE  test=72.60%  198s  177602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.766  train=3.9%  val=3.9%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.553  train=46.4%  val=45.8%  (p=2)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.534  train=43.2%  val=43.2%  (p=6)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.527  train=47.3%  val=46.7%  (p=4)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.524  train=48.8%  val=49.2%  (p=14)\n",
            "  [ClassicFF+LA] LR reduced at epoch 46 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.513  train=53.8%  val=52.8%  (p=24)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.517  train=52.8%  val=51.8%  (p=9)\n",
            "  [ClassicFF+LA a=0.5] ep  70  loss=2.512  train=50.0%  val=49.8%  (p=19)\n",
            "  [ClassicFF+LA] LR reduced at epoch 71 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  80  loss=2.509  train=54.3%  val=53.8%  (p=29)\n",
            "  [ClassicFF+LA a=0.5] ep  90  loss=2.507  train=54.3%  val=53.3%  (p=39)\n",
            "  [ClassicFF+LA] LR reduced at epoch 91 (#3)\n",
            "  [ClassicFF+LA a=0.5] ep 100  loss=2.509  train=55.7%  val=54.6%  (p=49)\n",
            "  [ClassicFF+LA a=0.5] ep 110  loss=2.505  train=55.4%  val=53.9%  (p=59)\n",
            "  [ClassicFF+LA] Early stop at epoch 111\n",
            "  [ClassicFF+LA a=0.5] DONE  test=53.83%  85s  177602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=0.907  train=86.9%  val=84.0%  (p=0)\n",
            "  [BP]       ep  10  loss=0.077  train=98.6%  val=94.2%  (p=0)\n",
            "  [BP]       ep  20  loss=0.110  train=97.0%  val=93.2%  (p=7)\n",
            "  [BP]       ep  30  loss=0.063  train=98.5%  val=94.2%  (p=17)\n",
            "  [BP]       ep  40  loss=0.076  train=98.1%  val=93.9%  (p=7)\n",
            "  [BP]       ep  50  loss=0.074  train=97.9%  val=93.8%  (p=17)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 53 (reduction #1)\n",
            "  [BP]       ep  60  loss=0.001  train=100.0%  val=95.1%  (p=5)\n",
            "  [BP]       ep  70  loss=0.000  train=100.0%  val=95.2%  (p=15)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 75 (reduction #2)\n",
            "  [BP]       ep  80  loss=0.000  train=100.0%  val=95.3%  (p=1)\n",
            "  [BP]       ep  90  loss=0.000  train=100.0%  val=95.3%  (p=11)\n",
            "  [BP]       LR reduced (×0.5) -> 1.3e-03 at epoch 99 (reduction #3)\n",
            "  [BP]       ep 100  loss=0.000  train=100.0%  val=95.2%  (p=21)\n",
            "  [BP]       ep 110  loss=0.000  train=100.0%  val=95.2%  (p=31)\n",
            "  [BP]       LR reduced (×0.5) -> 6.3e-04 at epoch 119 (reduction #4)\n",
            "  [BP]       ep 120  loss=0.000  train=100.0%  val=95.2%  (p=41)\n",
            "  [BP]       ep 130  loss=0.000  train=100.0%  val=95.2%  (p=51)\n",
            "  [BP]       Early stop at epoch 139 (best_val=95.3%, 4 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=95.3%)\n",
            "  [BP]       DONE  test=95.55%  25s  177626 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.377  train=23.4%  val=23.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.131  train=51.7%  val=50.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.995  train=62.3%  val=60.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.923  train=66.8%  val=64.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.889  train=67.2%  val=65.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.859  train=66.4%  val=64.9%  (p=12)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 58 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.848  train=66.6%  val=64.1%  (p=22)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.846  train=67.4%  val=65.7%  (p=32)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.839  train=67.3%  val=65.9%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.841  train=67.5%  val=66.5%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.836  train=67.0%  val=66.3%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 107 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.830  train=67.0%  val=66.1%  (p=23)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.836  train=67.5%  val=66.4%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 127 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.826  train=67.5%  val=66.0%  (p=43)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.830  train=67.5%  val=66.2%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 147 (best_val=66.6%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=66.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=65.8%, calibrated=52.9%, linear=70.3%, mlp=78.1%, temperature=66.6%\n",
            "    test=65.75%  82s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.736/g-=-0.947/sep=1.683/th=0.000 | L1:g+=0.766/g-=-0.999/sep=1.766/th=0.000\n",
            "    Avg specialist: acc=90.5%, separation=2.9\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.404  train=21.7%  val=21.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.261  train=47.0%  val=46.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.212  train=58.4%  val=57.8%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.187  train=61.5%  val=59.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.176  train=61.4%  val=60.2%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.165  train=60.2%  val=58.8%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 52 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.163  train=59.3%  val=58.5%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.160  train=58.8%  val=57.9%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 72 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.159  train=58.4%  val=57.2%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=1.163  train=58.2%  val=56.8%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 92 (best_val=60.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=60.7%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=59.2%, calibrated=51.0%, linear=70.3%, mlp=79.7%, temperature=65.2%\n",
            "    test=59.17%  59s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.642/g-=-0.281/sep=0.923/th=0.000 | L1:g+=0.826/g-=-0.095/sep=0.921/th=0.000\n",
            "    Avg specialist: acc=84.8%, separation=1.8\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.427  train=3.2%  val=3.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.315  train=6.5%  val=6.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.290  train=8.4%  val=8.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.278  train=8.9%  val=9.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.278  train=9.0%  val=8.8%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 50 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.272  train=9.0%  val=9.0%  (p=20)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.272  train=9.3%  val=9.1%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.273  train=9.0%  val=8.9%  (p=13)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 77 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=1.275  train=9.5%  val=9.3%  (p=23)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=1.276  train=9.3%  val=9.0%  (p=33)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 97 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=1.275  train=9.2%  val=9.0%  (p=43)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=1.274  train=9.5%  val=9.2%  (p=53)\n",
            "  [ModularFF] Early stop at epoch 117 (best_val=9.4%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=9.4%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=8.7%, calibrated=0.1%, linear=53.4%, mlp=63.3%, temperature=15.4%\n",
            "    test=8.72%  75s  88400 params\n",
            "    Goodness (spec0): L0:g+=-0.072/g-=-0.002/sep=-0.069/th=0.000 | L1:g+=0.057/g-=0.029/sep=0.028/th=0.000\n",
            "    Avg specialist: acc=50.6%, separation=-0.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.369  train=37.7%  val=36.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.102  train=52.8%  val=51.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.985  train=64.8%  val=62.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.916  train=67.3%  val=64.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.883  train=67.9%  val=65.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.855  train=67.1%  val=65.1%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.844  train=67.3%  val=65.4%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 62 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.841  train=67.8%  val=65.9%  (p=28)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.833  train=68.0%  val=66.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.837  train=67.9%  val=66.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.832  train=67.5%  val=65.9%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 105 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.826  train=67.2%  val=65.5%  (p=25)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.833  train=67.7%  val=66.1%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> 1.3e-03 at epoch 125 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.823  train=67.7%  val=65.9%  (p=45)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.826  train=67.7%  val=65.6%  (p=55)\n",
            "  [ModularFF] Early stop at epoch 145 (best_val=66.5%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=66.5%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=65.5%, calibrated=54.4%, linear=70.3%, mlp=77.3%, temperature=67.3%\n",
            "    test=65.45%  81s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.956/g-=-0.912/sep=1.868/th=0.000 | L1:g+=0.991/g-=-0.999/sep=1.990/th=0.000\n",
            "    Avg specialist: acc=90.7%, separation=2.9\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.400  train=36.7%  val=35.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.249  train=49.8%  val=48.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.209  train=61.2%  val=58.5%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.186  train=63.0%  val=61.1%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.175  train=64.2%  val=62.4%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.163  train=62.5%  val=60.5%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 59 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.159  train=60.1%  val=58.0%  (p=21)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.155  train=60.4%  val=58.9%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 79 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=1.154  train=58.3%  val=56.4%  (p=41)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=1.158  train=57.4%  val=55.3%  (p=51)\n",
            "  [ModularFF] Early stop at epoch 99 (best_val=62.4%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=62.4%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=62.6%, calibrated=50.5%, linear=71.1%, mlp=78.5%, temperature=66.5%\n",
            "    test=62.65%  63s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.704/g-=-0.396/sep=1.100/th=0.000 | L1:g+=0.923/g-=-0.118/sep=1.041/th=0.000\n",
            "    Avg specialist: acc=84.7%, separation=1.8\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.427  train=6.8%  val=6.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.315  train=11.6%  val=10.7%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.288  train=13.0%  val=11.7%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.277  train=12.4%  val=11.2%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.277  train=12.0%  val=11.4%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 42 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.271  train=12.0%  val=11.4%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.271  train=11.6%  val=11.1%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 62 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=1.272  train=11.2%  val=10.6%  (p=48)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=1.274  train=11.4%  val=10.8%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 82 (best_val=12.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=12.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=tanh] DONE\n",
            "    Meta: argmax=13.1%, calibrated=0.0%, linear=54.2%, mlp=67.8%, temperature=15.6%\n",
            "    test=13.05%  53s  306800 params\n",
            "    Goodness (spec0): L0:g+=-0.018/g-=-0.001/sep=-0.017/th=0.000 | L1:g+=-0.000/g-=-0.000/sep=0.000/th=0.000\n",
            "    Avg specialist: acc=50.4%, separation=0.0\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/LetterRecog/results_2layer_seed42.json\n",
            "\n",
            "  --- LetterRecog 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 68.6% (177,602 params)\n",
            "  ClassicFF_SGD: 3.5% (177,602 params)\n",
            "  ClassicFF_Embed: 46.6% (168,054 params)\n",
            "  ClassicFF_Additive: 72.6% (177,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 53.8% (177,602 params)\n",
            "  BP: 95.5% (177,626 params)\n",
            "  ModularFF_50_50_a0.0: 65.8% (88,400 params)\n",
            "  ModularFF_50_50_a0.5: 59.2% (88,400 params)\n",
            "  ModularFF_50_50_a1.0: 8.7% (88,400 params)\n",
            "  ModularFF_100_100_a0.0: 65.5% (306,800 params)\n",
            "  ModularFF_100_100_a0.5: 62.6% (306,800 params)\n",
            "  ModularFF_100_100_a1.0: 13.1% (306,800 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: LetterRecog | ACTIVATION: hardlimit\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  LetterRecog | seed=42 | K=26 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [400, 400]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=2.760  train=8.4%  val=8.3%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=2.705  train=10.8%  val=10.2%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  20  loss=2.710  train=8.8%  val=8.7%  (p=12)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 5.0e-03 at epoch 28 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=2.705  train=8.5%  val=7.8%  (p=22)\n",
            "  [ClassicFF_ADAM] ep  40  loss=2.710  train=8.8%  val=6.9%  (p=32)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> 2.5e-03 at epoch 48 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  50  loss=2.708  train=9.2%  val=7.8%  (p=42)\n",
            "  [ClassicFF_ADAM] ep  60  loss=2.711  train=8.0%  val=7.3%  (p=52)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 68 (best_val=10.6%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=10.6%)\n",
            "  [ClassicFF_ADAM] DONE  test=9.80%  62s  177602 params  (LR=0.01, adam, act=hardlimit)\n",
            "    Goodness: L0:g+=0.500/g-=0.498/sep=0.002/th=0.498 | L1:g+=0.622/g-=0.377/sep=0.245/th=0.483\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=2.773  train=4.3%  val=4.2%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=2.772  train=4.4%  val=4.0%  (p=9)\n",
            "  [ClassicFF_SGD] ep  20  loss=2.773  train=4.4%  val=3.9%  (p=19)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 1.5e-02 at epoch 21 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  30  loss=2.772  train=4.3%  val=4.1%  (p=29)\n",
            "  [ClassicFF_SGD] ep  40  loss=2.773  train=4.4%  val=4.0%  (p=39)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> 7.5e-03 at epoch 41 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  50  loss=2.772  train=4.4%  val=4.0%  (p=49)\n",
            "  [ClassicFF_SGD] ep  60  loss=2.773  train=4.4%  val=4.0%  (p=59)\n",
            "  [ClassicFF_SGD] Early stop at epoch 61 (best_val=4.2%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=4.2%)\n",
            "  [ClassicFF_SGD] DONE  test=4.28%  53s  177602 params  (LR=0.03, sgd, act=hardlimit)\n",
            "    Goodness: L0:g+=0.502/g-=0.499/sep=0.002/th=0.501 | L1:g+=0.467/g-=0.470/sep=-0.003/th=0.469\n",
            "\n",
            "--- Classic FF (Learned Embedding) ---\n",
            "  [ClassicFF-Embed] ep   1  loss=2.748  train=11.2%  val=10.8%  (p=0)\n",
            "  [ClassicFF-Embed] ep  10  loss=2.688  train=8.3%  val=8.1%  (p=9)\n",
            "  [ClassicFF-Embed] ep  20  loss=2.686  train=8.1%  val=7.9%  (p=19)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 21 (#1)\n",
            "  [ClassicFF-Embed] ep  30  loss=2.689  train=8.0%  val=7.6%  (p=29)\n",
            "  [ClassicFF-Embed] ep  40  loss=2.692  train=6.7%  val=6.7%  (p=39)\n",
            "  [ClassicFF-Embed] LR reduced at epoch 41 (#2)\n",
            "  [ClassicFF-Embed] ep  50  loss=2.687  train=7.9%  val=7.0%  (p=49)\n",
            "  [ClassicFF-Embed] ep  60  loss=2.688  train=8.3%  val=7.4%  (p=59)\n",
            "  [ClassicFF-Embed] Early stop at epoch 61\n",
            "  [ClassicFF-Embed] DONE  test=9.80%  56s  168054 params\n",
            "\n",
            "--- Classic FF (Additive Hidden) ---\n",
            "  [ClassicFF-Additive] ep   1  loss=2.719  train=30.8%  val=29.1%  (p=0)\n",
            "  [ClassicFF-Additive] ep  10  loss=2.545  train=21.2%  val=22.1%  (p=7)\n",
            "  [ClassicFF-Additive] ep  20  loss=2.568  train=27.7%  val=28.2%  (p=17)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 23 (#1)\n",
            "  [ClassicFF-Additive] ep  30  loss=2.572  train=31.0%  val=30.1%  (p=27)\n",
            "  [ClassicFF-Additive] ep  40  loss=2.570  train=32.5%  val=31.9%  (p=37)\n",
            "  [ClassicFF-Additive] LR reduced at epoch 43 (#2)\n",
            "  [ClassicFF-Additive] ep  50  loss=2.568  train=36.6%  val=35.2%  (p=47)\n",
            "  [ClassicFF-Additive] ep  60  loss=2.569  train=36.2%  val=36.0%  (p=57)\n",
            "  [ClassicFF-Additive] Early stop at epoch 63\n",
            "  [ClassicFF-Additive] DONE  test=50.68%  56s  177602 params\n",
            "\n",
            "--- Classic FF (LocalAdapt alpha=0.5) ---\n",
            "  [ClassicFF+LA a=0.5] ep   1  loss=2.798  train=11.9%  val=10.9%  (p=0)\n",
            "  [ClassicFF+LA a=0.5] ep  10  loss=2.687  train=21.2%  val=19.7%  (p=5)\n",
            "  [ClassicFF+LA a=0.5] ep  20  loss=2.687  train=19.0%  val=17.2%  (p=15)\n",
            "  [ClassicFF+LA] LR reduced at epoch 25 (#1)\n",
            "  [ClassicFF+LA a=0.5] ep  30  loss=2.683  train=20.0%  val=19.2%  (p=25)\n",
            "  [ClassicFF+LA a=0.5] ep  40  loss=2.683  train=19.9%  val=18.5%  (p=35)\n",
            "  [ClassicFF+LA] LR reduced at epoch 45 (#2)\n",
            "  [ClassicFF+LA a=0.5] ep  50  loss=2.685  train=20.1%  val=19.1%  (p=45)\n",
            "  [ClassicFF+LA a=0.5] ep  60  loss=2.684  train=20.5%  val=19.2%  (p=55)\n",
            "  [ClassicFF+LA] Early stop at epoch 65\n",
            "  [ClassicFF+LA a=0.5] DONE  test=22.43%  63s  177602 params\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP]       ep   1  loss=1.550  train=72.5%  val=68.7%  (p=0)\n",
            "  [BP]       ep  10  loss=0.460  train=86.2%  val=82.8%  (p=1)\n",
            "  [BP]       ep  20  loss=0.396  train=87.8%  val=84.3%  (p=0)\n",
            "  [BP]       ep  30  loss=0.384  train=88.5%  val=83.5%  (p=4)\n",
            "  [BP]       ep  40  loss=0.407  train=87.3%  val=83.1%  (p=14)\n",
            "  [BP]       LR reduced (×0.5) -> 5.0e-03 at epoch 46 (reduction #1)\n",
            "  [BP]       ep  50  loss=0.411  train=87.3%  val=82.6%  (p=24)\n",
            "  [BP]       ep  60  loss=0.409  train=87.6%  val=81.9%  (p=34)\n",
            "  [BP]       LR reduced (×0.5) -> 2.5e-03 at epoch 66 (reduction #2)\n",
            "  [BP]       ep  70  loss=0.409  train=87.1%  val=81.7%  (p=44)\n",
            "  [BP]       ep  80  loss=0.415  train=86.7%  val=81.1%  (p=54)\n",
            "  [BP]       Early stop at epoch 86 (best_val=84.5%, 2 LR reductions)\n",
            "  [BP]       Restored best checkpoint (val=84.5%)\n",
            "  [BP]       DONE  test=83.62%  17s  177626 params\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.378  train=22.0%  val=22.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.231  train=35.7%  val=34.4%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.193  train=37.8%  val=36.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.176  train=34.7%  val=34.4%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 40 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.177  train=29.2%  val=28.4%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.173  train=26.7%  val=26.1%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 60 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.176  train=24.3%  val=23.6%  (p=40)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=1.177  train=23.5%  val=22.9%  (p=50)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=1.173  train=22.7%  val=22.3%  (p=60)\n",
            "  [ModularFF] Early stop at epoch 80 (best_val=36.6%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=36.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=35.9%, calibrated=27.8%, linear=59.4%, mlp=65.4%, temperature=34.9%\n",
            "    test=35.93%  57s  88400 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.229/sep=0.771/th=0.500 | L1:g+=1.000/g-=0.444/sep=0.556/th=0.500\n",
            "    Avg specialist: acc=78.7%, separation=0.9\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.410  train=18.7%  val=19.1%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.289  train=26.1%  val=25.5%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.260  train=22.2%  val=22.6%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 24 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.246  train=15.7%  val=15.5%  (p=26)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.248  train=13.6%  val=12.0%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 44 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.238  train=12.5%  val=12.7%  (p=46)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.237  train=12.0%  val=12.1%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 64 (best_val=29.7%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=29.7%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=29.4%, calibrated=32.0%, linear=51.9%, mlp=58.1%, temperature=38.0%\n",
            "    test=29.45%  51s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.931/g-=0.358/sep=0.573/th=0.500 | L1:g+=0.980/g-=0.467/sep=0.513/th=0.500\n",
            "    Avg specialist: acc=79.4%, separation=0.4\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.443  train=15.6%  val=15.2%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.352  train=18.7%  val=18.2%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.320  train=16.6%  val=16.7%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 23 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.306  train=13.7%  val=13.1%  (p=27)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.308  train=13.0%  val=12.1%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 43 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.301  train=11.5%  val=11.1%  (p=47)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.301  train=10.6%  val=10.4%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 63 (best_val=23.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=23.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=21.7%, calibrated=27.5%, linear=46.1%, mlp=51.1%, temperature=32.2%\n",
            "    test=21.70%  50s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.940/g-=0.271/sep=0.669/th=0.500 | L1:g+=0.540/g-=0.349/sep=0.191/th=0.500\n",
            "    Avg specialist: acc=78.8%, separation=0.2\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.367  train=38.0%  val=38.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=1.211  train=36.1%  val=35.1%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=1.193  train=40.5%  val=39.7%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 23 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=1.187  train=36.2%  val=35.9%  (p=27)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=1.189  train=33.6%  val=33.1%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 43 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=1.184  train=32.6%  val=31.9%  (p=47)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=1.188  train=31.6%  val=30.8%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 63 (best_val=45.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=45.8%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=45.4%, calibrated=40.6%, linear=50.1%, mlp=59.0%, temperature=48.2%\n",
            "    test=45.42%  45s  306800 params\n",
            "    Goodness (spec0): L0:g+=1.000/g-=0.162/sep=0.838/th=0.500 | L1:g+=1.000/g-=0.354/sep=0.646/th=0.500\n",
            "    Avg specialist: acc=81.5%, separation=0.5\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.400  train=32.8%  val=32.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=1.279  train=33.3%  val=33.7%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=1.257  train=28.1%  val=26.9%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=1.242  train=17.5%  val=17.5%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 32 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=1.245  train=14.7%  val=14.3%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=1.236  train=14.0%  val=13.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 52 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=1.235  train=12.9%  val=12.2%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=1.238  train=12.1%  val=12.0%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 72 (best_val=34.1%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=34.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=32.5%, calibrated=27.9%, linear=58.8%, mlp=66.7%, temperature=31.9%\n",
            "    test=32.50%  58s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.999/g-=0.667/sep=0.332/th=0.500 | L1:g+=1.000/g-=0.778/sep=0.222/th=0.500\n",
            "    Avg specialist: acc=76.4%, separation=0.7\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.434  train=28.5%  val=28.5%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=1.345  train=22.9%  val=23.6%  (p=9)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=1.317  train=17.0%  val=17.9%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> 5.0e-03 at epoch 21 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=1.306  train=14.6%  val=14.8%  (p=29)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=1.307  train=12.0%  val=12.4%  (p=39)\n",
            "  [ModularFF] LR reduced (×0.5) -> 2.5e-03 at epoch 41 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=1.300  train=11.3%  val=11.7%  (p=49)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=1.299  train=11.5%  val=11.7%  (p=59)\n",
            "  [ModularFF] Early stop at epoch 61 (best_val=28.5%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=28.5%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=hardlimit] DONE\n",
            "    Meta: argmax=28.0%, calibrated=29.9%, linear=43.0%, mlp=44.3%, temperature=36.1%\n",
            "    test=28.02%  49s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.899/g-=0.406/sep=0.493/th=0.500 | L1:g+=0.550/g-=0.374/sep=0.176/th=0.500\n",
            "    Avg specialist: acc=79.3%, separation=0.2\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/LetterRecog/results_2layer_seed42.json\n",
            "\n",
            "  --- LetterRecog 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 9.8% (177,602 params)\n",
            "  ClassicFF_SGD: 4.3% (177,602 params)\n",
            "  ClassicFF_Embed: 9.8% (168,054 params)\n",
            "  ClassicFF_Additive: 50.7% (177,602 params)\n",
            "  ClassicFF_LocalAdapt_a0.5: 22.4% (177,602 params)\n",
            "  BP: 83.6% (177,626 params)\n",
            "  ModularFF_50_50_a0.0: 35.9% (88,400 params)\n",
            "  ModularFF_50_50_a0.5: 29.4% (88,400 params)\n",
            "  ModularFF_50_50_a1.0: 21.7% (88,400 params)\n",
            "  ModularFF_100_100_a0.0: 45.4% (306,800 params)\n",
            "  ModularFF_100_100_a0.5: 32.5% (306,800 params)\n",
            "  ModularFF_100_100_a1.0: 28.0% (306,800 params)\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: LetterRecog | ACTIVATION: perceptron\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  LetterRecog | seed=42 | K=26 | dim=16\n",
            "  2-LAYER EXPERIMENTS\n",
            "  Classic FF: [400, 400]\n",
            "  ModularFF archs: [[50, 50], [100, 100]]\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF (One-Hot, Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=1.999  train=4.3%  val=3.7%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=1.997  train=5.1%  val=4.9%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  20  loss=1.995  train=6.2%  val=5.6%  (p=1)\n",
            "  [ClassicFF_ADAM] ep  30  loss=1.993  train=7.1%  val=6.1%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  40  loss=1.992  train=8.1%  val=7.3%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  50  loss=1.991  train=8.3%  val=7.7%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  60  loss=1.990  train=8.6%  val=8.4%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  70  loss=1.990  train=9.1%  val=8.4%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  80  loss=1.989  train=9.1%  val=8.3%  (p=8)\n",
            "  [ClassicFF_ADAM] ep  90  loss=1.988  train=9.2%  val=8.5%  (p=18)\n",
            "  [ClassicFF_ADAM] ep 100  loss=1.988  train=9.3%  val=8.3%  (p=8)\n",
            "  [ClassicFF_ADAM] ep 110  loss=1.988  train=9.3%  val=8.3%  (p=7)\n",
            "  [ClassicFF_ADAM] ep 120  loss=1.987  train=9.3%  val=9.0%  (p=17)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 123 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep 130  loss=1.987  train=9.6%  val=8.5%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 140  loss=1.988  train=9.8%  val=9.3%  (p=4)\n",
            "  [ClassicFF_ADAM] ep 150  loss=1.987  train=9.4%  val=8.9%  (p=3)\n",
            "  [ClassicFF_ADAM] ep 160  loss=1.987  train=9.3%  val=8.7%  (p=13)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 167 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep 170  loss=1.987  train=9.3%  val=9.2%  (p=23)\n",
            "  [ClassicFF_ADAM] ep 180  loss=1.987  train=9.4%  val=8.4%  (p=33)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 187 (reduction #3)\n",
            "  [ClassicFF_ADAM] ep 190  loss=1.987  train=9.0%  val=8.5%  (p=43)\n",
            "  [ClassicFF_ADAM] ep 200  loss=1.987  train=9.4%  val=8.6%  (p=53)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 207 (best_val=9.7%, 3 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=9.7%)\n",
            "  [ClassicFF_ADAM] DONE  test=9.07%  155s  177600 params  (LR=0.01, adam, act=perceptron)\n",
            "    Goodness: L0:g+=0.500/g-=0.500/sep=0.000/th=0.500 | L1:g+=0.540/g-=0.524/sep=0.016/th=0.500\n",
            "\n",
            "--- Classic FF (One-Hot, SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=1.999  train=4.3%  val=4.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=1.994  train=7.0%  val=6.8%  (p=0)\n",
            "  [ClassicFF_SGD] ep  20  loss=1.988  train=8.5%  val=8.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  30  loss=1.983  train=9.4%  val=9.3%  (p=0)\n",
            "  [ClassicFF_SGD] ep  40  loss=1.979  train=9.8%  val=10.0%  (p=0)\n",
            "  [ClassicFF_SGD] ep  50  loss=1.975  train=10.0%  val=9.3%  (p=10)\n",
            "  [ClassicFF_SGD] ep  60  loss=1.972  train=10.6%  val=10.3%  (p=1)\n",
            "  [ClassicFF_SGD] ep  70  loss=1.970  train=10.9%  val=10.1%  (p=6)\n",
            "  [ClassicFF_SGD] ep  80  loss=1.968  train=11.2%  val=9.8%  (p=16)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 84 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  90  loss=1.967  train=11.0%  val=10.6%  (p=26)\n",
            "  [ClassicFF_SGD] ep 100  loss=1.967  train=11.0%  val=10.0%  (p=36)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 104 (reduction #2)\n",
            "  [ClassicFF_SGD] ep 110  loss=1.967  train=11.3%  val=10.7%  (p=46)\n",
            "  [ClassicFF_SGD] ep 120  loss=1.965  train=11.5%  val=10.8%  (p=56)\n",
            "  [ClassicFF_SGD] Early stop at epoch 124 (best_val=10.9%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=10.9%)\n",
            "  [ClassicFF_SGD] DONE  test=9.72%  92s  177600 params  (LR=0.03, sgd, act=perceptron)\n",
            "    Goodness: L0:g+=0.499/g-=0.500/sep=-0.001/th=0.500 | L1:g+=0.618/g-=0.590/sep=0.028/th=0.500\n",
            "\n",
            "--- BP Baseline ---\n",
            "  [BP] Skipped (perceptron activation is FF-specific)\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=1.001  train=3.4%  val=3.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.969  train=13.2%  val=12.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.958  train=21.1%  val=19.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.949  train=25.7%  val=25.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.943  train=28.9%  val=29.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.937  train=31.7%  val=31.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.935  train=33.7%  val=33.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.929  train=35.2%  val=35.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.928  train=35.8%  val=35.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.925  train=37.5%  val=36.9%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.923  train=38.5%  val=38.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.921  train=39.5%  val=39.2%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.919  train=39.8%  val=40.3%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.918  train=40.6%  val=40.3%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.915  train=41.1%  val=41.4%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.915  train=41.8%  val=41.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.915  train=42.4%  val=41.9%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.913  train=42.8%  val=42.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.911  train=43.4%  val=43.2%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.910  train=43.4%  val=42.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.909  train=43.9%  val=43.3%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.908  train=43.9%  val=43.4%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.908  train=44.5%  val=43.7%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.905  train=44.6%  val=43.5%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.905  train=44.9%  val=44.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.905  train=45.4%  val=44.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.903  train=45.4%  val=45.3%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.903  train=45.1%  val=45.0%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.903  train=46.0%  val=45.5%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 282 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.902  train=46.3%  val=46.0%  (p=28)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.902  train=46.1%  val=45.6%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.900  train=46.4%  val=45.5%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.899  train=46.6%  val=46.5%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.899  train=47.4%  val=45.6%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.898  train=47.0%  val=46.1%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 350  loss=0.897  train=47.0%  val=46.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 360  loss=0.898  train=47.5%  val=47.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 370  loss=0.897  train=47.7%  val=46.7%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 380  loss=0.896  train=47.6%  val=46.2%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 381 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 390  loss=0.896  train=47.9%  val=47.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 400  loss=0.897  train=47.6%  val=47.1%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 410  loss=0.895  train=48.0%  val=47.3%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 420  loss=0.896  train=48.4%  val=47.8%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 430  loss=0.896  train=48.5%  val=48.0%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 440  loss=0.894  train=48.5%  val=48.0%  (p=11)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 450  loss=0.896  train=48.1%  val=47.6%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 460  loss=0.894  train=48.6%  val=47.9%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 465 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 470  loss=0.894  train=48.9%  val=48.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 480  loss=0.894  train=48.7%  val=48.7%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 490  loss=0.894  train=49.0%  val=48.1%  (p=17)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 500  loss=0.892  train=49.1%  val=48.9%  (p=6)\n",
            "  [ModularFF] Restored best checkpoint (val=49.7%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=48.8%, calibrated=42.3%, linear=45.1%, mlp=49.0%, temperature=50.8%\n",
            "    test=48.77%  273s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.593/g-=0.393/sep=0.200/th=0.500 | L1:g+=0.558/g-=0.436/sep=0.122/th=0.500\n",
            "    Avg specialist: acc=85.5%, separation=0.2\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=1.000  train=3.6%  val=3.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.937  train=27.8%  val=27.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.890  train=44.9%  val=43.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.854  train=53.2%  val=51.9%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.822  train=58.5%  val=57.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.791  train=61.3%  val=60.0%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.764  train=64.0%  val=63.9%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.736  train=65.9%  val=64.7%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.714  train=67.3%  val=66.5%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.692  train=68.0%  val=67.6%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.670  train=68.8%  val=68.1%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.639  train=69.1%  val=67.7%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.618  train=70.0%  val=68.8%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.592  train=70.2%  val=69.2%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.577  train=70.3%  val=69.0%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.553  train=70.5%  val=69.1%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 153 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.535  train=70.6%  val=69.2%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.517  train=70.7%  val=69.0%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 174 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.502  train=70.4%  val=68.7%  (p=26)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.483  train=70.3%  val=69.1%  (p=36)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 194 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.467  train=70.5%  val=69.1%  (p=46)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.454  train=69.9%  val=68.3%  (p=56)\n",
            "  [ModularFF] Early stop at epoch 214 (best_val=69.9%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=69.9%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=68.5%, calibrated=59.7%, linear=64.1%, mlp=70.9%, temperature=69.7%\n",
            "    test=68.53%  116s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.940/g-=0.040/sep=0.900/th=0.500 | L1:g+=0.873/g-=0.096/sep=0.778/th=0.500\n",
            "    Avg specialist: acc=88.8%, separation=0.9\n",
            "\n",
            "--- ModularFF arch=[50, 50] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=1.000  train=4.0%  val=4.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.892  train=38.5%  val=38.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.810  train=54.9%  val=53.9%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.752  train=60.8%  val=58.4%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.705  train=64.1%  val=62.9%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.655  train=66.0%  val=64.3%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.611  train=66.6%  val=66.1%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.567  train=67.7%  val=67.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.528  train=67.5%  val=66.4%  (p=8)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.501  train=67.8%  val=66.5%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 92 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.468  train=67.3%  val=66.3%  (p=28)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.430  train=67.4%  val=65.2%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 112 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.413  train=67.6%  val=65.8%  (p=48)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.383  train=66.6%  val=65.2%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 132 (best_val=67.8%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=67.8%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=65.7%, calibrated=57.6%, linear=64.0%, mlp=71.5%, temperature=68.3%\n",
            "    test=65.72%  72s  88400 params\n",
            "    Goodness (spec0): L0:g+=0.891/g-=0.178/sep=0.713/th=0.500 | L1:g+=0.913/g-=0.180/sep=0.733/th=0.500\n",
            "    Avg specialist: acc=88.0%, separation=0.9\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.993  train=7.3%  val=7.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.968  train=20.6%  val=21.6%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.958  train=29.3%  val=30.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.951  train=34.0%  val=34.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.947  train=37.9%  val=37.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.943  train=39.9%  val=39.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.940  train=42.5%  val=41.7%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.937  train=44.2%  val=43.5%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.934  train=45.1%  val=43.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.933  train=45.8%  val=45.0%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.931  train=47.1%  val=46.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.930  train=47.8%  val=46.9%  (p=4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.929  train=48.3%  val=46.8%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.926  train=49.2%  val=47.6%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.926  train=49.0%  val=48.4%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.925  train=49.7%  val=48.3%  (p=12)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.924  train=50.3%  val=48.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.924  train=51.3%  val=50.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.923  train=51.5%  val=50.0%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.923  train=52.1%  val=51.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.920  train=52.4%  val=50.9%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.920  train=52.6%  val=51.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.919  train=53.2%  val=51.2%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.919  train=53.1%  val=52.0%  (p=6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.917  train=53.1%  val=52.1%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.918  train=53.8%  val=52.8%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.916  train=53.7%  val=51.9%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 265 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 270  loss=0.916  train=54.3%  val=52.3%  (p=25)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 280  loss=0.916  train=54.7%  val=53.2%  (p=35)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 290  loss=0.916  train=54.9%  val=54.0%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 300  loss=0.915  train=54.6%  val=53.2%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 310  loss=0.915  train=54.6%  val=53.3%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 320  loss=0.914  train=55.8%  val=54.0%  (p=2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 330  loss=0.914  train=55.5%  val=54.0%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 340  loss=0.912  train=55.9%  val=53.8%  (p=17)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 350  loss=0.911  train=54.9%  val=53.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 360  loss=0.914  train=55.9%  val=54.7%  (p=3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 370  loss=0.911  train=55.9%  val=54.9%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 380  loss=0.911  train=56.0%  val=55.2%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 390  loss=0.912  train=56.2%  val=54.6%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 400  loss=0.912  train=56.3%  val=55.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 410  loss=0.910  train=56.9%  val=55.8%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 420  loss=0.911  train=56.7%  val=56.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 430  loss=0.911  train=56.8%  val=55.7%  (p=1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 440  loss=0.910  train=57.0%  val=55.5%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 449 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 450  loss=0.911  train=56.6%  val=55.5%  (p=21)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 460  loss=0.910  train=56.9%  val=55.7%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 469 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 470  loss=0.910  train=56.8%  val=55.8%  (p=41)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 480  loss=0.909  train=57.0%  val=55.9%  (p=51)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 490  loss=0.909  train=57.1%  val=55.2%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 500  loss=0.909  train=57.3%  val=55.8%  (p=17)\n",
            "  [ModularFF] Restored best checkpoint (val=56.6%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=56.2%, calibrated=51.6%, linear=47.4%, mlp=49.7%, temperature=52.0%\n",
            "    test=56.17%  273s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.548/g-=0.431/sep=0.117/th=0.500 | L1:g+=0.566/g-=0.452/sep=0.113/th=0.500\n",
            "    Avg specialist: acc=87.4%, separation=0.2\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.992  train=8.4%  val=9.0%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.931  train=38.7%  val=37.8%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.887  train=51.6%  val=50.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.851  train=59.0%  val=57.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.820  train=63.0%  val=61.2%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.789  train=66.0%  val=64.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.761  train=67.9%  val=66.6%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.731  train=69.3%  val=66.8%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.704  train=70.0%  val=68.2%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.683  train=71.0%  val=69.7%  (p=1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.658  train=71.4%  val=69.6%  (p=5)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.629  train=71.5%  val=70.4%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 120  loss=0.606  train=71.8%  val=70.2%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 124 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 130  loss=0.580  train=71.9%  val=70.1%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 140  loss=0.564  train=72.0%  val=70.1%  (p=7)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 150  loss=0.542  train=71.8%  val=69.9%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 153 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 160  loss=0.526  train=71.8%  val=70.8%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 170  loss=0.506  train=71.8%  val=70.4%  (p=12)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 180  loss=0.493  train=71.6%  val=70.6%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 190  loss=0.474  train=71.5%  val=70.4%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 192 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 200  loss=0.456  train=71.1%  val=70.4%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 210  loss=0.442  train=70.9%  val=69.7%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 212 (reduction #4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 220  loss=0.433  train=70.9%  val=70.0%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 230  loss=0.423  train=70.8%  val=69.5%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 232 (best_val=70.9%, 4 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=70.9%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=69.7%, calibrated=61.1%, linear=67.7%, mlp=74.0%, temperature=71.8%\n",
            "    test=69.67%  127s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.962/g-=0.130/sep=0.832/th=0.500 | L1:g+=0.927/g-=0.150/sep=0.777/th=0.500\n",
            "    Avg specialist: acc=89.1%, separation=1.0\n",
            "\n",
            "--- ModularFF arch=[100, 100] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.992  train=9.2%  val=10.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.883  train=46.5%  val=45.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.802  train=60.3%  val=58.6%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.741  train=66.1%  val=64.0%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.691  train=68.6%  val=66.9%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.640  train=69.6%  val=67.9%  (p=2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.592  train=70.0%  val=68.7%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.550  train=70.4%  val=69.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.506  train=70.2%  val=69.2%  (p=7)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.481  train=70.0%  val=68.3%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 93 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.447  train=69.8%  val=68.8%  (p=27)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.417  train=69.2%  val=67.8%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 113 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.397  train=69.2%  val=68.5%  (p=47)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 130  loss=0.369  train=68.6%  val=67.3%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 133 (best_val=70.0%, 2 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=70.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=67.5%, calibrated=59.4%, linear=66.9%, mlp=73.1%, temperature=70.2%\n",
            "    test=67.55%  73s  306800 params\n",
            "    Goodness (spec0): L0:g+=0.970/g-=0.107/sep=0.863/th=0.500 | L1:g+=0.939/g-=0.110/sep=0.829/th=0.500\n",
            "    Avg specialist: acc=88.7%, separation=0.9\n",
            "\n",
            "✓ 2-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/LetterRecog/results_2layer_seed42.json\n",
            "\n",
            "  --- LetterRecog 2-Layer Summary ---\n",
            "  ClassicFF_Adam: 9.1% (177,600 params)\n",
            "  ClassicFF_SGD: 9.7% (177,600 params)\n",
            "  ModularFF_50_50_a0.0: 48.8% (88,400 params)\n",
            "  ModularFF_50_50_a0.5: 68.5% (88,400 params)\n",
            "  ModularFF_50_50_a1.0: 65.7% (88,400 params)\n",
            "  ModularFF_100_100_a0.0: 56.2% (306,800 params)\n",
            "  ModularFF_100_100_a0.5: 69.7% (306,800 params)\n",
            "  ModularFF_100_100_a1.0: 67.5% (306,800 params)\n",
            "\n",
            "==========================================================================================\n",
            " LetterRecog — 2-LAYER CROSS-ACTIVATION SUMMARY\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               97.0%    76.9%      76.5%      -0.4%\n",
            "tanh               95.5%    72.6%      65.8%      -6.8%\n",
            "hardlimit          83.6%    50.7%      45.4%      -5.3%\n",
            "perceptron           N/A     9.7%      69.7%     +59.9%\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n",
            " 2-LAYER EXPERIMENTS COMPLETE (all activations)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "  DATASET: MNIST\n",
            "======================================================================\n",
            "\n",
            "  GELU:\n",
            "    seed=42:\n",
            "      BP:                  98.2%\n",
            "      ClassicFF (Adam):    97.9%\n",
            "      ClassicFF (SGD):     69.8%\n",
            "      ClassicFF (Embed):   59.0%\n",
            "      ClassicFF (Additive):93.7%\n",
            "      ClassicFF (LA a=0.5):96.0%\n",
            "      ModularFF (best):    97.6% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  TANH:\n",
            "    seed=42:\n",
            "      BP:                  97.8%\n",
            "      ClassicFF (Adam):    87.8%\n",
            "      ClassicFF (SGD):     11.3%\n",
            "      ClassicFF (Embed):   77.2%\n",
            "      ClassicFF (Additive):92.5%\n",
            "      ClassicFF (LA a=0.5):85.8%\n",
            "      ModularFF (best):    92.4% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  HARDLIMIT:\n",
            "    seed=42:\n",
            "      BP:                  89.5%\n",
            "      ClassicFF (Adam):    17.0%\n",
            "      ClassicFF (SGD):     9.5%\n",
            "      ClassicFF (Embed):   19.2%\n",
            "      ClassicFF (Additive):69.0%\n",
            "      ClassicFF (LA a=0.5):28.7%\n",
            "      ModularFF (best):    47.6% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  PERCEPTRON:\n",
            "    seed=42:\n",
            "      BP:                  N/A\n",
            "      ClassicFF (Adam):    14.3%\n",
            "      ClassicFF (SGD):     18.1%\n",
            "      ClassicFF (Embed):   0.0%\n",
            "      ClassicFF (Additive):0.0%\n",
            "      ClassicFF (LA a=0.5):0.0%\n",
            "      ModularFF (best):    91.4% [ModularFF_50_50_a1.0]\n",
            "\n",
            "======================================================================\n",
            "  DATASET: FashionMNIST\n",
            "======================================================================\n",
            "\n",
            "  GELU:\n",
            "    seed=42:\n",
            "      BP:                  88.2%\n",
            "      ClassicFF (Adam):    85.5%\n",
            "      ClassicFF (SGD):     52.4%\n",
            "      ClassicFF (Embed):   47.7%\n",
            "      ClassicFF (Additive):83.9%\n",
            "      ClassicFF (LA a=0.5):84.4%\n",
            "      ModularFF (best):    88.2% [ModularFF_50_50_a0.0]\n",
            "\n",
            "  TANH:\n",
            "    seed=42:\n",
            "      BP:                  86.7%\n",
            "      ClassicFF (Adam):    76.8%\n",
            "      ClassicFF (SGD):     27.4%\n",
            "      ClassicFF (Embed):   77.1%\n",
            "      ClassicFF (Additive):83.5%\n",
            "      ClassicFF (LA a=0.5):78.0%\n",
            "      ModularFF (best):    84.0% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  HARDLIMIT:\n",
            "    seed=42:\n",
            "      BP:                  70.2%\n",
            "      ClassicFF (Adam):    27.3%\n",
            "      ClassicFF (SGD):     10.4%\n",
            "      ClassicFF (Embed):   25.4%\n",
            "      ClassicFF (Additive):56.4%\n",
            "      ClassicFF (LA a=0.5):39.8%\n",
            "      ModularFF (best):    39.2% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  PERCEPTRON:\n",
            "    seed=42:\n",
            "      BP:                  N/A\n",
            "      ClassicFF (Adam):    16.6%\n",
            "      ClassicFF (SGD):     22.9%\n",
            "      ClassicFF (Embed):   0.0%\n",
            "      ClassicFF (Additive):0.0%\n",
            "      ClassicFF (LA a=0.5):0.0%\n",
            "      ModularFF (best):    83.6% [ModularFF_50_50_a1.0]\n",
            "\n",
            "======================================================================\n",
            "  DATASET: Pendigits\n",
            "======================================================================\n",
            "\n",
            "  GELU:\n",
            "    seed=42:\n",
            "      BP:                  96.7%\n",
            "      ClassicFF (Adam):    95.7%\n",
            "      ClassicFF (SGD):     65.4%\n",
            "      ClassicFF (Embed):   53.3%\n",
            "      ClassicFF (Additive):92.5%\n",
            "      ClassicFF (LA a=0.5):84.3%\n",
            "      ModularFF (best):    93.7% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  TANH:\n",
            "    seed=42:\n",
            "      BP:                  97.0%\n",
            "      ClassicFF (Adam):    88.2%\n",
            "      ClassicFF (SGD):     41.7%\n",
            "      ClassicFF (Embed):   80.3%\n",
            "      ClassicFF (Additive):89.8%\n",
            "      ClassicFF (LA a=0.5):83.9%\n",
            "      ModularFF (best):    86.9% [ModularFF_50_50_a0.0]\n",
            "\n",
            "  HARDLIMIT:\n",
            "    seed=42:\n",
            "      BP:                  94.5%\n",
            "      ClassicFF (Adam):    33.7%\n",
            "      ClassicFF (SGD):     11.9%\n",
            "      ClassicFF (Embed):   32.3%\n",
            "      ClassicFF (Additive):69.8%\n",
            "      ClassicFF (LA a=0.5):43.0%\n",
            "      ModularFF (best):    72.1% [ModularFF_50_50_a0.0]\n",
            "\n",
            "  PERCEPTRON:\n",
            "    seed=42:\n",
            "      BP:                  N/A\n",
            "      ClassicFF (Adam):    21.4%\n",
            "      ClassicFF (SGD):     38.1%\n",
            "      ClassicFF (Embed):   0.0%\n",
            "      ClassicFF (Additive):0.0%\n",
            "      ClassicFF (LA a=0.5):0.0%\n",
            "      ModularFF (best):    89.4% [ModularFF_50_50_a0.5]\n",
            "\n",
            "======================================================================\n",
            "  DATASET: LetterRecog\n",
            "======================================================================\n",
            "\n",
            "  GELU:\n",
            "    seed=42:\n",
            "      BP:                  97.0%\n",
            "      ClassicFF (Adam):    74.9%\n",
            "      ClassicFF (SGD):     31.3%\n",
            "      ClassicFF (Embed):   26.0%\n",
            "      ClassicFF (Additive):76.9%\n",
            "      ClassicFF (LA a=0.5):58.0%\n",
            "      ModularFF (best):    76.5% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  TANH:\n",
            "    seed=42:\n",
            "      BP:                  95.5%\n",
            "      ClassicFF (Adam):    68.6%\n",
            "      ClassicFF (SGD):     3.5%\n",
            "      ClassicFF (Embed):   46.6%\n",
            "      ClassicFF (Additive):72.6%\n",
            "      ClassicFF (LA a=0.5):53.8%\n",
            "      ModularFF (best):    65.8% [ModularFF_50_50_a0.0]\n",
            "\n",
            "  HARDLIMIT:\n",
            "    seed=42:\n",
            "      BP:                  83.6%\n",
            "      ClassicFF (Adam):    9.8%\n",
            "      ClassicFF (SGD):     4.3%\n",
            "      ClassicFF (Embed):   9.8%\n",
            "      ClassicFF (Additive):50.7%\n",
            "      ClassicFF (LA a=0.5):22.4%\n",
            "      ModularFF (best):    45.4% [ModularFF_100_100_a0.0]\n",
            "\n",
            "  PERCEPTRON:\n",
            "    seed=42:\n",
            "      BP:                  N/A\n",
            "      ClassicFF (Adam):    9.1%\n",
            "      ClassicFF (SGD):     9.7%\n",
            "      ClassicFF (Embed):   0.0%\n",
            "      ClassicFF (Additive):0.0%\n",
            "      ClassicFF (LA a=0.5):0.0%\n",
            "      ModularFF (best):    69.7% [ModularFF_100_100_a0.5]\n",
            "\n",
            "==========================================================================================\n",
            " 2-LAYER RESULTS — MNIST (K=10) — ALL ACTIVATIONS\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               98.2%    97.9%      97.6%      -0.3%\n",
            "tanh               97.8%    92.5%      92.4%      -0.0%\n",
            "hardlimit          89.5%    69.0%      47.6%     -21.4%\n",
            "perceptron           N/A    18.1%      91.4%     +73.3%\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " 2-LAYER RESULTS — FashionMNIST (K=10) — ALL ACTIVATIONS\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               88.2%    85.5%      88.2%      +2.7%\n",
            "tanh               86.7%    83.5%      84.0%      +0.5%\n",
            "hardlimit          70.2%    56.4%      39.2%     -17.2%\n",
            "perceptron           N/A    22.9%      83.6%     +60.6%\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " 2-LAYER RESULTS — Pendigits (K=10) — ALL ACTIVATIONS\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               96.7%    95.7%      93.7%      -1.9%\n",
            "tanh               97.0%    89.8%      86.9%      -2.9%\n",
            "hardlimit          94.5%    69.8%      72.1%      +2.3%\n",
            "perceptron           N/A    38.1%      89.4%     +51.3%\n",
            "==========================================================================================\n",
            "\n",
            "==========================================================================================\n",
            " 2-LAYER RESULTS — LetterRecog (K=26) — ALL ACTIVATIONS\n",
            "==========================================================================================\n",
            "Activation            BP  FF best  ModularFF    Δ vs FF\n",
            "------------------------------------------------------------------------------------------\n",
            "gelu               97.0%    76.9%      76.5%      -0.4%\n",
            "tanh               95.5%    72.6%      65.8%      -6.8%\n",
            "hardlimit          83.6%    50.7%      45.4%      -5.3%\n",
            "perceptron           N/A     9.7%      69.7%     +59.9%\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# CELL 4a: Run 2-Layer Experiments (v6 — Activation Sweep)\n",
        "# ================================================================\n",
        "# Runs all 2-layer experiments for all datasets × activations:\n",
        "#   - ClassicFF (Adam + SGD), Embed, Additive, LocalAdapt\n",
        "#   - BP Baseline\n",
        "#   - ModularFF: all architectures × alpha values\n",
        "#   - Activations: gelu, tanh, hardlimit, perceptron\n",
        "#   - Loop order: DATASET (outer) × ACTIVATION (inner)\n",
        "#     → completes all activations per dataset before moving on\n",
        "# ================================================================\n",
        "\n",
        "#ACTIVATIONS = ['gelu', 'tanh', 'hardlimit', 'perceptron']\n",
        "ACTIVATIONS = ['perceptron']\n",
        "\n",
        "LEARNABLE_THETA = True  # Set False for fixed theta (default values per activation)\n",
        "\n",
        "SHOW_SPECIALIST_PERF = False\n",
        "\n",
        "ALL_RESULTS = {}  # key: (ds_name, activation) -> {seed -> results}\n",
        "\n",
        "print('\\n' + '#'*70)\n",
        "print('#  2-LAYER EXPERIMENTS (with Activation Sweep)')\n",
        "print('#  Datasets:', CONFIG['datasets_to_run'])\n",
        "print('#  Activations:', ACTIVATIONS)\n",
        "print('#  Alpha values:', CONFIG.get('alpha_values', [0.0, 0.5, 1.0]))\n",
        "print('#  Adam LR:', CONFIG['lr'], '| SGD LR:', CONFIG.get('hinton_sgd_lr', 0.03),\n",
        "      '| Batch:', CONFIG['batch_size'], '| Patience:', CONFIG['early_stop_patience'])\n",
        "print('#  Loop order: DATASET (outer) × ACTIVATION (inner)')\n",
        "print('#'*70)\n",
        "\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    if ds_name not in DATASETS:\n",
        "        print(f'WARNING: {ds_name} not loaded, skipping')\n",
        "        continue\n",
        "\n",
        "    print('\\n' + '='*70)\n",
        "    print(f'  DATASET: {ds_name}')\n",
        "    print('='*70)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        print('\\n' + '#'*70)\n",
        "        print(f'#  DATASET: {ds_name} | ACTIVATION: {act}')\n",
        "        print('#'*70)\n",
        "\n",
        "        result_key = (ds_name, act)\n",
        "        ALL_RESULTS[result_key] = {}\n",
        "\n",
        "        for seed in CONFIG['seeds']:\n",
        "            ALL_RESULTS[result_key][seed] = run_2layer_experiment(\n",
        "                ds_name, seed, CONFIG,\n",
        "                show_specialist_perf=SHOW_SPECIALIST_PERF,\n",
        "                activation=act,\n",
        "                learnable_theta=LEARNABLE_THETA\n",
        "            )\n",
        "\n",
        "    # --- Per-dataset summary (all activations) ---\n",
        "    print('\\n' + '='*90)\n",
        "    print(f' {ds_name} — 2-LAYER CROSS-ACTIVATION SUMMARY')\n",
        "    print('='*90)\n",
        "    print(f'{\"Activation\":<15} {\"BP\":>8} {\"FF best\":>8} {\"ModularFF\":>10} {\"Δ vs FF\":>10}')\n",
        "    print('-'*90)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            bp_acc = res.get('BP', {}).get('test_acc', 0)\n",
        "            ff_best = max(\n",
        "                res.get('ClassicFF_Adam', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_SGD', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_Embed', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_Additive', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_LocalAdapt_a0.5', {}).get('test_acc', 0),\n",
        "            )\n",
        "            best_mod = max(\n",
        "                (v.get('test_acc', 0) for k, v in res.items() if k.startswith('ModularFF')),\n",
        "                default=0\n",
        "            )\n",
        "            delta = best_mod - ff_best\n",
        "            delta_str = f'+{delta:.1f}%' if delta > 0 else f'{delta:.1f}%'\n",
        "            bp_str = f'{bp_acc:.1f}%' if bp_acc > 0 else '  N/A'\n",
        "            print(f'{act:<15} {bp_str:>8} {ff_best:>7.1f}% {best_mod:>9.1f}% {delta_str:>10}')\n",
        "\n",
        "    print('='*90)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FINAL SUMMARY (all datasets × all activations)\n",
        "# ================================================================\n",
        "print('\\n' + '='*70)\n",
        "print(' 2-LAYER EXPERIMENTS COMPLETE (all activations)')\n",
        "print('='*70)\n",
        "\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'  DATASET: {ds_name}')\n",
        "    print(f'{\"=\"*70}')\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        print(f'\\n  {act.upper()}:')\n",
        "\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            bp_acc = res.get('BP', {}).get('test_acc', 0)\n",
        "            ff_adam = res.get('ClassicFF_Adam', {}).get('test_acc', 0)\n",
        "            ff_sgd = res.get('ClassicFF_SGD', {}).get('test_acc', 0)\n",
        "            ff_embed = res.get('ClassicFF_Embed', {}).get('test_acc', 0)\n",
        "            ff_add = res.get('ClassicFF_Additive', {}).get('test_acc', 0)\n",
        "            ff_la = res.get('ClassicFF_LocalAdapt_a0.5', {}).get('test_acc', 0)\n",
        "            ff_best = max(ff_adam, ff_sgd, ff_embed, ff_add, ff_la)\n",
        "\n",
        "            best_mod_acc, best_mod_key = 0, ''\n",
        "            for key, val in res.items():\n",
        "                if key.startswith('ModularFF') and 'test_acc' in val:\n",
        "                    if val['test_acc'] > best_mod_acc:\n",
        "                        best_mod_acc = val['test_acc']\n",
        "                        best_mod_key = key\n",
        "\n",
        "            print(f'    seed={seed}:')\n",
        "            bp_str = f'{bp_acc:.1f}%' if bp_acc > 0 else 'N/A'\n",
        "            print(f'      BP:                  {bp_str}')\n",
        "            print(f'      ClassicFF (Adam):    {ff_adam:.1f}%')\n",
        "            print(f'      ClassicFF (SGD):     {ff_sgd:.1f}%')\n",
        "            print(f'      ClassicFF (Embed):   {ff_embed:.1f}%')\n",
        "            print(f'      ClassicFF (Additive):{ff_add:.1f}%')\n",
        "            print(f'      ClassicFF (LA a=0.5):{ff_la:.1f}%')\n",
        "            print(f'      ModularFF (best):    {best_mod_acc:.1f}% [{best_mod_key}]')\n",
        "\n",
        "# ================================================================\n",
        "# GRAND COMPARISON TABLE\n",
        "# ================================================================\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    K = DATASETS[ds_name]['num_classes']\n",
        "    print(f'\\n{\"=\"*90}')\n",
        "    print(f' 2-LAYER RESULTS — {ds_name} (K={K}) — ALL ACTIVATIONS')\n",
        "    print(f'{\"=\"*90}')\n",
        "    print(f'{\"Activation\":<15} {\"BP\":>8} {\"FF best\":>8} {\"ModularFF\":>10} {\"Δ vs FF\":>10}')\n",
        "    print('-'*90)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            bp_acc = res.get('BP', {}).get('test_acc', 0)\n",
        "            ff_best = max(\n",
        "                res.get('ClassicFF_Adam', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_SGD', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_Embed', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_Additive', {}).get('test_acc', 0),\n",
        "                res.get('ClassicFF_LocalAdapt_a0.5', {}).get('test_acc', 0),\n",
        "            )\n",
        "\n",
        "            best_mod = max(\n",
        "                (v.get('test_acc', 0) for k, v in res.items() if k.startswith('ModularFF')),\n",
        "                default=0\n",
        "            )\n",
        "\n",
        "            delta = best_mod - ff_best\n",
        "            delta_str = f'+{delta:.1f}%' if delta > 0 else f'{delta:.1f}%'\n",
        "            bp_str = f'{bp_acc:.1f}%' if bp_acc > 0 else '  N/A'\n",
        "            print(f'{act:<15} {bp_str:>8} {ff_best:>7.1f}% {best_mod:>9.1f}% {delta_str:>10}')\n",
        "\n",
        "    print('='*90)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcAZmGVLxqTV",
        "outputId": "d391092a-8a41-4254-87db-f7c16f644752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "######################################################################\n",
            "#  PHASE 2: 4-LAYER HINTON COMPARISON (with Activation Sweep)\n",
            "#  Datasets: ['FashionMNIST']\n",
            "#  Activations: ['perceptron']\n",
            "#  Alpha values: [0.0, 0.5, 1.0]\n",
            "#  Adam LR: 0.01 | SGD LR: 0.03 | Batch: 256 | Patience: 60\n",
            "#  Loop order: DATASET (outer) × ACTIVATION (inner)\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  DATASET: FashionMNIST\n",
            "======================================================================\n",
            "\n",
            "######################################################################\n",
            "#  DATASET: FashionMNIST | ACTIVATION: perceptron\n",
            "######################################################################\n",
            "\n",
            "======================================================================\n",
            "  FashionMNIST | seed=42 | K=10 | dim=784\n",
            "  4-LAYER HINTON COMPARISON (Phase 2)\n",
            "  Classic FF 4L: [2000, 2000, 2000, 2000]\n",
            "  ModularFF 4L: [550, 550, 550, 550] × 10 specialists\n",
            "  Adam LR=0.01, SGD LR=0.03, batch=256, patience=60, min_ep=50\n",
            "======================================================================\n",
            "\n",
            "--- Classic FF 4-Layer (Adam) ---\n",
            "  [ClassicFF_ADAM] ep   1  loss=4.000  train=10.2%  val=10.4%  (p=0)\n",
            "  [ClassicFF_ADAM] ep  10  loss=3.993  train=10.6%  val=10.7%  (p=5)\n",
            "  [ClassicFF_ADAM] ep  20  loss=3.989  train=18.8%  val=18.4%  (p=2)\n",
            "  [ClassicFF_ADAM] ep  30  loss=3.980  train=12.7%  val=12.6%  (p=12)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 38 (reduction #1)\n",
            "  [ClassicFF_ADAM] ep  40  loss=3.975  train=13.2%  val=13.5%  (p=22)\n",
            "  [ClassicFF_ADAM] ep  50  loss=3.968  train=14.9%  val=14.9%  (p=32)\n",
            "  [ClassicFF_ADAM] LR reduced (×0.5) -> ? at epoch 58 (reduction #2)\n",
            "  [ClassicFF_ADAM] ep  60  loss=3.964  train=14.4%  val=14.2%  (p=42)\n",
            "  [ClassicFF_ADAM] ep  70  loss=3.962  train=14.8%  val=15.1%  (p=52)\n",
            "  [ClassicFF_ADAM] Early stop at epoch 78 (best_val=26.9%, 2 LR reductions)\n",
            "  [ClassicFF_ADAM] Restored best checkpoint (val=26.9%)\n",
            "  [ClassicFF_ADAM] DONE  test=27.21%  744s  13596000 params  (LR=0.01, adam, act=perceptron)\n",
            "    Goodness: L0:g+=0.220/g-=0.222/sep=-0.002/th=0.500 | L1:g+=0.497/g-=0.486/sep=0.010/th=0.500 | L2:g+=0.605/g-=0.597/sep=0.008/th=0.500 | L3:g+=0.628/g-=0.635/sep=-0.008/th=0.500\n",
            "\n",
            "--- Classic FF 4-Layer (SGD, LR=0.03) ---\n",
            "  [ClassicFF_SGD] ep   1  loss=4.000  train=8.8%  val=8.9%  (p=0)\n",
            "  [ClassicFF_SGD] ep  10  loss=3.990  train=8.4%  val=8.3%  (p=3)\n",
            "  [ClassicFF_SGD] ep  20  loss=3.974  train=12.0%  val=12.3%  (p=2)\n",
            "  [ClassicFF_SGD] ep  30  loss=3.964  train=12.8%  val=12.5%  (p=12)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 38 (reduction #1)\n",
            "  [ClassicFF_SGD] ep  40  loss=3.955  train=15.5%  val=15.8%  (p=22)\n",
            "  [ClassicFF_SGD] ep  50  loss=3.939  train=9.2%  val=9.2%  (p=32)\n",
            "  [ClassicFF_SGD] LR reduced (×0.5) -> ? at epoch 58 (reduction #2)\n",
            "  [ClassicFF_SGD] ep  60  loss=3.936  train=15.7%  val=16.0%  (p=42)\n",
            "  [ClassicFF_SGD] ep  70  loss=3.938  train=12.5%  val=12.6%  (p=52)\n",
            "  [ClassicFF_SGD] Early stop at epoch 78 (best_val=23.2%, 2 LR reductions)\n",
            "  [ClassicFF_SGD] Restored best checkpoint (val=23.2%)\n",
            "  [ClassicFF_SGD] DONE  test=24.13%  740s  13596000 params  (LR=0.03, sgd, act=perceptron)\n",
            "    Goodness: L0:g+=0.054/g-=0.053/sep=0.001/th=0.500 | L1:g+=0.703/g-=0.685/sep=0.018/th=0.500 | L2:g+=0.937/g-=0.940/sep=-0.002/th=0.500 | L3:g+=0.091/g-=0.087/sep=0.004/th=0.500\n",
            "\n",
            "--- ModularFF 4L arch=[550, 550, 550, 550] (alpha=0.0) ---\n",
            "  [ModularFF a=0.0 ld=uniform] ep   1  loss=0.931  train=37.6%  val=37.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  10  loss=0.887  train=77.0%  val=77.5%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  20  loss=0.890  train=71.1%  val=71.3%  (p=10)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  30  loss=0.886  train=77.1%  val=77.5%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  40  loss=0.886  train=73.1%  val=73.2%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 41 (reduction #1)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  50  loss=0.887  train=74.2%  val=74.4%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  60  loss=0.886  train=74.2%  val=74.6%  (p=9)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  70  loss=0.889  train=74.9%  val=75.2%  (p=19)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 71 (reduction #2)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  80  loss=0.885  train=71.6%  val=71.9%  (p=29)\n",
            "  [ModularFF a=0.0 ld=uniform] ep  90  loss=0.888  train=72.2%  val=72.4%  (p=5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 100  loss=0.885  train=80.7%  val=80.5%  (p=15)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 105 (reduction #3)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 110  loss=0.883  train=77.9%  val=77.8%  (p=25)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 120  loss=0.884  train=77.2%  val=77.5%  (p=35)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 125 (reduction #4)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 130  loss=0.881  train=75.6%  val=75.9%  (p=45)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 140  loss=0.880  train=76.3%  val=76.2%  (p=8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 150  loss=0.879  train=84.4%  val=84.2%  (p=0)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 160  loss=0.882  train=80.1%  val=80.3%  (p=10)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 170 (reduction #5)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 170  loss=0.882  train=82.4%  val=82.1%  (p=20)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 180  loss=0.876  train=62.0%  val=61.8%  (p=30)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 190 (reduction #6)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 190  loss=0.883  train=69.7%  val=69.5%  (p=40)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 200  loss=0.883  train=79.6%  val=79.4%  (p=50)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 210  loss=0.883  train=68.7%  val=67.9%  (p=7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 220  loss=0.881  train=82.2%  val=81.9%  (p=17)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 223 (reduction #7)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 230  loss=0.882  train=84.3%  val=83.7%  (p=27)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 240  loss=0.879  train=78.6%  val=78.7%  (p=37)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 243 (reduction #8)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 250  loss=0.880  train=78.8%  val=78.8%  (p=47)\n",
            "  [ModularFF a=0.0 ld=uniform] ep 260  loss=0.884  train=74.8%  val=74.8%  (p=57)\n",
            "  [ModularFF] Early stop at epoch 263 (best_val=84.4%, 8 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.4%)\n",
            "  [ModularFF a=0.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=83.5%, calibrated=83.0%, linear=52.2%, mlp=78.6%, temperature=81.8%\n",
            "    test=83.45%  652s  13409000 params\n",
            "    Goodness (spec0): L0:g+=0.613/g-=0.271/sep=0.341/th=0.500 | L1:g+=0.521/g-=0.452/sep=0.069/th=0.500 | L2:g+=0.520/g-=0.449/sep=0.071/th=0.500 | L3:g+=0.526/g-=0.458/sep=0.068/th=0.500\n",
            "    Avg specialist: acc=92.8%, separation=0.5\n",
            "\n",
            "--- ModularFF 4L arch=[550, 550, 550, 550] (alpha=0.5) ---\n",
            "  [ModularFF a=0.5 ld=uniform] ep   1  loss=0.877  train=54.6%  val=54.4%  (p=0)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  10  loss=0.495  train=79.2%  val=79.2%  (p=2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  20  loss=0.361  train=77.9%  val=78.4%  (p=6)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  30  loss=0.316  train=75.1%  val=75.1%  (p=16)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 34 (reduction #1)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  40  loss=0.292  train=77.6%  val=77.4%  (p=4)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  50  loss=0.276  train=82.4%  val=82.3%  (p=14)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  60  loss=0.272  train=77.2%  val=77.6%  (p=8)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  70  loss=0.262  train=78.6%  val=78.6%  (p=18)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 72 (reduction #2)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  80  loss=0.257  train=77.7%  val=77.6%  (p=28)\n",
            "  [ModularFF a=0.5 ld=uniform] ep  90  loss=0.253  train=79.6%  val=79.4%  (p=38)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 92 (reduction #3)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 100  loss=0.267  train=81.4%  val=81.0%  (p=48)\n",
            "  [ModularFF a=0.5 ld=uniform] ep 110  loss=0.263  train=79.8%  val=79.4%  (p=58)\n",
            "  [ModularFF] Early stop at epoch 112 (best_val=84.1%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=84.1%)\n",
            "  [ModularFF a=0.5 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=82.6%, calibrated=82.3%, linear=82.1%, mlp=83.8%, temperature=83.1%\n",
            "    test=82.62%  280s  13409000 params\n",
            "    Goodness (spec0): L0:g+=0.925/g-=0.085/sep=0.839/th=0.500 | L1:g+=0.909/g-=0.161/sep=0.749/th=0.500 | L2:g+=0.904/g-=0.198/sep=0.706/th=0.500 | L3:g+=0.896/g-=0.232/sep=0.664/th=0.500\n",
            "    Avg specialist: acc=93.3%, separation=2.9\n",
            "\n",
            "--- ModularFF 4L arch=[550, 550, 550, 550] (alpha=1.0) ---\n",
            "  [ModularFF a=1.0 ld=uniform] ep   1  loss=0.857  train=72.1%  val=72.4%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  10  loss=0.331  train=79.9%  val=79.8%  (p=3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  20  loss=0.236  train=82.8%  val=83.1%  (p=0)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  30  loss=0.206  train=81.0%  val=81.6%  (p=6)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  40  loss=0.193  train=81.0%  val=80.8%  (p=4)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  50  loss=0.181  train=83.9%  val=83.8%  (p=14)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 56 (reduction #1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  60  loss=0.181  train=84.2%  val=84.4%  (p=24)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  70  loss=0.178  train=85.1%  val=85.0%  (p=1)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  80  loss=0.171  train=84.6%  val=84.5%  (p=11)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 89 (reduction #2)\n",
            "  [ModularFF a=1.0 ld=uniform] ep  90  loss=0.174  train=81.7%  val=81.6%  (p=21)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 100  loss=0.178  train=84.4%  val=84.2%  (p=31)\n",
            "  [ModularFF] LR reduced (×0.5) -> ? at epoch 109 (reduction #3)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 110  loss=0.175  train=80.9%  val=80.6%  (p=41)\n",
            "  [ModularFF a=1.0 ld=uniform] ep 120  loss=0.170  train=84.9%  val=84.5%  (p=51)\n",
            "  [ModularFF] Early stop at epoch 129 (best_val=85.0%, 3 LR reductions)\n",
            "  [ModularFF] Restored best checkpoint (val=85.0%)\n",
            "  [ModularFF a=1.0 kaiming trainable no-prune act=perceptron] DONE\n",
            "    Meta: argmax=83.2%, calibrated=81.8%, linear=82.1%, mlp=82.8%, temperature=82.0%\n",
            "    test=83.24%  322s  13409000 params\n",
            "    Goodness (spec0): L0:g+=0.850/g-=0.057/sep=0.793/th=0.500 | L1:g+=0.835/g-=0.055/sep=0.780/th=0.500 | L2:g+=0.822/g-=0.055/sep=0.767/th=0.500 | L3:g+=0.808/g-=0.055/sep=0.752/th=0.500\n",
            "    Avg specialist: acc=93.8%, separation=3.3\n",
            "\n",
            "✓ 4-Layer results saved: /content/drive/My Drive/Research/ModularFF/Results/FashionMNIST/results_4layer_seed42.json\n",
            "\n",
            "  --- FashionMNIST Summary ---\n",
            "  ClassicFF 4L (Adam):  27.2%\n",
            "  ClassicFF 4L (SGD):   24.1%\n",
            "  ModularFF 4L (best):  83.5%\n",
            "\n",
            "==========================================================================================\n",
            " FashionMNIST — 4-LAYER CROSS-ACTIVATION SUMMARY\n",
            "==========================================================================================\n",
            "Activation        FF(Adam)    FF(SGD)  ModularFF   Δ vs best FF\n",
            "------------------------------------------------------------------------------------------\n",
            "perceptron           27.2%      24.1%      83.5%         +56.2%\n",
            "==========================================================================================\n",
            "\n",
            "======================================================================\n",
            " PHASE 2: 4-LAYER HINTON COMPARISON COMPLETE (all activations)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "  DATASET: FashionMNIST\n",
            "======================================================================\n",
            "\n",
            "  PERCEPTRON:\n",
            "    seed=42:\n",
            "      ClassicFF 4L (Adam): 27.2% (13,596,000 params)\n",
            "      ClassicFF 4L (SGD):  24.1% (13,596,000 params)\n",
            "      ModularFF 4L (best): 83.5% (13,409,000 params) [ModularFF_4L_550_550_550_550_a0.0]\n",
            "      --> ModularFF wins by +56.2% (vs best ClassicFF)\n",
            "\n",
            "✓ Results: /content/drive/My Drive/Research/ModularFF/Results/\n",
            "✓ Logs: /content/drive/My Drive/Research/ModularFF/Logs/experiment_logs.csv\n",
            "\n",
            "==========================================================================================\n",
            " 4-LAYER RESULTS — FashionMNIST (K=10) — ALL ACTIVATIONS\n",
            "==========================================================================================\n",
            "Activation        FF(Adam)    FF(SGD)  ModularFF   Δ vs best FF\n",
            "------------------------------------------------------------------------------------------\n",
            "perceptron           27.2%      24.1%      83.5%         +56.2%\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ================================================================\n",
        "# CELL 4: Run Experiments (v6 — 4-Layer + Activation Sweep)\n",
        "# ================================================================\n",
        "# Phase 2: 4-Layer Hinton Comparison with Activation Sweep\n",
        "# - Datasets: FashionMNIST, Pendigits, LetterRecog (4-layer configs)\n",
        "# - Alpha: [0.0, 0.5, 1.0]\n",
        "# - Classic FF: both Adam and SGD (Hinton config)\n",
        "# - Activations: gelu, tanh, hardlimit, perceptron\n",
        "# - Loop order: DATASET (outer) × ACTIVATION (inner)\n",
        "# ================================================================\n",
        "\n",
        "#ACTIVATIONS = ['gelu', 'tanh', 'hardlimit', 'perceptron']\n",
        "ACTIVATIONS = ['perceptron']\n",
        "LEARNABLE_THETA = True  # Set False for fixed theta (default values per activation)\n",
        "\n",
        "# ================================================================\n",
        "# EXPERIMENT FLAGS\n",
        "# ================================================================\n",
        "\n",
        "RUN_4LAYER = True              # Run 4-layer Hinton comparison\n",
        "SHOW_SPECIALIST_PERF = False   # Print per-specialist table (verbose)\n",
        "\n",
        "# ================================================================\n",
        "# RUN EXPERIMENTS\n",
        "# ================================================================\n",
        "\n",
        "ALL_RESULTS = {}  # key: (ds_name, activation) -> {seed -> results}\n",
        "\n",
        "print('\\n' + '#'*70)\n",
        "print('#  PHASE 2: 4-LAYER HINTON COMPARISON (with Activation Sweep)')\n",
        "print('#  Datasets:', CONFIG['datasets_to_run'])\n",
        "print('#  Activations:', ACTIVATIONS)\n",
        "print('#  Alpha values: [0.0, 0.5, 1.0]')\n",
        "print('#  Adam LR:', CONFIG['lr'], '| SGD LR:', CONFIG.get('hinton_sgd_lr', 0.03),\n",
        "      '| Batch:', CONFIG['batch_size'], '| Patience:', CONFIG['early_stop_patience'])\n",
        "print('#  Loop order: DATASET (outer) × ACTIVATION (inner)')\n",
        "print('#'*70)\n",
        "\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    if ds_name not in DATASETS:\n",
        "        print(f'WARNING: {ds_name} not loaded, skipping')\n",
        "        continue\n",
        "\n",
        "    print('\\n' + '='*70)\n",
        "    print(f'  DATASET: {ds_name}')\n",
        "    print('='*70)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        print('\\n' + '#'*70)\n",
        "        print(f'#  DATASET: {ds_name} | ACTIVATION: {act}')\n",
        "        print('#'*70)\n",
        "\n",
        "        result_key = (ds_name, act)\n",
        "        ALL_RESULTS[result_key] = {}\n",
        "\n",
        "        for seed in CONFIG['seeds']:\n",
        "            ALL_RESULTS[result_key][seed] = run_4layer_experiment(\n",
        "                ds_name, seed, CONFIG,\n",
        "                show_specialist_perf=SHOW_SPECIALIST_PERF,\n",
        "                activation=act,\n",
        "                learnable_theta=LEARNABLE_THETA\n",
        "            )\n",
        "\n",
        "    # --- Per-dataset summary (all activations) ---\n",
        "    print('\\n' + '='*90)\n",
        "    print(f' {ds_name} — 4-LAYER CROSS-ACTIVATION SUMMARY')\n",
        "    print('='*90)\n",
        "    print(f'{\"Activation\":<15} {\"FF(Adam)\":>10} {\"FF(SGD)\":>10} {\"ModularFF\":>10} {\"Δ vs best FF\":>14}')\n",
        "    print('-'*90)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            ff_adam = res.get('ClassicFF_4L_Adam', {}).get('test_acc', 0)\n",
        "            ff_sgd = res.get('ClassicFF_4L_SGD', {}).get('test_acc', 0)\n",
        "            ff_best = max(ff_adam, ff_sgd)\n",
        "            best_acc = max(\n",
        "                (v.get('test_acc', 0) for k, v in res.items() if k.startswith('ModularFF_4L')),\n",
        "                default=0\n",
        "            )\n",
        "            delta = best_acc - ff_best\n",
        "            delta_str = f'+{delta:.1f}%' if delta > 0 else f'{delta:.1f}%'\n",
        "            print(f'{act:<15} {ff_adam:>9.1f}% {ff_sgd:>9.1f}% {best_acc:>9.1f}% {delta_str:>14}')\n",
        "\n",
        "    print('='*90)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FINAL SUMMARY\n",
        "# ================================================================\n",
        "print('\\n' + '='*70)\n",
        "print(' PHASE 2: 4-LAYER HINTON COMPARISON COMPLETE (all activations)')\n",
        "print('='*70)\n",
        "\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'  DATASET: {ds_name}')\n",
        "    print(f'{\"=\"*70}')\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        print(f'\\n  {act.upper()}:')\n",
        "\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            ff_adam = res.get('ClassicFF_4L_Adam', {}).get('test_acc', 0)\n",
        "            ff_adam_params = res.get('ClassicFF_4L_Adam', {}).get('params', 0)\n",
        "            ff_sgd = res.get('ClassicFF_4L_SGD', {}).get('test_acc', 0)\n",
        "            ff_sgd_params = res.get('ClassicFF_4L_SGD', {}).get('params', 0)\n",
        "            ff_best = max(ff_adam, ff_sgd)\n",
        "\n",
        "            best_acc, best_key, best_params = 0, '', 0\n",
        "            for key, val in res.items():\n",
        "                if key.startswith('ModularFF_4L') and 'test_acc' in val:\n",
        "                    if val['test_acc'] > best_acc:\n",
        "                        best_acc = val['test_acc']\n",
        "                        best_key = key\n",
        "                        best_params = val.get('params', 0)\n",
        "\n",
        "            print(f'    seed={seed}:')\n",
        "            print(f'      ClassicFF 4L (Adam): {ff_adam:.1f}% ({ff_adam_params:,} params)')\n",
        "            print(f'      ClassicFF 4L (SGD):  {ff_sgd:.1f}% ({ff_sgd_params:,} params)')\n",
        "            print(f'      ModularFF 4L (best): {best_acc:.1f}% ({best_params:,} params) [{best_key}]')\n",
        "            if best_acc > ff_best:\n",
        "                print(f'      --> ModularFF wins by +{best_acc - ff_best:.1f}% (vs best ClassicFF)')\n",
        "            elif ff_best > best_acc:\n",
        "                winner = 'Adam' if ff_adam >= ff_sgd else 'SGD'\n",
        "                print(f'      --> ClassicFF ({winner}) wins by +{ff_best - best_acc:.1f}%')\n",
        "            else:\n",
        "                print(f'      --> TIE')\n",
        "\n",
        "print(f'\\n\\u2713 Results: {CONFIG[\"results_path\"]}')\n",
        "print(f'\\u2713 Logs: {LOGGER.path}')\n",
        "\n",
        "# ================================================================\n",
        "# GRAND COMPARISON TABLE\n",
        "# ================================================================\n",
        "for ds_name in CONFIG['datasets_to_run']:\n",
        "    K = DATASETS[ds_name]['num_classes']\n",
        "    print(f'\\n{\"=\"*90}')\n",
        "    print(f' 4-LAYER RESULTS — {ds_name} (K={K}) — ALL ACTIVATIONS')\n",
        "    print(f'{\"=\"*90}')\n",
        "    print(f'{\"Activation\":<15} {\"FF(Adam)\":>10} {\"FF(SGD)\":>10} {\"ModularFF\":>10} {\"Δ vs best FF\":>14}')\n",
        "    print('-'*90)\n",
        "\n",
        "    for act in ACTIVATIONS:\n",
        "        result_key = (ds_name, act)\n",
        "        if result_key not in ALL_RESULTS:\n",
        "            continue\n",
        "        for seed in CONFIG['seeds']:\n",
        "            if seed not in ALL_RESULTS[result_key]:\n",
        "                continue\n",
        "            res = ALL_RESULTS[result_key][seed]\n",
        "\n",
        "            ff_adam = res.get('ClassicFF_4L_Adam', {}).get('test_acc', 0)\n",
        "            ff_sgd = res.get('ClassicFF_4L_SGD', {}).get('test_acc', 0)\n",
        "            ff_best = max(ff_adam, ff_sgd)\n",
        "\n",
        "            best_acc = max(\n",
        "                (v.get('test_acc', 0) for k, v in res.items() if k.startswith('ModularFF_4L')),\n",
        "                default=0\n",
        "            )\n",
        "\n",
        "            delta = best_acc - ff_best\n",
        "            delta_str = f'+{delta:.1f}%' if delta > 0 else f'{delta:.1f}%'\n",
        "\n",
        "            print(f'{act:<15} {ff_adam:>9.1f}% {ff_sgd:>9.1f}% {best_acc:>9.1f}% {delta_str:>14}')\n",
        "\n",
        "    print('='*90)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_QZZiAgr-tE"
      },
      "outputs": [],
      "source": [
        "########## end of cell 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cSqsdkJJvic"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 5: Analysis — Activation Sweep Results (v5)\n",
        "# ================================================================\n",
        "# Handles activation dimension + 2-layer / 4-layer split\n",
        "# File naming: results_2layer_seed42.json, results_4layer_seed42.json\n",
        "# Each JSON contains keys like \"ClassicFF_Adam\", \"ModularFF_4L_50_50_50_50_a0.0\"\n",
        "# with an \"activation\" field inside each entry\n",
        "# ================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ================================================================\n",
        "# LOAD ALL RESULTS\n",
        "# ================================================================\n",
        "\n",
        "def load_all_results():\n",
        "    \"\"\"Load all JSON result files, keyed by (dataset, depth, activation, seed).\"\"\"\n",
        "    all_res = {}\n",
        "    for ds in CONFIG['datasets_to_run']:\n",
        "        result_dir = os.path.join(CONFIG['results_path'], ds)\n",
        "        if not os.path.exists(result_dir):\n",
        "            continue\n",
        "        for fname in os.listdir(result_dir):\n",
        "            if not fname.endswith('.json'):\n",
        "                continue\n",
        "            fpath = os.path.join(result_dir, fname)\n",
        "\n",
        "            # Parse filename: results_2layer_seed42.json or results_4layer_seed42.json\n",
        "            # Also handle old format: results_seed42.json\n",
        "            m = re.match(r'results_(\\d+layer)_seed(\\d+)\\.json', fname)\n",
        "            if m:\n",
        "                depth = m.group(1)  # \"2layer\" or \"4layer\"\n",
        "                seed = int(m.group(2))\n",
        "            else:\n",
        "                m2 = re.match(r'results_seed(\\d+)\\.json', fname)\n",
        "                if m2:\n",
        "                    depth = \"2layer\"\n",
        "                    seed = int(m2.group(1))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            with open(fpath, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Group entries by activation\n",
        "            for method_key, val in data.items():\n",
        "                if not isinstance(val, dict):\n",
        "                    continue\n",
        "                act = val.get('activation', 'relu')  # default to relu for old results\n",
        "                key = (ds, depth, act, seed)\n",
        "                if key not in all_res:\n",
        "                    all_res[key] = {}\n",
        "                all_res[key][method_key] = val\n",
        "\n",
        "    return all_res\n",
        "\n",
        "\n",
        "print('Loading results...')\n",
        "ALL_RES = load_all_results()\n",
        "print(f'✓ Loaded {len(ALL_RES)} (dataset, depth, activation, seed) combinations\\n')\n",
        "\n",
        "# Discover what's available\n",
        "available = {}\n",
        "for (ds, depth, act, seed) in ALL_RES.keys():\n",
        "    if ds not in available:\n",
        "        available[ds] = {'depths': set(), 'activations': set(), 'seeds': set()}\n",
        "    available[ds]['depths'].add(depth)\n",
        "    available[ds]['activations'].add(act)\n",
        "    available[ds]['seeds'].add(seed)\n",
        "\n",
        "for ds, info in sorted(available.items()):\n",
        "    print(f'  {ds}: depths={sorted(info[\"depths\"])}, '\n",
        "          f'activations={sorted(info[\"activations\"])}, '\n",
        "          f'seeds={sorted(info[\"seeds\"])}')\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ================================================================\n",
        "\n",
        "def get_acc(all_res, ds, depth, act, seed, method_key):\n",
        "    \"\"\"Get test accuracy for a specific (ds, depth, activation, seed, method).\"\"\"\n",
        "    key = (ds, depth, act, seed)\n",
        "    if key not in all_res:\n",
        "        return None\n",
        "    entry = all_res[key]\n",
        "    # Exact match\n",
        "    if method_key in entry:\n",
        "        return entry[method_key].get('test_acc')\n",
        "    # Partial match\n",
        "    for k, v in entry.items():\n",
        "        if method_key in k:\n",
        "            return v.get('test_acc')\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_meta_accs(all_res, ds, depth, act, seed, method_key):\n",
        "    \"\"\"Get all meta-layer accuracies for a specific entry.\"\"\"\n",
        "    key = (ds, depth, act, seed)\n",
        "    if key not in all_res:\n",
        "        return {}\n",
        "    entry = all_res[key]\n",
        "    for k, v in entry.items():\n",
        "        if method_key in k or k == method_key:\n",
        "            return v.get('meta_results', {})\n",
        "    return {}\n",
        "\n",
        "\n",
        "def get_best_acc(all_res, ds, depth, act, seed, method_prefix):\n",
        "    \"\"\"Get best test accuracy (including meta-layer) for a method prefix.\"\"\"\n",
        "    key = (ds, depth, act, seed)\n",
        "    if key not in all_res:\n",
        "        return None\n",
        "    best = 0\n",
        "    for k, v in all_res[key].items():\n",
        "        if not k.startswith(method_prefix):\n",
        "            continue\n",
        "        acc = v.get('test_acc', 0)\n",
        "        best = max(best, acc)\n",
        "        for m_acc in v.get('meta_results', {}).values():\n",
        "            best = max(best, m_acc)\n",
        "    return best if best > 0 else None\n",
        "\n",
        "\n",
        "def get_goodness_info(all_res, ds, depth, act, seed, method_key):\n",
        "    \"\"\"Get goodness diagnostics from an entry.\"\"\"\n",
        "    key = (ds, depth, act, seed)\n",
        "    if key not in all_res:\n",
        "        return None\n",
        "    for k, v in all_res[key].items():\n",
        "        if method_key in k or k == method_key:\n",
        "            return v.get('goodness_diagnostics')\n",
        "    return None\n",
        "\n",
        "\n",
        "def fmt(val):\n",
        "    if val is None:\n",
        "        return '---'\n",
        "    return f'{val:.1f}%'\n",
        "\n",
        "\n",
        "def fmt_delta(val, ref):\n",
        "    if val is None or ref is None:\n",
        "        return '---'\n",
        "    d = val - ref\n",
        "    sign = '+' if d > 0 else ''\n",
        "    return f'{sign}{d:.1f}%'\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TABLE A: ACTIVATION COMPARISON — 2-Layer\n",
        "# For each activation: best ClassicFF, best ModularFF, BP, delta\n",
        "# ================================================================\n",
        "for depth, depth_label in [('2layer', '2-LAYER'), ('4layer', '4-LAYER')]:\n",
        "    print(f'\\n{\"=\"*90}')\n",
        "    print(f' TABLE: {depth_label} ACTIVATION COMPARISON')\n",
        "    print(f'{\"=\"*90}')\n",
        "\n",
        "    # Get all activations available for this depth\n",
        "    all_acts = set()\n",
        "    for (ds, d, act, seed) in ALL_RES.keys():\n",
        "        if d == depth:\n",
        "            all_acts.add(act)\n",
        "    all_acts = sorted(all_acts)\n",
        "\n",
        "    if not all_acts:\n",
        "        print('  No results found for this depth.')\n",
        "        continue\n",
        "\n",
        "    for ds in CONFIG['datasets_to_run']:\n",
        "        if ds not in available:\n",
        "            continue\n",
        "        seeds = sorted(available[ds]['seeds'])\n",
        "        seed = seeds[0]  # Use first seed for display\n",
        "\n",
        "        ds_label = {'XOR': 'XOR (K=4)', 'MNIST': 'MNIST', 'FashionMNIST': 'FashionMNIST',\n",
        "                    'Pendigits': 'Pendigits', 'LetterRecog': 'LetterRecog'}.get(ds, ds)\n",
        "\n",
        "        print(f'\\n  {ds_label}:')\n",
        "        header = f'  {\"Activation\":12s}  {\"BP\":>8s}  {\"FF(Adam)\":>10s}  {\"FF(SGD)\":>10s}  {\"ModularFF\":>10s}  {\"Δ vs FF\":>10s}'\n",
        "        print(header)\n",
        "        print('  ' + '-' * (len(header) - 2))\n",
        "\n",
        "        for act in all_acts:\n",
        "            # BP\n",
        "            bp = get_acc(ALL_RES, ds, depth, act, seed, 'BP')\n",
        "\n",
        "            # Classic FF\n",
        "            ff_adam = get_acc(ALL_RES, ds, depth, act, seed, 'ClassicFF_Adam')\n",
        "            ff_sgd = get_acc(ALL_RES, ds, depth, act, seed, 'ClassicFF_SGD')\n",
        "\n",
        "            # Best ClassicFF (including variants for 2-layer)\n",
        "            ff_best = 0\n",
        "            for k_prefix in ['ClassicFF']:\n",
        "                key = (ds, depth, act, seed)\n",
        "                if key in ALL_RES:\n",
        "                    for k, v in ALL_RES[key].items():\n",
        "                        if k.startswith('ClassicFF'):\n",
        "                            acc = v.get('test_acc', 0)\n",
        "                            ff_best = max(ff_best, acc)\n",
        "            ff_best = ff_best if ff_best > 0 else None\n",
        "\n",
        "            # Best ModularFF (including all meta-layers)\n",
        "            mod_best = get_best_acc(ALL_RES, ds, depth, act, seed, 'ModularFF')\n",
        "\n",
        "            delta = fmt_delta(mod_best, ff_best) if mod_best and ff_best else '---'\n",
        "\n",
        "            row = f'  {act:12s}  {fmt(bp):>8s}  {fmt(ff_adam):>10s}  {fmt(ff_sgd):>10s}  {fmt(mod_best):>10s}  {delta:>10s}'\n",
        "            print(row)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TABLE B: GOODNESS DIAGNOSTICS — Learnable Theta Values\n",
        "# ================================================================\n",
        "print(f'\\n{\"=\"*90}')\n",
        "print(f' TABLE: LEARNABLE THETA VALUES (4-Layer, per activation)')\n",
        "print(f'{\"=\"*90}')\n",
        "\n",
        "for ds in CONFIG['datasets_to_run']:\n",
        "    if ds not in available:\n",
        "        continue\n",
        "    seeds = sorted(available[ds]['seeds'])\n",
        "    seed = seeds[0]\n",
        "\n",
        "    all_acts = sorted(a for (d, dp, a, s) in ALL_RES.keys() if d == ds and dp == '4layer')\n",
        "    if not all_acts:\n",
        "        continue\n",
        "\n",
        "    ds_label = {'XOR': 'XOR', 'MNIST': 'MNIST', 'FashionMNIST': 'FashionMNIST',\n",
        "                'Pendigits': 'Pendigits', 'LetterRecog': 'LetterRecog'}.get(ds, ds)\n",
        "    print(f'\\n  {ds_label} — ClassicFF 4L (Adam):')\n",
        "\n",
        "    for act in all_acts:\n",
        "        key = (ds, '4layer', act, seed)\n",
        "        if key not in ALL_RES:\n",
        "            continue\n",
        "        for k, v in ALL_RES[key].items():\n",
        "            if 'ClassicFF_Adam' in k:\n",
        "                gd = v.get('goodness_diagnostics', {})\n",
        "                if gd:\n",
        "                    layers = gd if isinstance(gd, list) else []\n",
        "                    theta_str = ' | '.join(\n",
        "                        f'L{i}: th={l.get(\"theta\", \"?\"):.3f}, sep={l.get(\"separation\", \"?\"):.3f}'\n",
        "                        for i, l in enumerate(layers)\n",
        "                    ) if layers else str(gd)\n",
        "                    print(f'    {act:12s}: {theta_str}')\n",
        "                break\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# TABLE C: DETAILED METHOD COMPARISON (per activation)\n",
        "# ================================================================\n",
        "for depth, depth_label in [('2layer', '2-LAYER'), ('4layer', '4-LAYER')]:\n",
        "    print(f'\\n{\"=\"*90}')\n",
        "    print(f' TABLE: {depth_label} DETAILED RESULTS BY ACTIVATION')\n",
        "    print(f'{\"=\"*90}')\n",
        "\n",
        "    for ds in CONFIG['datasets_to_run']:\n",
        "        if ds not in available:\n",
        "            continue\n",
        "        seeds = sorted(available[ds]['seeds'])\n",
        "        seed = seeds[0]\n",
        "\n",
        "        all_acts = sorted(a for (d, dp, a, s) in ALL_RES.keys() if d == ds and dp == depth)\n",
        "        if not all_acts:\n",
        "            continue\n",
        "\n",
        "        for act in all_acts:\n",
        "            key = (ds, depth, act, seed)\n",
        "            if key not in ALL_RES:\n",
        "                continue\n",
        "\n",
        "            print(f'\\n  {ds} | {depth} | {act}:')\n",
        "            entries = ALL_RES[key]\n",
        "            for method_key in sorted(entries.keys()):\n",
        "                val = entries[method_key]\n",
        "                acc = val.get('test_acc', 0)\n",
        "                params = val.get('total_params', '?')\n",
        "                mr = val.get('meta_results', {})\n",
        "                meta_str = ''\n",
        "                if mr:\n",
        "                    best_meta = max(mr, key=mr.get)\n",
        "                    meta_str = f'  (best meta: {best_meta}={mr[best_meta]:.1f}%)'\n",
        "                print(f'    {method_key:45s}  {acc:6.1f}%  [{params} params]{meta_str}')\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SUMMARY CSV\n",
        "# ================================================================\n",
        "print(f'\\n{\"-\"*40}')\n",
        "\n",
        "summary_rows = []\n",
        "for (ds, depth, act, seed), entries in ALL_RES.items():\n",
        "    for method_key, val in entries.items():\n",
        "        row = {\n",
        "            'dataset': ds, 'depth': depth, 'activation': act, 'seed': seed,\n",
        "            'method': method_key, 'test_acc': val.get('test_acc'),\n",
        "            'total_params': val.get('total_params'),\n",
        "        }\n",
        "        for m, v in val.get('meta_results', {}).items():\n",
        "            row[f'meta_{m}'] = v\n",
        "        summary_rows.append(row)\n",
        "\n",
        "if summary_rows:\n",
        "    df = pd.DataFrame(summary_rows)\n",
        "    csv_path = os.path.join(CONFIG['results_path'], 'activation_sweep_summary.csv')\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f'✓ Summary saved: {csv_path}')\n",
        "    print(f'  {len(df)} entries across {df[\"activation\"].nunique()} activations, '\n",
        "          f'{df[\"depth\"].nunique()} depths, {df[\"dataset\"].nunique()} datasets')\n",
        "\n",
        "print('\\n✓ Analysis complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM_h_TKisCiG"
      },
      "outputs": [],
      "source": [
        "######### end of cell 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jx0u1swJ0Fn"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# CELL 6: Visualization (v4)\n",
        "# ================================================================\n",
        "\n",
        "DS_LABELS = {'XOR': '2D XOR (K=4)', 'MNIST': 'MNIST (K=10)',\n",
        "             'Pendigits': 'Pendigits (K=10)', 'LetterRecog': 'Letters (K=26)'}\n",
        "\n",
        "\n",
        "def load_results_for_viz():\n",
        "    all_res = {}\n",
        "    for ds in CONFIG['datasets_to_run']:\n",
        "        all_res[ds] = {}\n",
        "        result_dir = os.path.join(CONFIG['results_path'], ds)\n",
        "        if not os.path.exists(result_dir):\n",
        "            continue\n",
        "        for fname in os.listdir(result_dir):\n",
        "            if fname.endswith('.json'):\n",
        "                seed = int(fname.replace('results_seed', '').replace('.json', '').split('_')[0])\n",
        "                fpath = os.path.join(result_dir, fname)\n",
        "                with open(fpath, 'r') as f:\n",
        "                    all_res[ds][seed] = json.load(f)\n",
        "    return all_res\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FIGURE 1: XOR Analysis\n",
        "# ================================================================\n",
        "\n",
        "def fig_xor_analysis():\n",
        "    if 'XOR' not in DATASETS:\n",
        "        print('  XOR not loaded, skipping')\n",
        "        return\n",
        "\n",
        "    ds = DATASETS['XOR']\n",
        "    arch = ARCHITECTURES['XOR']\n",
        "    set_seed(42)\n",
        "    dev = CONFIG['device']\n",
        "\n",
        "    ens = ModularFFEnsemble(ds['input_dim'], arch['modularff'], ds['num_classes'],\n",
        "                        CONFIG['lr'], CONFIG['theta_neuron'], dev,\n",
        "                        use_meta_layer=False)\n",
        "    for _ in range(60):\n",
        "        ens.train_epoch(ds['X_train'], ds['y_train'], alpha=0.3)\n",
        "\n",
        "    cff = ClassicFF(ds['input_dim'], arch['classic_ff'], ds['num_classes'], CONFIG['lr'], dev)\n",
        "    loader = make_loader(ds['X_train'], ds['y_train'], CONFIG['batch_size'])\n",
        "    for _ in range(60):\n",
        "        cff.train_epoch(loader)\n",
        "\n",
        "    res = 100\n",
        "    xr = np.linspace(-1.2, 1.2, res)\n",
        "    xx, yy = np.meshgrid(xr, xr)\n",
        "    grid = np.column_stack([xx.ravel(), yy.ravel()]).astype(np.float32)\n",
        "\n",
        "    G = ens._all_goodness(grid).cpu().numpy()\n",
        "    modularff_pred = ens.predict(grid, 'argmax')\n",
        "    ff_pred = cff.predict(grid)\n",
        "    modularff_acc = ens.evaluate(ds['X_test'], ds['y_test'], 'argmax')\n",
        "    ff_acc = cff.evaluate(ds['X_test'], ds['y_test'])\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    cmap = plt.cm.Set1\n",
        "\n",
        "    for k in range(4):\n",
        "        ax = axes[k // 2, k % 2]\n",
        "        im = ax.contourf(xx, yy, G[:, k].reshape(res, res), levels=20, cmap='hot')\n",
        "        ax.axhline(0, color='white', ls='--', alpha=0.5)\n",
        "        ax.axvline(0, color='white', ls='--', alpha=0.5)\n",
        "        ax.set_title(f'Specialist {k}')\n",
        "        plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "\n",
        "    ax = axes[0, 2]\n",
        "    ax.contourf(xx, yy, modularff_pred.reshape(res, res), levels=4, cmap=cmap, alpha=0.4)\n",
        "    for k in range(4):\n",
        "        m = ds['y_test'] == k\n",
        "        ax.scatter(ds['X_test'][m, 0], ds['X_test'][m, 1], c=[cmap(k)], s=10)\n",
        "    ax.set_title(f'ModularFF ({modularff_acc:.1f}%)')\n",
        "\n",
        "    ax = axes[1, 2]\n",
        "    ax.contourf(xx, yy, ff_pred.reshape(res, res), levels=4, cmap=cmap, alpha=0.4)\n",
        "    for k in range(4):\n",
        "        m = ds['y_test'] == k\n",
        "        ax.scatter(ds['X_test'][m, 0], ds['X_test'][m, 1], c=[cmap(k)], s=10)\n",
        "    ax.set_title(f'Classic FF ({ff_acc:.1f}%)')\n",
        "\n",
        "    plt.suptitle('XOR Analysis', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(CONFIG['figures_path'], 'XOR', 'xor_analysis.png')\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    print(f'\\u2713 Saved: {path}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FIGURE 2: Convergence\n",
        "# ================================================================\n",
        "\n",
        "def fig_convergence():\n",
        "    all_res = load_results_for_viz()\n",
        "    ds_list = [ds for ds in CONFIG['datasets_to_run'] if ds in all_res and 42 in all_res[ds]]\n",
        "\n",
        "    if not ds_list:\n",
        "        print('  No results found')\n",
        "        return\n",
        "\n",
        "    n = len(ds_list)\n",
        "    fig, axes = plt.subplots(1, n, figsize=(6*n, 5))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, ds in enumerate(ds_list):\n",
        "        ax = axes[idx]\n",
        "        res = all_res[ds][42]\n",
        "\n",
        "        for key, label, color, ls in [\n",
        "            ('BP', 'BP', 'black', '-'),\n",
        "            ('ClassicFF', 'Classic FF', 'blue', '--'),\n",
        "            ('ModularFF_a0.0', 'ModularFF α=0', 'green', ':'),\n",
        "            ('ModularFF_a0.3', 'ModularFF α=0.3', 'red', '-'),\n",
        "            ('ModularFF_a1.0', 'ModularFF α=1.0', 'purple', '-.'),\n",
        "        ]:\n",
        "            if key in res and 'val_acc' in res[key]:\n",
        "                v = res[key]['val_acc']\n",
        "                ax.plot(range(1, len(v)+1), v, label=label, color=color, ls=ls, lw=1.5)\n",
        "\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Val Accuracy (%)')\n",
        "        ax.set_title(DS_LABELS.get(ds, ds))\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.2)\n",
        "\n",
        "    plt.suptitle('Convergence', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(CONFIG['figures_path'], 'convergence', 'convergence.png')\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    print(f'\\u2713 Saved: {path}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FIGURE 3: Alpha Effect\n",
        "# ================================================================\n",
        "\n",
        "def fig_alpha_effect():\n",
        "    all_res = load_results_for_viz()\n",
        "    ds_list = [ds for ds in CONFIG['datasets_to_run'] if ds in all_res]\n",
        "\n",
        "    if not ds_list:\n",
        "        print('  No results found')\n",
        "        return\n",
        "\n",
        "    n = len(ds_list)\n",
        "    fig, axes = plt.subplots(1, n, figsize=(5*n, 4))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    alphas = CONFIG['alpha_values']\n",
        "    x = np.arange(len(alphas))\n",
        "\n",
        "    for idx, ds in enumerate(ds_list):\n",
        "        ax = axes[idx]\n",
        "        means = []\n",
        "        for a in alphas:\n",
        "            vals = [all_res[ds][s].get(f'ModularFF_a{a}', {}).get('test_acc', 0)\n",
        "                    for s in CONFIG['seeds'] if s in all_res[ds]]\n",
        "            means.append(np.mean(vals) if vals else 0)\n",
        "\n",
        "        ax.bar(x, means, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(alphas))))\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels([str(a) for a in alphas])\n",
        "        ax.set_xlabel('α')\n",
        "        ax.set_ylabel('Test Acc (%)')\n",
        "        ax.set_title(DS_LABELS.get(ds, ds))\n",
        "\n",
        "        ff_vals = [all_res[ds][s].get('ClassicFF', {}).get('test_acc', 0)\n",
        "                   for s in CONFIG['seeds'] if s in all_res[ds]]\n",
        "        if ff_vals:\n",
        "            ax.axhline(np.mean(ff_vals), color='blue', ls='--', label='Classic FF')\n",
        "            ax.legend(fontsize=8)\n",
        "\n",
        "    plt.suptitle('Effect of α', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(CONFIG['figures_path'], 'alpha_effect.png')\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    print(f'\\u2713 Saved: {path}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FIGURE 4: Meta-Layer Comparison\n",
        "# ================================================================\n",
        "\n",
        "def fig_meta_comparison():\n",
        "    all_res = load_results_for_viz()\n",
        "    ds_list = [ds for ds in CONFIG['datasets_to_run'] if ds in all_res]\n",
        "\n",
        "    if not ds_list:\n",
        "        print('  No results found')\n",
        "        return\n",
        "\n",
        "    n = len(ds_list)\n",
        "    fig, axes = plt.subplots(1, n, figsize=(5*n, 4))\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    meta_types = ['argmax', 'calibrated', 'linear', 'mlp', 'temperature']\n",
        "    x = np.arange(len(meta_types))\n",
        "    colors = {'argmax': 'gray', 'calibrated': 'lightblue',\n",
        "              'linear': 'green', 'mlp': 'orange', 'temperature': 'purple'}\n",
        "\n",
        "    for idx, ds in enumerate(ds_list):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        best_a, best_acc = 0.3, 0\n",
        "        for a in CONFIG['alpha_values']:\n",
        "            key = f'ModularFF_a{a}'\n",
        "            vals = [all_res[ds][s].get(key, {}).get('test_acc', 0)\n",
        "                    for s in CONFIG['seeds'] if s in all_res[ds]]\n",
        "            if vals and np.mean(vals) > best_acc:\n",
        "                best_acc = np.mean(vals)\n",
        "                best_a = a\n",
        "\n",
        "        key = f'ModularFF_a{best_a}'\n",
        "        means = []\n",
        "        for mt in meta_types:\n",
        "            vals = []\n",
        "            for s in CONFIG['seeds']:\n",
        "                if s in all_res[ds] and key in all_res[ds][s]:\n",
        "                    mr = all_res[ds][s][key].get('meta_results', {})\n",
        "                    if mt in mr:\n",
        "                        vals.append(mr[mt])\n",
        "            means.append(np.mean(vals) if vals else 0)\n",
        "\n",
        "        ax.bar(x, means, color=[colors[m] for m in meta_types])\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(meta_types, rotation=45, ha='right')\n",
        "        ax.set_ylabel('Test Acc (%)')\n",
        "        ax.set_title(f'{DS_LABELS.get(ds, ds)}\\n(α={best_a})')\n",
        "\n",
        "    plt.suptitle('Meta-Layer Comparison', fontsize=13, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(CONFIG['figures_path'], 'meta_comparison.png')\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    print(f'\\u2713 Saved: {path}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FIGURE 5: Per-Specialist Performance\n",
        "# ================================================================\n",
        "\n",
        "def fig_specialist_performance():\n",
        "    all_res = load_results_for_viz()\n",
        "    ds_list = [ds for ds in CONFIG['datasets_to_run'] if ds in all_res]\n",
        "\n",
        "    if not ds_list:\n",
        "        print('  No results found')\n",
        "        return\n",
        "\n",
        "    for ds in ds_list:\n",
        "        # Find best alpha\n",
        "        best_a = 0.3\n",
        "        best_acc = 0\n",
        "        for a in CONFIG['alpha_values']:\n",
        "            key = f'ModularFF_a{a}'\n",
        "            for s in CONFIG['seeds']:\n",
        "                if s in all_res[ds] and key in all_res[ds][s]:\n",
        "                    acc = all_res[ds][s][key].get('test_acc', 0)\n",
        "                    if acc > best_acc:\n",
        "                        best_acc = acc\n",
        "                        best_a = a\n",
        "\n",
        "        key = f'ModularFF_a{best_a}'\n",
        "        seed = CONFIG['seeds'][0]\n",
        "\n",
        "        if seed not in all_res[ds] or key not in all_res[ds][seed]:\n",
        "            continue\n",
        "\n",
        "        sr = all_res[ds][seed][key].get('specialist_results', {})\n",
        "        if not sr:\n",
        "            continue\n",
        "\n",
        "        K = len(sr)\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        specialists = sorted(sr.keys(), key=lambda x: int(x))\n",
        "        acc = [sr[k]['accuracy'] for k in specialists]\n",
        "        sens = [sr[k]['sensitivity'] for k in specialists]\n",
        "        spec = [sr[k]['specificity'] for k in specialists]\n",
        "        sep = [sr[k]['separation'] for k in specialists]\n",
        "\n",
        "        x = np.arange(K)\n",
        "\n",
        "        # Accuracy\n",
        "        ax = axes[0]\n",
        "        ax.bar(x, acc, color='steelblue')\n",
        "        ax.axhline(np.mean(acc), color='red', ls='--', label=f'Mean={np.mean(acc):.1f}%')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(specialists)\n",
        "        ax.set_xlabel('Specialist')\n",
        "        ax.set_ylabel('Binary Accuracy (%)')\n",
        "        ax.set_title('Per-Specialist Accuracy')\n",
        "        ax.legend()\n",
        "\n",
        "        # Sensitivity vs Specificity\n",
        "        ax = axes[1]\n",
        "        width = 0.35\n",
        "        ax.bar(x - width/2, sens, width, label='Sensitivity', color='green')\n",
        "        ax.bar(x + width/2, spec, width, label='Specificity', color='orange')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(specialists)\n",
        "        ax.set_xlabel('Specialist')\n",
        "        ax.set_ylabel('%')\n",
        "        ax.set_title('Sensitivity vs Specificity')\n",
        "        ax.legend()\n",
        "\n",
        "        # Separation\n",
        "        ax = axes[2]\n",
        "        colors = ['green' if s > 0 else 'red' for s in sep]\n",
        "        ax.bar(x, sep, color=colors)\n",
        "        ax.axhline(0, color='black', lw=0.5)\n",
        "        ax.axhline(np.mean(sep), color='red', ls='--', label=f'Mean={np.mean(sep):.1f}')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(specialists)\n",
        "        ax.set_xlabel('Specialist')\n",
        "        ax.set_ylabel('G_pos - G_neg')\n",
        "        ax.set_title('Goodness Separation')\n",
        "        ax.legend()\n",
        "\n",
        "        plt.suptitle(f'{DS_LABELS.get(ds, ds)} — Per-Specialist Performance (α={best_a})',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        path = os.path.join(CONFIG['figures_path'], f'{ds}_specialist_perf.png')\n",
        "        plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "        print(f'\\u2713 Saved: {path}')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# RUN ALL FIGURES\n",
        "# ================================================================\n",
        "print('Generating figures...\\n')\n",
        "\n",
        "print('--- Figure 1: XOR ---')\n",
        "try:\n",
        "    fig_xor_analysis()\n",
        "except Exception as e:\n",
        "    print(f'  Error: {e}')\n",
        "\n",
        "print('\\n--- Figure 2: Convergence ---')\n",
        "try:\n",
        "    fig_convergence()\n",
        "except Exception as e:\n",
        "    print(f'  Error: {e}')\n",
        "\n",
        "print('\\n--- Figure 3: Alpha Effect ---')\n",
        "try:\n",
        "    fig_alpha_effect()\n",
        "except Exception as e:\n",
        "    print(f'  Error: {e}')\n",
        "\n",
        "print('\\n--- Figure 4: Meta-Layer ---')\n",
        "try:\n",
        "    fig_meta_comparison()\n",
        "except Exception as e:\n",
        "    print(f'  Error: {e}')\n",
        "\n",
        "print('\\n--- Figure 5: Specialist Performance ---')\n",
        "try:\n",
        "    fig_specialist_performance()\n",
        "except Exception as e:\n",
        "    print(f'  Error: {e}')\n",
        "\n",
        "print(f'\\n\\u2713 Figures saved to: {CONFIG[\"figures_path\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRbG7fstsHw2"
      },
      "outputs": [],
      "source": [
        "######## end of cell 6 and codebase"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}